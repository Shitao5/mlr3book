---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers need not only software tools but also datasets.
  In addition, large-scale computational experiments require the use of high-performance computing (HPC) clusters.
  [OpenML](http://www.openml.org/) is an open platform that facilitates the sharing and dissemination of machine learning research data and addresses the first of these issues.
  It provides unique identifiers and standardised (meta)data formats that make it easy to find relevant datasets by querying for specific properties, and to share them using their identifier.
  The first part of this chapter covers the basics of OpenML and shows how it can be used via the mlr3oml interface package, which conveniently integrates mlr3 and OpenML via its REST API.
  We then show how to simplify the execution of benchmark experiments on HPC clusters using the R package [batchtools](https://github.com/mllg/batchtools), which provides a convenience layer for interoperating with different scheduling systems like Slurm or LSF in a standardised way.
---

# Large-Scale Benchmarking: OpenML and Batchtools {#sec-ls-benchmarking}

{{< include _setup.qmd >}}

```{r, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

A common benchmarking scenario in machine learning is to compare a set of Learners $L_1, ..., L_n$ by evaluating their performance on a set of tasks $T_1, ..., T_k$ using resamplings $R_1, ..., R_k$ and performance measure $M$.
We are then interested in comparing the performance of learners $L_i$ with respect to measure $M$ on the tasks $T_j$.

The whole benchmark experiment consists of many individual resampling experiments, each defined by a triple $(L_i, T_j, R_j)$.
Running it consists of running each of these experiments and evaluating the results using the measure $M$.
This can be summarized like in @tbl-ml-benchmark below, where we summarize the task-resampling combination as a Problem.

| Learner   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment. {#tbl-ml-benchmark}

The table above should seem familiar as it is the result of calling the `$aggregate()` method on a `r ref("BenchmarkResult")`.
The code below, compares a classification and partition tree with a random forest using a simple holdout resampling and three classification tasks.

```{r}
library(mlr3learners)

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  lrns(c("classif.rpart", "classif.ranger")), 
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

bmr$aggregate(msr("classif.acc"))
```

Empirical evaluation of machine learning methods requires not only software tools, but also datasets.
Furthermore, if the computational experiments are large, there is a need to run them on high performance computing (HPC) clusters.
This chapter covers how to

1. easily find relevant datasets and tasks, and 
1. run large-scale benchmark experiments on HPC clusters.

The first point will be addressed with the help of the OpenML platform by @openml2013 and its interface package `r ref_pkg("mlr3oml")`.
For the second issue, we will show how the R package `r ref_pkg("batchtools")` (@batchtools) and its mlr3 integration `r ref_pkg("mlr3batchmark")` can greatly simplify the effort required to interact with a scheduling system on an HPC cluster.

**OpenML**

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of the earlier introduced problems $P_j$ is tantamount.
For the machine learning case it is therefore helpful to

1. have convenient access to a large collection of datasets and be able to filter them for specific properties, and
2. be able to easily share these datasets with others so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.


`r link("https://www.openml.org/", "OpenML")` is a platform that facilitates the sharing and dissemination of machine learning research data and - among many other things - satisfies the two desiderata mentioned above.
Like mlr3, OpenML is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface (@feurer2021openml).
Its goal is to make it easier for researchers to find the data they need to answer the questions they pose.
Its design was guided by the [FAIR](https://www.go-fair.org/fair-principles/) principles, which stand for **F**indability, **A**Accessibility, **I**Interoperability and **R**Reusability.
The purpose of these principles is to make scientific data and results more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.

More specifically, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible via a REST API or via the web interface.

In this chapter we will cover the main features of OpenML and how to use them via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform and how to do it.

OpenML has several types of objects that can be shared and accessed via the website or the API.
The following is a brief summary of the OpenML entities that we will cover:

*   [**OML Dataset**](https://www.openml.org/search?type=data\&status=active): A (usually tabular) dataset with additional metadata.
    The latter may include a description of the dataset and its properties, a licence, or meta-features of the data.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("DataBackend", text = "mlr3::DataBackend")`.
*   [**OML Task**](https://www.openml.org/search?type=task\&sort=runs): A machine learning task, i.e. a concrete problem specification on an OML dataset.
    This includes a split into a training and a test set, which differs from the mlr3 `r ref("Task")` definition.
    For this reason it can be converted to either `r ref("Task", text = "mlr3::Task")` or `r ref("Resampling", text = "mlr3::Resampling")`.
*   [**OML Task Collection**](https://www.openml.org/search?type=study\&sort=tasks_included\&study_type=task): A container object containing tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 (@bischl2021openml), which is a curated collection of classification tasks.

For example, the dataset with ID 31 is the "credit-g" dataset and can be accessed through the link <https://openml.org/d/31>.

Note that while OpenML also contains representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
While the `r ref_pkg("mlr3oml")` package also supports working with these objects, datasets and tasks are what OpenML is currently most used for.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.

**Batchtools**

Once we have set up the experiment design as in @tbl-ml-benchmark, the next step is to run it.
This is conceptually straight-forward, because we are facing an embarrassingly parallel problems.
Not only are the resample experiments $E_{i, j}$ independent, but even their individual iterations  $E^1_{i, j}, ..., E^{n_j}_{i, j}$ can be run independently. Here, $n_j$ refers to the number of resampling iterations of $R_j$.
However, if the experiment is large, parallelization via `r ref_pkg("future")` might not be enough and usage of a HPC cluster is required.
While access to such clusters is common, the effort required to operate these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework for conveniently running large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.
In @sec-batchtools we will teach you how to use the package.

## OpenML {#sec-openml}

In this section we show how to search, load, and work with OpenML objects using the interface package `r ref_pkg("mlr3oml")`.

### Dataset {#sec-openml-dataset}

Arguably the most important entity on OpenML is the dataset.
When someone wants to access datasets on OpenML, the two possible situations are that they want to

1.  access specific datasets whose IDs they know, or
2.  find datasets with specific properties.

We start with the first scenario.
As an exemplary use-case, we might be interested in how the random forest implementation in `r ref_pkg("ranger")` compares to modern AutoML tools.
After some research, one might find the AutoML benchmark \[@amlb2022], where 9 AutoML systems are compared on over 100 classification and regression problems.
Our goal is then to compare the performance of the random forest with the results of this study.
By using the same tasks as in the AutoML benchmark, we avoid having to reevaluate the AutoML systems on a new set of tasks.
This will serve as the guiding example through this chapter.

Since the AutoML benchmark uses datasets from OpenML, we can easily retrieve these datasets using their IDs.
As an example, we can access the dataset with ID 1590.
We load it into R using the function `r ref("mlr3oml::odt")`, which returns an object of the class `r ref("OMLData")`.

```{r}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about 48842 adults -- such as their *age* or *education* -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*.
The major difference between a `r ref("OMLData")` object and a typical `data.frame` in R is that the former comes with additional metadata that is accessible through its fields.
This might include a licence or data *qualities*, which are properties of the datasets.

```{r}
odata$license
odata$qualities[1:4, ]
```

<!-- odata$qualities["NumberOfFeatures", value] -->

The underlying dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```

:::{.callout-note}
When the code snippets from this section are executed, various objects have to be downloaded from the OpenML server.
Many objects can be cached so that they don't have to be downloaded each time they are used.
This can be enabled by setting the `mlr3oml.cache` option to either `TRUE` or a specific path to be used as the cache folder.
:::

As we have now loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The `r mlr3` class that comes closest to the OpenML Dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.
This is a reoccuring theme throughout this section, i.e. the OpenML and mlr3 objects are well interoperable.

```{r}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create a `r ref("mlr3::Task")` from the `r ref("DataBackend")` and use the mlr3 toolbox as usual.

```{r}
task = as_task_classif(backend, target = "class")
task
```

However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
I.e. we might be second of the two scenarios outlined earlier, where we first have to find datasets matching our requirements.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for filtering the existing dataset.
The `r ref("mlr3oml::list_oml_data")` allows to execute such requests from R.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(1, 4), 
  number_instances = c(100, 1000)
)
```

We can inspect the returned `data.table`, where we only show a subset of the columns for readability.
By looking at the table we confirm that indeed only datasets with the specified properties were returned.

```{r}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-info}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML API documentation: <https://www.openml.org/apis>.
:::

### Task {#sec-openml-task}

While the previous section showed how to access, find and convert datasets, we will now focus on OpenML tasks that are built on top of the dataset object.
Similar to mlr3, OpenML has different types of tasks, such as regression and classification.
And while we ignored this issue in the previous section, simply using the same datasets from the AutoML benchmark is not enough to allow for a cross-study comparison.
Other important aspects include the choice of features and how the data was split into training and test sets.
Fortunately, the AutoML benchmark shares not only the data but also the task IDs.

The task associated with the adult data from earlier has task ID 359983 and can be accessed via <https://openml.org/t/359983>.
We can load it into R using the `r ref("mlr3oml::otsk")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590 from earlier.
In addition to the data ID, the printed output also tells us the task type, the target variable^\[this may differ from the default target used in the previous section] and the estimation procedure.

```{r}
otask = otsk(359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.

```{r}
head(otask$task_splits)
```

The OpenML task can be converted to either an mlr3 `r ref("Task")` or an instantiated `r ref("ResamplingCustom")`.
For the first of these we will use the `r ref("as_task")` converter function.

```{r}
task = as_task(otask)
task
```

The resampling (instantiated on the task created above) can be created using the `r ref("as_resampling")` converter.

```{r}
resampling = as_resampling(otask)
resampling$task_hash == task$hash

resampling
```

As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors.

```{r}
resampling = rsmp("oml", task_id = 359983)
task = tsk("oml", task_id = 359983)
```

Continuing with our original goal of comparing the random forest implemented in `ranger` with modern AutoML tools, we define the learner of interest.
As the adult dataset contains missing values, we perform an out of range imputation before fitting the model using `r ref_pkg("mlr3pipelines")`.
We suppress the output during training by specifying `verbose = FALSE`.

```{r}
learner = po("imputeoor") %>>% lrn("classif.ranger", verbose = FALSE)
learner
```

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- learner = as_learner(po("imputeoor") %>>% lrn("classif.ranger")) -->

<!-- if (file.exists(file.path(getwd(), "openml", "rr.rds"))) { -->

<!--   rr = readRDS(file.path(getwd(), "openml", "rr.rds")) -->

<!-- } else { -->

<!--   rr = resample(task, learner, resampling) -->

<!--   saveRDS(rr, file.path(getwd(), "openml", "rr.rds")) -->

<!-- } -->

<!-- rr$aggregate(msr("classif.acc")) -->

<!-- ``` -->

We can then run the experiment as usual by calling `r ref("resample")`.

```{r}
rr = resample(task, learner, resampling)
rr$aggregate(msr("classif.acc"))
```

Because we used the same task definition and data-splits as in the AutoML benchmark, the resulting `r ref("ResampleResult")` is comparable with the results from the AutoML benchmark, that we can e.g. access through its website <https://openml.github.io/automlbenchmark/>.
The highest accuracy achieved on the adult dataset at the time of writing this book was achieved by the TPOT AutoML system (@olson2016tpot) with score of 88.27%, unsurprisingly beating our vanilla random forest.

:::{.callout-warning}
It is important to ensure that not only the `r ref("ResampleResult")` is comparable with the results of a previous study, but also that the same `r ref("Measure")` definition is used.
:::

While we have now learned how to access and use OpenML tasks with known IDs, the second common scenario is when one wants to find specific tasks on the OpenML website.
As an example, suppose we are only interested in binary classification problems.
We can run this query by using the `r ref("mlr3oml::list_oml_tasks")` function and specifying the `task_type` and `number_classes` arguments.
We limit the response to 5 and show only selected columns for readability.

```{r}
otasks = list_oml_tasks(
  type = "classif", 
  number_classes = 2, 
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

We can confirm that only classification tasks with two classes are contained in the response.

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the task collection, which bundles existing OpenML tasks in a container object.
This allows for the creation of *benchmark suites*, which are curated collections of tasks, usually satisfying certain quality criteria.
Many research areas have such agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time.
One example for such a benchmark suite is the aforementioned AutoML benchmark that is used to compare existing AutoML methods.
The classification tasks of this benchmark suite are contained in the collection with ID 271, which can be accessed through <https://openml.org/s/271>.
Other benchmark suites that are available on OpenML are e.g. the OpenML CC-18 which contains curated classification tasks (@bischl2021openml) or a benchmark for tabular deep learning which includes regression and classification problems (@grinsztajn2022why).

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML). 
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output informs us that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r}

otask_collection
```

We can get an overview of these tasks by accessing the `$tasks` field.
For compactness, we only show a subset of the columns.

```{r}
otask_collection$tasks[, .(id, data, target, task_splits)]
```

Coming back to our original goal of comparing the `ranger` random forest implementation with existing AutoML systems, the next step would be to evaluate the it on the whole benchmark suite as opposed to only the *adult* task from above.
To do this, we first need to create the tasks and resamplings from the collection.
If we wanted to get *all* tasks and resamplings, we could achieve this using the converters `r ref("as_tasks")` and `r ref("as_resamplings")`.
In order to keep the runtime manageable, we only use a selected subset of 6 of those tasks. 

<!-- And also because the albert dataset fails ... -->

```{r}
#| eval: false
ids = otask_collection$task_ids[c(1, 5, 6, 8, 9, 10)]
tasks = lapply(ids, function(id) tsk("oml", task_id = id))
resamplings = lapply(ids, function(id) rsmp("oml", task_id = id))

learner = lrn("classif.ranger")
```

```{r}
#| echo: false

learner = lrn("classif.ranger")

if (file.exists(file.path(getwd(), "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(getwd(), "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(getwd(), "openml", "resamplings.rds"))
}

if (file.exists(file.path(getwd(), "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(getwd(), "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(getwd(), "openml", "tasks.rds"))
}
```

Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality, which creates an experiment design from the cartesian product of all tasks, learners and resamplings.
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml")` function which can be used in such a scenario.

```{r}
large_design = benchmark_grid_oml(tasks, learner, resamplings)
large_design
```

Because this is already a relatively large experiment (especially if we used all tasks), we will use this as the starting point for illustrating how the R package `r ref_pkg("batchtools")` can be used to run such an experiment on a HPC cluster.

## Batchtools {#sec-batchtools}

In this section we are concerned with the question of how to execute large-scale experiments on HPC clusters.
We will start by giving a brief summary of the HPC basics and will then show how the R packages `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` simplfy the interaction with such clusters.

### HPC Basics {#sec-hpc-basics}

<!-- Explain what a HPC cluster is -->
A High-Performance Computing (HPC) cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used to solve complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
A HPC cluster typically consists of multiple compute nodes, each with multiple CPU and GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously, hence making it useful for benchmarking.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a scheduling system like Slurm (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the resources.

@fig-hpc below contains a rough sketch of such an architecture.
Multiple users have access to the cluster -- in this case Ann and Bob -- and can add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue.
One such instruction -- representing one entry in the queue -- will be referred to as a **computational job**.
The scheduling system controls when these computational jobs will be executed.
Apart from submitting, other essential operations are the killing and querying of computational jobs.

```{r}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

<!-- What do we want to do with the HPC cluster -->

When benchmarking machine learning algorithms, we are interested in the execution of embarrassingly parallel jobs.
Using the terminology from the introduction, we have independent experiments $E_{i, j} = (A_i, P_j)$ (see @tbl-ml-benchmark) that we want to execute.
The notion of an experiment does not have to coincide with a computational job, e.g. one computational job can consist of the execution of more than one experiment.

Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. users rely on self-discipline to ensure a clean folder structure

In the next section, we will show how `r ref_pkg("batchtools")` mitigates these problems.

### Benchmarking on a HPC Cluster {#sec-hpc-benchmark}


As a first example, we use the random forest experiment where we have left off at the end of @sec-openml-collection.
We will first show how one could use `r ref_pkg("batchtools")` to execute such an experiemnt on a HPC cluster.
Afterwards we will show how the `r ref_pkg("mlr3batchmark")` package can be used as a convenience layer to simplify the process even further.


<!-- What we wanna do conceptually -->
Our goal now is to run the benchmark design shown below on a HPC cluster. 
You may imagine this to be a large study, such that a `benchmark()` call and parallelization via `r ref_pkg("future")` would not result in a reasonable runtime.

```{r}
large_design
```

As we have mentioned already in the introduction, the execution of this benchmark experiment is an embarrassingly parallel problem. 
We have defined an experiment $E_{i, j} = (L_i, T_j, R_j)$ as one resample experiment defined by a learner, task and resampling.
Each resampling experiment can be further split up into iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ are number of iterations of resampling $R_j$.
One such resmapling iteration $E^l_{i, j}$ defines a computational workload that is independent of all other iterations and will constitute a batchtools *job*, which is to be distinguished from a *computational job* which we have already defined earlier.

The first step is always to create an (or load an existing) experiment registry, using the function `r ref("batchtools::makeExperimentRegistry")` (or `r ref("batchtools::loadRegistry")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

:::{.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry. 
The R package `r ref_pkg("batchtools")` also has a different registry object that constructed with `r ref("batchtools::makeRegistry")` that we will not cover in this book.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry stores the:

* job definitions
* log outputs and status of running and finished jobs
* job results
* "cluster function", which defines the interaction with the scheduling system

While the first three bullet points should be self-explanatory, the latter needs some explanation.
By configuring the scheduling system through a "cluster function", it is possible to make unify interaction with a scheduling system (like submitting jobs) independent on the scheduling software.
However, we will first focus on defining the experiments and will only come back to the "cluster function" once we will continue with submission of computational jobs to the HPC cluster.

We create a registry using a temporary directory by specifying `file.dir = NA` below, but we could also provide a proper path to be used as the registry folder.
Per default, the interactive cluster function is chosen, which allows to experiment with `r ref_pkg("batchtools")` locally.
We also set a global seed for the registry, and all 

```{r, cache = FALSE}
library(mlr3batchmark)
library(batchtools)

reg = makeExperimentRegistry(file.dir = NA, seed = 1)
```

All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object from above.
For that reason, we do not pass it explicitly.

When printing the registry, we see that that (unsurprisingly) there are 0 Algorithms, Problems, and Jobs registered.
We are also informed that the "Interactive" cluster function is used and about the path of the working directory that would be used when running cluster jobs. 

:::{.callout-info}
Due to historic reasons, the terms `Job` and `Experiment` are very similar in `r ref_pkg("batchtools")` and for our purposes in this chapter they are interchangeable.
:::

```{r}
reg
```

The next step is to populate the registry with algorithms, problems.
These can then be used to define the experiments.
To understand their purpose, we need to understand how `r ref_pkg("batchtools")` characterizes an experiment (aka job): 

Experiment $E$ is defined by applying Algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ using problem parameters $\xi_P$.

In our case, one experiment will consist of conducting the $l$-th resampling iteration of learner $L_i$ on task $T_j$ using resampling $R_j$.
This formalization is independent of machine learning and very generally applicable. 
In the following, the goal is therefore to reformulate a resampling iteration $E^l_{i, j}$ in terms of these general concepts of problem, algorithm, and their parameters.

:::{.callout-note}
It has to be stressed that we only show one way, how to run `r ref_pkg("mlr3")` experiments using `r ref_pkg("batchtools")`.
There are also other ways to do this and what is best, depends on the circumstances.
:::

We can register a problem, by calling `r ref("batchtools::addProblem")` which -- besides the registry, seed and caching options -- takes the following arguments: 

* `name`, to identify the problem
* `data` to represent the static data part of a problem 
* `fun`, which takes in the `data` and problem parameters $\xi_P$ and returns a concrete problem instance

We register the first task-resampling combination using the task ID as the name.
The `data` is the static part on which the algorithm will work, while the `fun` creates a specific problem instance from the `data` and additional problem parameters.
In this case, the problem parameter $\xi_P$ is the resampling iteration.

```{r, cache = FALSE}
task = large_design$task[[1L]]
task
resampling = large_design$resampling[[1L]]

addProblem(
  name = task$id,
  data = list(task = task, resampling = resampling), 
  fun = function(data, iteration, ...) {
    list(
      task = data$task, 
      resampling = data$resampling, 
      iteration = iteration
    )
  }
)
```

When calling `r ref("batchtools::addProblem")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

```{r}
reg$problems
```


The next step is to register the algorithm we want to run.
We can add an algorithm by calling `r ref("batchtools::addAlgorithm")`.
Besides the registry, it takes in the arguments: 

* `name` to identify the algorithm
* `fun` which takes in the problem instance and parameters $\xi_A$ and defines what is computed when running an experiment. Its output is the experiment result.

We will call the algorithm that we add `run_learner` and it will take a learner as an argument. 
It receives a problem instance -- defined by applying the problem `fun` to a problem `data` -- and executes one iteration of a resampling and returns the learner's state and the predictions.

```{r, cache = FALSE}
addAlgorithm(
  "run_learner", 
  fun = function(instance, learner, ...) {
    library("mlr3verse")
    resampling = instance$resampling
    task = instance$task
    iteration = instance$iteration

    train_ids = resampling$train_set(iteration)
    test_ids = resampling$test_set(iteration)

    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    list(state = learner$state, prediction = prediction)
  }
)

reg$algorithms
```

:::{.callout-note}
Both the problem and algorithm `fun` also receive other arguments during execution.
These must either be explicitly named in the definitions of the algorithm and problem `fun`, or the argument `...` must be present.
For a more detailed explanation, we refer to the `r ref_pkg("batchtools")` docs.
:::

As we have now defined a problem and an algorithm, we can define experiments with concrete algorithm and problem parameters $\xi_A$ and $\xi_B$.
We can do this using the `r ref("batchtools::addExperiments")` function, which takes in the arguments

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem. 

* `algo.designs`, a named list of data frames (or ‘data.table’). The name must match the algorithm name while the column names correspond to parameters of the algorithm.

We now add all 10 resampling iterations as experiments.

```{r, cache = FALSE}
l = as_learner(
  ppl("robustify") %>>% lrn("classif.ranger")
)
addExperiments(
  prob.designs = list(yeast = data.table(iteration = 1:10)), 
  algo.designs = list(run_learner = data.table(learner = list(l)))
)
```

A job's seed is  defined by incrementing this initial seed when the job is added to the registry.
We can now also print the registry again, and see that indeed the algorithm, problem and experiments were added.

```{r, cache = FALSE}
reg
```

To get more detailed information, we can use `r ref("batchtools::getJobTable")`.
This returns a table summarizing information of all added jobs.
Among many other things, we see that each job has a unique ID that can be used to identify it.

```{r}
job_table = getJobTable()


job_table[1, c("job.id", "algorithm", "algo.pars", "problem", "prob.pars")]
```




```{r}
summarizeExperiments()
```



The complete workflow of adding algorithms, problems, and experiments is summarized in the image below.

```{r}
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

As we have no defined an experiment, we can run it.
We have defined 10 jobs, each representing one resampling iteration.
This means that we now have to define how the jobs are turned into computational jobs. 
This includes

1. specifying resource requirements for the scheduler and
1. possibly grouping multiple jobs into one computational job.

The resource requirements that can be specified depend on the cluster function that is set in the registry.
We earlier left the cluster function at its default value, which was the "Interactive" cluster function which is primarily intended for exploration of the package and testing of experiments.

Because we are now starting with interaction with the scheduling system, the time has come to properly configure it.
We are assuming for this example, that we are operating a Slurm cluster.
We therefore set the cluster function to a simple slurm cluster function as contructable by `r ref("batchtools::makeClusterFunctionsSlurm")`.


```{r}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm("slurm-simple")
```

```{r}
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

We update the `$cluster.function` field of the registry and save the registry, which is necessary in this case.

```{r}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry()
```

Assuming that we are running this program now on the login node of the Slurm cluster, we can call `r ref("batchtools::submitJobs")` to add the previously defined experiments to the queue of the Slurm scheduler.

The most important arguments of `submitJobs()` besides the registry are: 

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job, and 
* `resources`, which is a named list specifying the resource requirements with which the computational jobs will be run.

The ids we refer to here are the ids that can e.g. be accessed as the `job.id` column from the job table obtained earlier.
```{r}
job_table$job.id
```

In our case, we show how we can group five jobs into one computational jobs.
In addition to the grouping, we also specify the number of CPUs per computational job to 1, the walltime to 1 hour and the RAM limit to 8 GB.

```{r, cache = FALSE}
#| eval: false
chunks = data.table(job.id = 1:10, chunk = rep(1:2, each = 5))
chunks


submitJobs(chunks, resources = list(ncpus = 1, walltime = 3600, memory = 8000))
```


```{r, cache = FALSE}
#| echo: false
#| output: false
chunks = data.table(job.id = 1:10, chunk = rep(1:2, each = 5))

submitJobs(chunks)
waitForJobs()
```

```{r}
```



While the cluster jobs are being processed, `r ref_pkg("batchtools")` exposes many useful functions that allow the user to access information about the running cluster jobs.
This includes e.g. `r ref("batchtools::getStatus")` to get the status of the running job, `r ref("batchtools::showLog")` to inspect the logs or `r ref("batchtools::grepLogs")` to search the log files.

:::{.callout-tip}
A good approach is to submit the jobs within a R Session that is running persistently through different SSH session.
One option is to use TMUX (Terminal Multiplexer).
:::

When a cluster jobs finishes, it stores its return value in the registry folder.
Once everything is done, we can retrieve the job results using the `r ref("batchtools::loadResult")` function which takes in a job id as argument `id`, as well as a registry. 
It outputs the objects returned by the `fun` of the algorithm, which is in this case a list containing the learner's state and the prediction for the given resampling iteration.



```{r}
waitForJobs()
results = lapply(job_table$job.id, loadResult)
names(results[[1L]])
results[[1L]]$prediction
```

If we wanted to run the whole benchmark experiment defined in `large_design`, we would simply have to add the remaining five problems and then add the experiments identically to how we have just shown.
Instead of doing this, we will now however show how this whole process can be simplified using the `r ref_pkg("mlr3batchmark")` package.
To illustrate this, we start with a new registry.


```{r}
reg = makeExperimentRegistry(NA, seed = 1)
```

The whole process, of adding the algorithm, problems and experiments can be done by swapping the common `r ref("mlr3::benchmark")` call with the `r ref("mlr3batchmark::batchmark")` function.

```{r}
#| output: false
batchmark(large_design)
```

This now does something similar as the steps that we have just shown earlier.
Printing the registry, we see that one algorithm, six problems and 60 experiments are registered.

```{r}
reg
```

We can now submit them like before using `r ref("batchtools::submitJobs()")`. 
Once they are done, we can either load the individual results using `r ref("loadResult")`, or the complete `r ref("BenchmarkResult")` by calling `r ref("mlr3batchmark::reduceBatchmarkResult")`.


```{r}
#| echo: false
if (file.exists(file.path(getwd(), "openml", "bmr_large.rds"))) {
  bmr = readRDS(file.path(getwd(), "openml", "bmr_large.rds"))
} else {
  bmr = benchmark(large_design)
  saveRDS(bmr, file.path(getwd(), "openml", "bmr_large.rds"))
}

```


```{r}
#| eval: false
bmr = reduceBatchmarkResult()
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark")` on the `large_design` above.

```{r}
bmr
```


<!-- TODO: Look at the style guide and apply it here. -->
<!---->
<!-- TODO: Check that spelling is correct and put everything through deepl. -->
<!---->
<!-- TODO: Render as pdf and do own review -->
<!---->
<!-- TODO: Send it to Marc, Michel, Martin and Bernd -->
<!---->
<!-- TODO: Briefly cover seeding:  -->
<!-- TODO: Specify cache depdenencies and not force cache = FALSE:  -->
<!-- TODO: Squash all the commits to avoid uploading the large files :  -->


## Conclusion

In this chapter we have learned about two tools -- OpenML and batchtools -- that both aid in the design and execution of (large-scale) benchmark experiments. 
OpenML is a well-structured repository for machine learning research data that -- among many other things -- allows to access, share and filter datasets, tasks, and task collections.
Furthermore, we have shown how the R package `r ref_pkg("batchtools")` and its mlr3 connector `r ref_pkg("mlr3batchmark")` can considerably simplify the execution of large-scale benchmark experiments on HPC clusters.

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | OpenML Dataset                                                      |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | OpenML Task                                                         |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | OpenML Collection                                                   |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new                                                        |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a Problem                                                  |
| `r ref("addAlgorithm()")`           |-                           | Register an algorithm                                               |
| `r ref("addExperiments()")`         |-                           | Regiter experiments using existing algorithms and problems          |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduling system                                |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getJobStatus()")`           |-                           | Get the status of existing jobs                                     |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")`  |-                           | Load finished jobs as a benchmark result                            |

Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-ls-benchmarking}

### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")` and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.

## Exercises

1. TODO: OpenML
1. TODO: batchtools

