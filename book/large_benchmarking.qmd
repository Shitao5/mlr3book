---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers need not only software tools but also datasets.
  In addition, large-scale computational experiments require the use of high-performance computing (HPC) clusters.
  [OpenML](http://www.openml.org/) is an open platform that facilitates the sharing and dissemination of machine learning research data and addresses the first of these issues.
  It provides unique identifiers and standardised (meta)data formats that make it easy to find relevant datasets by querying for specific properties, and to share them using their identifier.
  The first part of this chapter covers the basics of OpenML and shows how it can be used via the mlr3oml interface package.
  We then show how to simplify the execution of benchmark experiments on HPC clusters using the R package batchtools, which provides a convenience layer for interoperating with different scheduling systems like Slurm or LSF.
---

# Large-Scale Benchmarking: OpenML and Batchtools {#sec-large-benchmarking}

{{< include _setup.qmd >}}

<!-- This chapter is a little annoying to render, because of the dependencies in the chunks. -->
<!-- If weird errors happen, delete the cache and render again. -->

```{r large_benchmarking-001_experiments-001, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

A common benchmarking scenario in machine learning is to compare a set of Learners $L_1, ..., L_n$ by evaluating their performance on a set of tasks $T_1, ..., T_k$ using resamplings $R_1, ..., R_k$ and performance measure $M$.
Such an experiment usually tries to answer which learner performs best on the given tasks with respect to the performance measure.

Such an experiment consists of many individual resampling experiments, each defined by a triple $E_{i, j} = (L_i, T_j, R_j)$.
Running it consists of running each of these experiments and then evaluating the results using the measure $M$.
@tbl-ml-benchmark illustrates such an experiment.

| Algorithm   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment. {#tbl-ml-benchmark}

The table above should seem familiar as it is the result of calling the `$aggregate()` method on a `r ref("BenchmarkResult")`.
We will now revisit how such an experiment can be defined and run in mlr3.

The code below, compares a classification and partition tree with a random forest using a simple holdout resampling and three classification tasks.

```{r large_benchmarking-002_experiments-002}
library(mlr3learners)

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  lrns(c("classif.rpart", "classif.ranger")),
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

bmr$aggregate(msr("classif.acc"))
```

What we have ignored so far are the questions of how to obtain datasets for such experiments, and what to do, when the experiments are large and requires execution on a high performance computing (HPC) cluster.
This chapter covers how to

1. easily find relevant datasets and tasks, and
1. run large-scale benchmark experiments on HPC clusters.

The first point will be addressed with the help of the OpenML platform [@openml2013] and its interface package `r ref_pkg("mlr3oml")`.
For the second issue, we will show how the R package `r ref_pkg("batchtools")` [@Batchtools] and its mlr3 integration `r ref_pkg("mlr3batchmark")` can greatly simplify the effort required to interact with a scheduling system on a HPC cluster.

**OpenML**

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of the datasets and tasks is tantamount.
It is therefore helpful to

1. have convenient access to a large collection of datasets and tasks and be able to filter them for specific properties, and
1. be able to easily share these with others, so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.


`r link("https://www.openml.org/", "OpenML")` is a platform that facilitates the sharing and dissemination of machine learning research data and - among many other things - satisfies the two desiderata mentioned above.
Like mlr3, OpenML is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface (@feurer2021openml).
Its goal is to make it easier for researchers to find the data they need eo answer the questions they have.
Its design was guided by the `r link("https://www.go-fair.org/fair-principles/", "FAIR")` principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The purpose of these principles is to make scientific data and results more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.

More specifically, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible via a REST API or via the web interface.

In this chapter we will cover the main features of OpenML and how to use them via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform and how to do it.

OpenML supports different types of objects and following is a brief summary of the OpenML entities that we will cover:

*   `r index("OpenML Dataset")`: A (usually tabular) dataset with additional metadata.
    The latter includes a description of the dataset and its properties, a licence, or meta-features of the data.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("DataBackend", text = "mlr3::DataBackend")`.
*   `r index("OpenML Task")`: A machine learning task, i.e. a concrete problem specification on an OpenML dataset.
    This includes a split into a training and a test set, which differs from the mlr3 `r ref("Task")` definition.
    For this reason it can be converted to either a `r ref("Task", text = "mlr3::Task")` or `r ref("Resampling", text = "mlr3::Resampling")`.
*   `r index("OpenML Task Collection")`: A container object that allows to group tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 (@bischl2021openml), which is a curated collection of classification tasks.

A simple example for an OpenML object is the dataset with ID 31 -- the well known "credit-g" data -- which can be accessed through the link <https://openml.org/d/31>.
Note that while OpenML also supports other objects such as representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.

**Batchtools**

In @tbl-ml-benchmark we have displayed a common experiment design for machine learning benchmarks. 
Once such a design is finalized the next step is to run and parallelize it.
Such experiments are conceptually straight-forward to parallelize, because we are facing an embarrassingly parallel problem.
Not only are the resample experiments independent, but even their individual iterations can be run independently.
However, if the experiment is large, parallelization via `r ref_pkg("future")` might not be enough and usage of a HPC cluster is required.
While access to such clusters is common, the effort required to operate these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework for conveniently running large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.
In @sec-batchtools we will teach you how to use the package.

## OpenML {#sec-openml}

In this section we show how to search, load, and work with OpenML objects using the interface package `r ref_pkg("mlr3oml")`.

### Dataset {#sec-openml-dataset}

Arguably the most important entity on OpenML is the dataset.
When someone wants to access datasets on OpenML, the two possible situations are that they want to

1.  access specific datasets whose IDs they know, or
2.  find datasets with specific properties.

We start with the first scenario.
As an exemplary use-case, we might be interested in how the random forest implementation in `r ref_pkg("ranger")` compares to modern AutoML tools.
After some research, one might find the AutoML benchmark [@amlb2022], where many AutoML systems are compared on over 100 classification and regression problems from OpenML.
Our goal is then to compare the performance of the `ranger` learner with the results of this study.
This will serve as the guiding example throughout this section.

Since the AutoML benchmark uses datasets from OpenML, we can easily retrieve these datasets using their IDs.
As an example, we will focus on the dataset with `r link("https://openml.org/d/1590", "ID 1590")`.
We load it into R using the function `r ref("mlr3oml::odt()")`, which returns an object of the class `r ref("OMLData")`.

```{r large_benchmarking-003_experiments-003}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about 48842 adults -- such as their *age* or *education* -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*.
The major difference between a `r ref("OMLData")` object and a typical `data.frame` in R is that the former comes with additional metadata that is accessible through its fields.
This might include a licence or data qualities, which are properties of the datasets.

```{r large_benchmarking-004_experiments-004}
odata$license
odata$qualities[1:4, ]
```

<!-- odata$qualities["NumberOfFeatures", value] -->

The underlying dataset can be accessed through the field `$data`.

```{r large_benchmarking-005_experiments-005}
odata$data[1:2, ]
```

:::{.callout-tip}
When working with OpenML objects, these are downloaded from the OpenML server.
Many objects can be cached, which can be enabled by setting the `mlr3oml.cache` option to either `TRUE` or a specific path to be used as the cache folder.
:::

As we have now loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The mlr3 class that comes closest to the OpenML dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend()")`.
This is a recurring theme throughout this section, i.e. the OpenML and mlr3 objects are well interoperable.

```{r large_benchmarking-006_experiments-006}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create an mlr3 `r ref("Task")` from the data bakend and use the mlr3 toolbox as usual.

```{r large_benchmarking-007_experiments-007}
task = as_task_classif(backend, target = "class")
task
```

However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
I.e. we might be in the second of the two scenarios outlined earlier, where we first have to find datasets matching our requirements.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for filtering the existing dataset.
The `r ref("list_oml_data")` function allows to execute such requests from R.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r large_benchmarking-008_experiments-008}
odatasets = list_oml_data(
  limit = 5,
  number_features = c(1, 4),
  number_instances = c(100, 1000)
)
```

By looking at the table we confirm that indeed only datasets with the specified properties were returned.
Here, we only show a subset of the columns for readability.

```{r large_benchmarking-009_experiments-009}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-tip}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML `r link("https://www.openml.org/apis", "API documentation")`.
:::

### Task {#sec-openml-task}

While the previous section showed how to access, find and convert datasets, we will now focus on OpenML tasks that are built on top of the dataset object.
Similar to mlr3, OpenML has different types of tasks, such as regression and classification.
And while we ignored this issue in the previous section, simply using the same datasets from the AutoML benchmark is not enough to allow for a cross-study comparison.
Other important aspects include the choice of features and how the data was split into training and test sets.
Fortunately, the AutoML benchmark shares not only the data but also the task IDs.

The task associated with the adult data from earlier has task `r link("https://openml.org/t/359983", "ID 1590")`.
We can load it into R using the `r ref("mlr3oml::otsk()")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590 from earlier.
In addition to the data ID, the printed output also tells us the task type, the target variable^[this may differ from the default target used in the previous section] and the estimation procedure.

```{r large_benchmarking-010_experiments-010}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-011_experiments-011}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.

```{r large_benchmarking-012_experiments-012}
head(otask$task_splits)
```

The OpenML task can be converted to either an mlr3 `r ref("Task")` or an instantiated `r ref("ResamplingCustom")`.
For the first of these we can use the `r ref("as_task()")` converter function.

```{r large_benchmarking-013_experiments-013}
task = as_task(otask)
task
```

The resampling (instantiated on the task created above) can be created using the `r ref("as_resampling()")` converter.

```{r large_benchmarking-014_experiments-014}
resampling = as_resampling(otask)
resampling$task_hash == task$hash

resampling
```

:::{.callout-tip}
As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors.
:::

Continuing with our original goal of comparing the random forest implemented in `ranger` with modern AutoML tools, we define the learner of interest.
As the adult dataset contains missing values, we perform an out of range imputation before fitting the model using `r ref_pkg("mlr3pipelines")`.
We suppress the output during training by specifying `verbose = FALSE`.

```{r large_benchmarking-016_experiments-016}
learner = po("imputeoor") %>>% lrn("classif.ranger", verbose = FALSE)
learner
```

We can then run the experiment as usual by calling `r ref("resample")`.

```{r large_benchmarking-017_experiments-017}
rr = resample(task, learner, resampling)
rr$aggregate(msr("classif.acc"))
```

Because we used the same task definition and data-splits as in the AutoML benchmark, the resulting `r ref("ResampleResult")` is comparable with the results from the AutoML benchmark, that we can e.g. access through its `r link("https://openml.github.io/automlbenchmark/", "website")`.
The highest accuracy achieved on the adult dataset at the time of writing this book was achieved by the TPOT AutoML system [@olson2016tpot] with score of 88.27%, unsurprisingly beating our vanilla random forest.

<!-- :::{.callout-warning} -->
<!-- It is important to ensure that not only the `r ref("ResampleResult")` is comparable with the results of a previous study, but also that the same `r ref("Measure")` definition is used. -->
<!-- ::: -->
<!---->
While we have now learned how to access and use OpenML tasks with known IDs, the second common scenario is when one wants to find specific tasks on the OpenML website.
As an example, suppose we are only interested in binary classification problems.
We can run this query by using the `r ref("mlr3oml::list_oml_tasks()")` function and specifying the `task_type` and `number_classes` arguments.
We limit the response to 5 and show only selected columns for readability.

```{r large_benchmarking-018_experiments-018}
otasks = list_oml_tasks(
  type = "classif",
  number_classes = 2,
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

We can confirm that only classification tasks with two classes are contained in the response.

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the task collection, which bundles existing OpenML tasks in a container object.
This allows for the creation of *benchmark suites*, which are curated collections of tasks, usually satisfying certain quality criteria.
Many research areas have such agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time.
One example for such a benchmark suite is the aforementioned AutoML benchmark that is used to compare AutoML methods.
The classification tasks of this benchmark suite are contained in the collection with `r link("https://www.openml.org/search?type=study&study_type=task&id=271", "ID 271")`.
Other benchmark suites that are available on OpenML are e.g. the `r link("https://www.openml.org/search?type=study&study_type=task&id=99", "OpenML CC-18")` which contains curated classification tasks [@bischl2021openml] or a `r link("https://www.openml.org/search?type=study&sort=tasks_included&study_type=task&id=304", "benchmark for tabular deep learning")` [@grinsztajn2022why].

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r large_benchmarking-019_experiments-019}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML).
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output informs us that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r large_benchmarking-020_experiments-020}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r large_benchmarking-021_experiments-021}

otask_collection
```

We can get an overview of these tasks by accessing the `$tasks` field.
For compactness, we only show a subset of the columns.

```{r large_benchmarking-022_experiments-022}
otask_collection$tasks[, .(id, data, target, task_splits)]
```

Coming back to our original goal of comparing the `ranger` random forest implementation with existing AutoML systems, the next step would be to evaluate  it on the whole benchmark suite as opposed to only the *adult* task from above.
To do so, we first need to create the tasks and resamplings from the collection.
If we wanted to get all tasks and resamplings, we could achieve this using the converters `r ref("as_tasks()")` and `r ref("as_resamplings()")`.
In order to keep the runtime manageable, we only use a selected subset of 6 of those tasks.

<!-- And also because the albert dataset fails ... -->

```{r large_benchmarking-023_experiments-023}
#| eval: false
ids = otask_collection$task_ids[c(1, 5, 6, 8, 9, 10)]
tasks = lapply(ids, function(id) tsk("oml", task_id = id))
resamplings = lapply(ids, function(id) rsmp("oml", task_id = id))

learner = lrn("classif.ranger")
```

```{r large_benchmarking-024_experiments-024}
#| echo: false

learner = lrn("classif.ranger")

if (file.exists(file.path(getwd(), "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(getwd(), "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(getwd(), "openml", "resamplings.rds"))
}

if (file.exists(file.path(getwd(), "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(getwd(), "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(getwd(), "openml", "tasks.rds"))
}
```

Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality, which creates an experiment design from the cartesian product of all tasks, learners and resamplings.
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml")` function which can be used in such a scenario, where tasks and resamplings are paired.

```{r large_benchmarking-025_experiments-025}
large_design = benchmark_grid_oml(tasks, learner, resamplings)
large_design
```

Because this is already a relatively large experiment (especially if we used all tasks), we will use this as the starting point for illustrating how the R package `r ref_pkg("batchtools")` can be used to run such an experiment on a HPC cluster.

## Batchtools {#sec-batchtools}

In this section we are concerned with the question of how to execute large-scale experiments on HPC clusters.
We will start by giving a brief summary of the HPC basics and will then show how the R packages `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` simplfy the interaction with such clusters.

### HPC Basics {#sec-hpc-basics}

<!-- Explain what a HPC cluster is -->
A High-Performance Computing (HPC) cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used to solve complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
A HPC cluster typically consists of multiple compute nodes, each with multiple CPU and GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously, hence making it useful for benchmarking.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a scheduling system like Slurm (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the resources.

@fig-hpc contains a rough sketch of a HPC architecture.
Multiple users have access to the cluster -- in this case Ann and Bob -- and can add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue when being logged in on the head node.
One such instruction -- representing one entry in the queue -- will be referred to as a `r define("computational job")`.
The scheduling system controls when these computational jobs are executed.

```{r large_benchmarking-026_experiments-026}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

<!-- What do we want to do with the HPC cluster -->
Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. users rely on self-discipline to ensure a clean folder structure

In the next section, we will show how `r ref_pkg("batchtools")` mitigates these problems.


### Benchmarking on a HPC Cluster {#sec-hpc-benchmark}

As a guiding example, we use the random forest experiment where we have left off at the end of @sec-openml-collection.
We will first show how one could use `r ref_pkg("batchtools")` to execute such an experiment on a HPC cluster.
Afterwards we will show how the `r ref_pkg("mlr3batchmark")` package can be used to simplify the process even further.


<!-- What we wanna do conceptually -->
Our goal now is to run the benchmark design shown below (and defined at the end of @sec-openml-collection) on a HPC cluster.
You may imagine this to be a large study, such that a `benchmark()` call and parallelization via `r ref_pkg("future")` would not result in a reasonable runtime.

```{r large_benchmarking-027_experiments-027}
large_design
```

If we want to execute this experiment using `r ref_pkg("batchtools")`, we need to understand how the batchtools package conceptualizes benchmarking.
The central concept is the experiment or `r define("job", "Job or Experiment")`:

Experiment (Job $E$ is defined by applying Algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ with problem parameters $\xi_P$.

A benchmark experiments then consists of running many such experiments with different algorithm (parameters) and problem (parameters).
Each such Experiment $E$ is independent of all other experiments and constitutes the basic level of computation that can be parallelized.

If we want to utilize batchtools to execute the `large_design` from above, we therefore have to translate the mlr3 concepts into its language.
There is not a single right way to do this, and we here show one of many possible solutions.

In the introduction of this chapter, we have defined a ML benchmark experiment as running a number of resampling experiments $E_{i, j} = (L_i, T_j, R_j)$ that are defined by a learner, task and resampling.
While it might seem natural to define one such resampling experiment as a job, each resample experiment can be split even further, namely into its iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ are number of iterations of resampling $R_j$.
We will understand one resampling iteration as a job (aka. experiment).

Note that such a job does not have to coincide with the notion of a computational job defined in the previous section.
One computational job can consist of (sequentially) executing multiple jobs.

We will now continue with showing how to define the batchtools algorithms and problems in such a way that they allow to define each resampling iteration as a job.
The first step is always to create an (or load an existing) experiment registry, using the function `r ref("batchtools::makeExperimentRegistry")` (or `r ref("batchtools::loadRegistry")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

:::{.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry.
The R package `r ref_pkg("batchtools")` also has a different registry object that constructed with `r ref("batchtools::makeRegistry")` that we will not cover in this book.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry will store the:

* algorithm, problem, and job definitions
* log outputs and status of running and finished jobs
* job results
* `r define("cluster function", "Batchtools object that allows to configure the interaction with the scheduling system.")`, which defines the interaction with the scheduling system

While the first three bullet points should be relatively clear, the latter needs some explanation.
By configuring the scheduling system through a "cluster function", it is possible to make the interaction with the scheduling system independent on the scheduling software.
We will come back to this later.

We create a registry using a temporary directory by specifying `file.dir = NA` below, but we could also provide a proper path to be used as the registry folder.
We also set a global seed for the registry.

```{r large_benchmarking-028_experiments-028}
library(mlr3batchmark)
library(batchtools)

reg = makeExperimentRegistry(file.dir = NA, seed = 1)
```

When printing the registry, we see that that (unsurprisingly) there are 0 algorithms, problems, and jobs registered.
We are also informed that the "Interactive" cluster function is used and about the path of the working directory that would be used when running cluster jobs.

```{r large_benchmarking-029_experiments-029}
reg
```

The next step is to populate the registry with algorithms and problems, which we will then use to define the jobs.
We can register a problem, by calling `r ref("batchtools::addProblem()")` whose main arguments beside the registry are:

* `name`, to identify the problem
* `data` to represent the static data part of a problem
* `fun`, which takes in the `data` and problem parameters $\xi_P$ and returns a concrete problem instance

We register the first task-resampling combination using the task ID as the name.
The `data` is the static part on which the algorithm will work, while the `fun` creates a specific problem instance from the `data` and additional problem parameters.
We define the problem parameter $\xi_P$ is the resampling iteration. 
The problem `fun` takes in the static problem -- a task and resampling -- and returns the problem instance by adding the problem parameter, namely the resampling iteration. 

```{r large_benchmarking-030_experiments-030}
task = large_design$task[[1L]]
task
resampling = large_design$resampling[[1L]]

addProblem(
  name = task$id,
  data = list(task = task, resampling = resampling),
  fun = function(data, iteration, ...) {
    list(
      task = data$task,
      resampling = data$resampling,
      iteration = iteration
    )
  }, 
  reg = reg
)
```

:::{.callout-tip}
All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but we would not have to do it.
:::

When calling `r ref("batchtools::addProblem")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

```{r large_benchmarking-031_experiments-031}
reg$problems
```

The next step is to register the algorithm we want to run.
We can add an algorithm by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to identify the algorithm
* `fun` which takes in the problem instance and parameters $\xi_A$ and defines what is computed when running an experiment. Its output is the experiment result.

We will call the algorithm that we add `"run_learner"` and it will take a learner and the as a parameter.
It receives a problem instance and the learner, executes one iteration of a resampling and returns the learner's state and the predictions.

```{r large_benchmarking-032_experiments-032}
addAlgorithm(
  "run_learner",
  fun = function(instance, learner, ...) {
    library("mlr3verse")
    resampling = instance$resampling
    task = instance$task
    iteration = instance$iteration

    train_ids = resampling$train_set(iteration)
    test_ids = resampling$test_set(iteration)

    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    list(state = learner$state, prediction = prediction)
  }, 
  reg = reg
)

reg$algorithms
```

:::{.callout-note}
Both the problem and algorithm `fun` also receive other arguments during execution.
These must either be explicitly named in the definitions of the algorithm and problem `fun`, or the argument `...` must be present.
For a more detailed explanation, we refer to the `r ref_pkg("batchtools")` documentation.
:::

As we have now defined a problem and an algorithm, we can define experiments with concrete algorithm and problem parameters $\xi_A$ and $\xi_B$.
We can do this using the `r ref("batchtools::addExperiments()")` function, which takes in the arguments:

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames (or ‘data.table’). The name must match the algorithm name while the column names correspond to parameters of the algorithm.

We now add all 10 resampling iterations as experiments.

```{r large_benchmarking-033_experiments-033}
l = as_learner(
  ppl("robustify") %>>% lrn("classif.ranger")
)
addExperiments(
  prob.designs = list(yeast = data.table(iteration = 1:10)),
  algo.designs = list(run_learner = data.table(learner = list(l))), 
  reg = reg
)
```

Whenever an experiment is added, the global seed set in the beginning is incremented and set as the seed for the experiment to ensure reproducibility.
We can now also print the registry again, and see that indeed the algorithm, problem and experiments were added.

```{r large_benchmarking-034_experiments-034}
reg
```

To get more detailed information, we can use `r ref("batchtools::getJobTable")`.
This returns a table summarizing information of all added jobs.
Among many other things, we see that each job has a unique ID.

```{r large_benchmarking-035_experiments-035}
job_table = getJobTable(reg = reg)


job_table[1:2, c("job.id", "algorithm", "algo.pars", "problem", "prob.pars")]
```

The complete workflow of adding algorithms, problems, and experiments is summarized in the image below.

```{r large_benchmarking-037_experiments-037}
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

We have defined 10 jobs, each representing one resampling iteration.
It is now time, to translate these jobs into computational jobs and run them.
This includes:

1. specifying resource requirements for the scheduler and
1. possibly grouping multiple jobs into one computational job.

The resource requirements that can be specified depend on the cluster function that is set in the registry.
We earlier left the cluster function at its default value, which was the "Interactive" cluster function that is primarily intended for exploration of the package and testing of experiments.

Because we are now starting with interaction with the scheduling system, the time has come to properly configure it.
We are assuming for this example that we are operating a Slurm cluster.
We therefore set the cluster function to a simple Slurm cluster function that can be constructed using `r ref("batchtools::makeClusterFunctionsSlurm()")`.

```{r large_benchmarking-038_experiments-038}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm(template = "slurm-simple")
```

```{r large_benchmarking-039_experiments-039}
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

We update the `$cluster.function` field of the registry and save the registry, which is necessary in this case.

```{r large_benchmarking-040_experiments-040}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry()
```

Assuming that we are running this program now on the login node of the Slurm cluster, we can call `r ref("batchtools::submitJobs()")` to add the previously defined experiments to the queue of the Slurm scheduler.

The most important arguments of this functino besides the registry are:

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job, and
* `resources`, which is a named list specifying the resource requirements with which the computational jobs will be run.

The ids we refer to here are the ids that can e.g. be accessed as the `job.id` column from the job table obtained earlier.

```{r large_benchmarking-041_experiments-041}
job_table$job.id
```

In our case, we show how we can group five jobs into one computational jobs.
In addition to the grouping, we also specify the number of CPUs per computational job to 1, the wall time to 1 hour and the RAM limit to 8 GB.

```{r large_benchmarking-042_experiments-042}
#| eval: false
chunks = data.table(job.id = 1:10, chunk = rep(1:2, each = 5))
chunks


submitJobs(
  ids = chunks, 
  resources = list(ncpus = 1, walltime = 3600, memory = 8000), 
  reg = reg
)
```


```{r large_benchmarking-043_experiments-043}
#| echo: false
#| output: false
chunks = data.table(job.id = 1:10, chunk = rep(1:2, each = 5))

submitJobs(ids = chunks, reg = reg)
waitForJobs(reg = reg)
```

While the cluster jobs are being processed, `r ref_pkg("batchtools")` exposes many useful functions that allow the user to access information about the running cluster jobs.
This includes e.g. `r ref("batchtools::getStatus")` to get the status of the running job, `r ref("batchtools::showLog")` to inspect the logs or `r ref("batchtools::grepLogs")` to search the log files.

:::{.callout-tip}
A good approach is to submit the jobs within a R Session that is running persistently through different SSH session.
One option is to use TMUX (Terminal Multiplexer).
:::

When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using the `r ref("batchtools::loadResult()")` function which takes in a job id as argument `id`, as well as a registry.
It outputs the objects returned by the `fun` of the algorithm, which is in this case a list containing the learner's state and the prediction for the given resampling iteration.

```{r large_benchmarking-045_experiments-045}
results = lapply(job_table$job.id, loadResult)
names(results[[1L]])
results[[1L]]$prediction
```

If we wanted to run the whole benchmark experiment defined in `large_design`, we would simply have to add the remaining five problems and then add the experiments identically to how we have just shown.
Instead, we will now show how this whole process can be simplified using the `r ref_pkg("mlr3batchmark")` package.
For this, we will start with a new registry.
```{r large_benchmarking-046_experiments-046}
reg = makeExperimentRegistry(NA, seed = 1)
```

The whole process, of adding the algorithm, problems, and experiments can be done by swapping the common `r ref("mlr3::benchmark")` call with the `r ref("mlr3batchmark::batchmark")` function.

```{r large_benchmarking-047_experiments-047}
#| output: false
batchmark(large_design)
```

This does something similar as the steps that we have just shown earlier.
Printing the registry, we see that one algorithm, six problems and 60 experiments are registered.

```{r large_benchmarking-048_experiments-048}
reg
```

We can now submit them like before using `r ref("batchtools::submitJobs()")`.
Once they are done, we can either load the individual results using `r ref("loadResult")`, or the complete `r ref("BenchmarkResult")` by calling `r ref("mlr3batchmark::reduceBatchmarkResult")`.


```{r large_benchmarking-049_experiments-049}
#| echo: false
if (file.exists(file.path(getwd(), "openml", "bmr_large.rds"))) {
  bmr = readRDS(file.path(getwd(), "openml", "bmr_large.rds"))
} else {
  bmr = benchmark(large_design)
  saveRDS(bmr, file.path(getwd(), "openml", "bmr_large.rds"))
}

```


```{r large_benchmarking-050_experiments-050}
#| eval: false
bmr = reduceBatchmarkResult()
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark")` on the `large_design` above.

```{r large_benchmarking-051_experiments-051}
bmr
```

<!-- TODO: Render as pdf and do own review -->
<!-- TODO: Check that spelling is correct and put everything through deepl. -->
<!-- TODO: Send it to Marc, Michel, Martin and Bernd -->

## Conclusion

In this chapter we have learned about two tools -- OpenML and batchtools -- that both aid in the design and execution of (large-scale) benchmark experiments.
OpenML is a well-structured repository for machine learning research data that -- among many other things -- allows to access, share and filter datasets, tasks, and task collections.
Furthermore, we have shown how the R package `r ref_pkg("batchtools")` and its mlr3 connector `r ref_pkg("mlr3batchmark")` can considerably simplify the execution of large-scale benchmark experiments on HPC clusters.

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | OpenML Dataset                                                      |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | OpenML Task                                                         |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | OpenML Collection                                                   |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new                                                        |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a Problem                                                  |
| `r ref("addAlgorithm()")`           |-                           | Register an algorithm                                               |
| `r ref("addExperiments()")`         |-                           | Regiter experiments using existing algorithms and problems          |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduling system                                |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getJobStatus()")`           |-                           | Get the status of existing jobs                                     |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |

Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-ls-benchmarking}

### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")` and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.

## Exercises

1. In this exercise, we will learn how to work with OpenML
   a. Look at the OpenML dataset with ID 31 on the OpenML `r link("https://openml.org", "website")`.
      Then load it into R and create an mlr3 task from it.
   b. One OpenML dataset can be associated with many tasks.
      Find one classification task that is associated with the dataset from the previous exercise (you can use the `r ref("list_oml_tasks()")` function for that).
      Construct an mlr3 task and resampling from this OpenML task.
   c. List 10 OpenML datasets that have less than 5000 observations, no missing values, and less than 10 features.
1. In this exercise, we are investigating the effect of the number of folds on the performance estimate and we will execute the experiment using `r ref_pkg("batchtools")`.
   We are therefore changing the perspective and consider the resampling method the algorithm.
   a. Create an experiment registry.
   b. Add a problem to the registry that accepts problem parameters `learner_id`, `task_id` and `measure_id` and returns a list with the corresponding learner, task, and measure, retrieved from the mlr3 dictionary.
   c. Add an algorithm for the cross-validation method that takes in parameter `folds`, evaluates the resampling and returns the scalar performance measure.
   d. Add an experiment using the penguins task, the `classif.rpart` learner and the accuracy measure.
      Add experiments for 2, 5, and 10 folds and repeat each experiment 12 times (read the documentation of `r ref("addExperiments")` on how to add repetitions of an experiment).
   e. Test one of the defined jobs using the batchtools function `r ref("testJob")`.
   f. Set the `cluster.function` of the registry to multicore `r ref("makeClusterFunctionsMulticore")` and save the registry.
   g. Submit the jobs to the cluster and wait until they are finished. Chunk the jobs so that 3 computational jobs run for roughly the same amount of time.
   h. Load and plot the experiment results using a box plot that compares the results for the different number of folds.
      (Optional: Explain the results)
