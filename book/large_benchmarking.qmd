---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers need not only software tools but also datasets.
  Furthermore, large-scale computational experiments require the use of high-performance computing (HPC) clusters.
  [OpenML](http://www.openml.org/) is an open platform that facilitates the sharing and dissemination of machine learning research data and addresses the first of these issues.
  It provides unique identifiers and standardised (meta)data formats that make it easy to find relevant datasets by querying for specific properties, and to share them using their identifier.
  The first part of this chapter covers the basics of OpenML and how it can be used via the `r ref_pkg("mlr3oml")` interface package.
  We then show how to simplify the execution of benchmark experiments on HPC clusters using the R package [batchtools](https://github.com/mllg/batchtools), which provides a convenience layer for interoperating with different scheduling systems like Slurm or LSF.
---

# Large-Scale Benchmarking: OpenML and Batchtools {#sec-large-benchmarking}

{{< include _setup.qmd >}}

<!-- This chapter is a little annoying to render, because of the dependencies in the chunks. -->
<!-- If weird errors happen, delete the cache and render again. -->

```{r large_benchmarking-001_experiments-001, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

A common benchmarking scenario in machine learning is to compare a set of Learners $L_1, \ldots L_n$ by evaluating their performance on a set of tasks $T_1, \ldots, T_k$ using resamplings $R_1, \ldots, R_k$ and - for the sake of simplicity - a *single* performance measure $M$.
Such experiments can, e.g., answer which learner performs best on the given tasks w.r.t. measure $M$.
We here focus on the most frequent case of a full experimental grid design where each resampling experiment is defined by the triple $E_{i, j} = (L_i, T_j, R_j)$.
Running the benchmark experiment consists of running each resample experiment and then evaluating the predictions using the measure $M$.
@tbl-ml-benchmark illustrates such an experiment.

| Algorithm   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment. {#tbl-ml-benchmark}

The table above should seem familiar as it is the result of calling the `$aggregate()` method on a `r ref("BenchmarkResult")`.
We will now revisit how such an experiment can be defined and run in mlr3.

:::{.callout-note}
The `r ref("benchmark()")` function can work on arbitrary designs, and `r ref("benchmark_grid()")` is just a helper function to create an exhaustive grid design.
If you want to evaluate a custom design, just provide a `data.table` with the columns `task`, `learner` and `resampling` for the corresponding objects.
Each row represents a resampling experiment to execute.
You can combine the resulting `r ref("BenchmarkResult")` with any `r ref("BenchmarkResult")` or `r ref("ResampleResult")` using the combine function `r ref("c()")`.
Just keep in mind that special care is required during the analysis of non-exhaustive grid designs, e.g., plots or aggregated performance values might be misleading.
:::


The code below compares a classification and partition tree with a random forest using a simple holdout resampling and three classification tasks.

```{r large_benchmarking-002_experiments-002}
library(mlr3learners)

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  lrns(c("classif.rpart", "classif.ranger")),
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

bmr$aggregate(msr("classif.acc"))
```

What we have ignored so far are the questions of how to obtain datasets for such experiments, and what to do, when the experiments are large and require execution on a high performance computing (HPC) cluster.
This chapter covers how to

1. easily find relevant datasets and tasks, and
1. run large-scale benchmark experiments on HPC clusters.

The first point will be addressed with the help of the `r link("https://www.openml.org/", "OpenML")` platform [@openml2013] and its mlr3 interface `r ref_pkg("mlr3oml")`.
For the second issue, we will illustrate how the R package `r ref_pkg("batchtools")` [@batchtools] and its mlr3 integration `r ref_pkg("mlr3batchmark")` can greatly simplify the effort required to interact with a scheduling system on HPC systems.

**OpenML**

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of the datasets and tasks is tantamount.
It is therefore helpful to

1. have convenient access to a large collection of datasets and tasks and be able to filter them for specific properties, and
1. be able to easily share these with others, so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.


OpenML is a platform that facilitates the sharing and dissemination of machine learning research data and - among many other things - satisfies the two desiderata mentioned above.
Like mlr3, it is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface [@feurer2021openml] or from Java.
Its goal is to make it easier for researchers to find the data they need to answer the questions they have.
Its design was guided by the `r link("https://www.go-fair.org/fair-principles/", "FAIR")` principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The purpose of these principles is to make scientific data more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.

More concretely, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible through a REST API or the web interface.

In this chapter we will cover the main features of OpenML and how to use them via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform and how to do it.
OpenML supports different types of objects and the following is a summary of the OpenML entities that we will cover:

*   `r define("OpenML Dataset")`: A (usually tabular) dataset with additional metadata.
    The latter includes e.g. a description of the dataset and its properties, a licence, and meta-features of the dataset.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("DataBackend", text = "mlr3::DataBackend")`.
    As most OpenML data sets also have a designated target column, they can often directly be converted to a `r ref("Task")`.
*   `r define("OpenML Task")`: A machine learning task, i.e. a concrete problem specification on an OpenML dataset.
    This includes splits into training and a test set, which differs from the mlr3 `r ref("Task")` definition.
    For this reason it can be converted to both a `r ref("Task", text = "mlr3::Task")` and corresponding `r ref("Resampling", text = "mlr3::Resampling")`.
*   `r define("OpenML Task Collection")`: A container object that allows to group tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 [@bischl2021openml], which is a curated collection of classification tasks.

A simple example for an OpenML object is the dataset with ID 31 -- the well known "credit-g" data -- which can be accessed through the link <https://openml.org/d/31>.

Note that while OpenML also supports other objects such as representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.

**Batchtools**

In @tbl-ml-benchmark we have displayed a common experiment setup for machine learning benchmarks, where we compare the performance of different learners on a set of tasks.
Once such a design is finalized, the next step is to run it in parallel.
Such experiments are conceptually straight-forward to parallelize, because we are facing an embarrassingly parallel problem.
Not only are the resample experiments independent, but even the individual resampling iterations do not need to share information.
However, if the experiment is large, parallelization on a local machine (see @sec-parallelization) might not be enough and usage of a HPC cluster is required.

:::{.callout-note}
The package `r ref_pkg("future")` comes with a plan for `r ref_pkg("batchtools")`.
However, for larger experiments the additional control over the execution which `batchtools` offers is invaluable.
Therefore, we recommend the `"batchtools"` plan only for moderately sized experiments which complete within a couple of hours.
:::


While access to HPC clusters is widespread in academia, the effort required to operate these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework for conveniently run large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.
In @sec-batchtools we will teach you how to use the package to run mlr3 experiments.

## OpenML {#sec-openml}

In this section we show how to search, load, and work with OpenML objects using the interface package `r ref_pkg("mlr3oml")`.

### Dataset {#sec-openml-dataset}

Arguably the most important entity on OpenML is the dataset.
When someone wants to access datasets on OpenML, the two possible situations are that they want to

1.  access specific datasets whose IDs they know, or
2.  find datasets with specific properties.

We start with the first scenario.
As an exemplary use-case, we might be interested in how the random forest implementation in `r ref_pkg("ranger")` compares to modern AutoML tools.
After some research, one might find the AutoML benchmark [@amlb2022], where many AutoML systems are compared on over 100 classification and regression problems from OpenML.
Our goal is then to compare the performance of the `ranger` with the results of this study.
This will serve as the guiding example throughout this section.

Since the AutoML benchmark uses datasets from OpenML, we can easily retrieve these datasets using their IDs.
As an example, we will focus on the dataset with `r link("https://openml.org/d/1590", "ID 1590")` -- the well-known adult data.
We load it into R using the function `r ref("mlr3oml::odt()")`, which returns an object of the class `r ref("OMLData")`.

```{r large_benchmarking-003_experiments-003}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about `r odata$nrow` adults -- such as their age or education -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*.
The major difference between a `r ref("OMLData")` object and a typical `data.frame` in R is that the former comes with additional metadata that is accessible through its fields.
This might include a licence or data qualities, which are properties of the datasets.

```{r large_benchmarking-004_experiments-004}
odata$license
odata$qualities[1:4, ]
```

<!-- odata$qualities["NumberOfFeatures", value] -->

The actual data can be accessed through the field `$data`.

```{r large_benchmarking-005_experiments-005}
odata$data[1:2, ]
```

:::{.callout-tip}
When working with OpenML objects, these are downloaded piecemeal from the OpenML server.
This way you can, e.g., access the metadata without loading the data set.
While accessing the `$data` slot in the above example, the download is automatically triggered, the downloaded data is imported in R and the `data.frame` gets stored in the `odata` object.
All subsequent accesses to `$data` will be transparently redirected to the in-memory `data.frame`.
Additionally, many objects can be permanently cached on the file system.
This which can be enabled by setting the option `mlr3oml.cache` to either `TRUE` or a specific path to be used as the cache folder.
:::

As we have loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The mlr3 class that comes closest to the OpenML dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend()")`.
This is a recurring theme throughout this section: OpenML and mlr3 objects are well interoperable.

```{r large_benchmarking-006_experiments-006}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create an mlr3 `r ref("Task")` from the data backend and use the mlr3 toolbox as usual.

```{r large_benchmarking-007_experiments-007}
task = as_task_classif(backend, target = "class")
task
```
Alternatively, as the OpenML adult dataset comes with a default target, you can also directly convert to a task with the appropriate type:
```{r}
task = as_task(odata)
```

<!-- FIXME: Transition is confusing here, we might need to restructure to subsubsections? -->
However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
I.e. we might be in the second of the two scenarios outlined earlier, where we first have to find datasets matching our requirements.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for filtering the existing dataset.
The `r ref("list_oml_data")` function allows to execute such requests from R.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r large_benchmarking-008_experiments-008}
odatasets = list_oml_data(
  limit = 5,
  number_features = c(1, 4),
  number_instances = c(100, 1000)
)
```

By looking at the table we confirm that indeed only datasets with the specified properties were returned.
Here, we only show a subset of the columns for readability.

```{r large_benchmarking-009_experiments-009}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-tip}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data()")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML `r link("https://www.openml.org/apis", "API documentation")`.
:::

### Task {#sec-openml-task}

While the previous section showed how to access, find and convert OpenML datasets, we will now focus on OpenML tasks that are built on top of the former.
Similar to mlr3, OpenML has different types of tasks, such as regression or classification.
While we ignored this issue in the previous section, simply using the same datasets from the AutoML benchmark is not enough to allow for a cross-study comparison.
Other important aspects include e.g. the choice of features and how the data was split into training and test sets.
Fortunately, the AutoML benchmark shares not only the data but also the task IDs.

The task associated with the adult data from earlier has `r link("https://openml.org/t/359983", "task ID 1590")`.
We can load it using the `r ref("mlr3oml::otsk()")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590.
The printed output also tells us the task type, the target variable^[This may differ from the default target used in the previous section.] and the estimation procedure, which is comparable with a mlr3 `r ref("Resampling")`.

```{r large_benchmarking-010_experiments-010}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-011_experiments-011}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.
In mlr3 terms, these are the instantiation of a mlr3 `r ref("Resampling")` on a specific task.

```{r large_benchmarking-012_experiments-012}
head(otask$task_splits)
```

The OpenML task can be converted to either an mlr3 `r ref("Task")` or an instantiated `r ref("ResamplingCustom")`.
For the first of these we can use the `r ref("as_task()")` converter function.

```{r large_benchmarking-013_experiments-013}
task = as_task(otask)
task
```

The accompanying resampling (instantiated on the task created above) can be created using the `r ref("as_resampling()")` converter.

```{r large_benchmarking-014_experiments-014}
resampling = as_resampling(otask)
resampling
```

:::{.callout-tip}
As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors, i.e. `tsk("oml", task_id = 359983)`.
:::

Continuing with our original goal of comparing the random forest implemented in `r ref_pkg("ranger")` with modern AutoML tools, we define the learner of interest.
As the adult dataset contains missing values, we perform an out of range imputation before fitting the model.
We suppress the output during training by specifying `verbose = FALSE`.

```{r large_benchmarking-016_experiments-016}
learner = po("imputeoor") %>>% lrn("classif.ranger", verbose = FALSE)
learner
```

We can run the experiment as usual by calling `r ref("resample()")`.

```{r large_benchmarking-017_experiments-017}
rr = resample(task, learner, resampling)
rr$aggregate(msr("classif.acc"))
```

Because we used the same task definition and data-splits as in the AutoML benchmark, the resulting `r ref("ResampleResult")` is comparable with the results from the AutoML benchmark, whose results we can access through its `r link("https://openml.github.io/automlbenchmark/", "website")`.
The highest accuracy achieved on the adult dataset at the time of writing this book was achieved by the TPOT AutoML system [@olson2016tpot] with score of 88.27%, unsurprisingly beating our vanilla random forest.

<!-- :::{.callout-warning} -->
<!-- It is important to ensure that not only the `r ref("ResampleResult")` is comparable with the results of a previous study, but also that the same `r ref("Measure")` definition is used. -->
<!-- ::: -->
<!---->
While we have now learned how to access and use OpenML tasks with known IDs, the second common scenario is to find tasks with certain properties on the OpenML website.
As an example, suppose we are only interested in binary classification problems.
We can run this query analogously to querying datasets by using the `r ref("mlr3oml::list_oml_tasks()")` function and specifying the `task_type` and `number_classes` arguments.
We limit the response to 5 and show only selected columns for readability.

```{r large_benchmarking-018_experiments-018}
otasks = list_oml_tasks(
  type = "classif",
  number_classes = 2,
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the OpenML task collection, which bundles existing OpenML tasks in a container object.
This allows for the creation of *`r define("benchmark suites")`, which are curated collections of tasks, usually satisfying certain quality criteria.
Many research areas have such agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time.
One example for such a benchmark suite is the aforementioned AutoML benchmark that is used to compare AutoML methods.
The classification tasks of this benchmark suite are contained in the collection with `r link("https://www.openml.org/search?type=study&study_type=task&id=271", "ID 271")`.
Other benchmark suites that are available on OpenML are e.g. the `r link("https://www.openml.org/search?type=study&study_type=task&id=99", "OpenML CC-18")` which contains curated classification tasks [@bischl2021openml] or a `r link("https://www.openml.org/search?type=study&sort=tasks_included&study_type=task&id=304", "benchmark for tabular deep learning")` [@grinsztajn2022why].

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r large_benchmarking-019_experiments-019}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML).
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output informs us that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r large_benchmarking-020_experiments-020}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r large_benchmarking-021_experiments-021}
otask_collection
```

We can get an overview of these tasks by accessing the `$tasks` field.
For compactness, we only show a subset of the columns.

```{r large_benchmarking-022_experiments-022}
otask_collection$tasks[, .(id, data, target, task_splits)]
```

Coming back to our original goal of comparing the `ranger` random forest implementation with existing AutoML systems, the next step would be to evaluate it on the whole benchmark suite as opposed to only the adult task from above.
To do so, we first need to create the tasks and resamplings from the collection.
If we wanted to get all tasks and resamplings, we could achieve this using the converters `r ref("as_tasks()")` and `r ref("as_resamplings()")`.
In order to keep the runtime manageable, we only use a selected subset of 6 of those tasks.
Instead of first creating `r ref("OMLTask")` objects and converting them, we can directly use the `"oml"` task and resampling.

<!-- And also because the albert dataset fails ... -->

```{r large_benchmarking-023_experiments-023}
#| eval: false
ids = otask_collection$task_ids[c(1, 5, 6, 8, 9, 10)]
otasks = lapply(ids, otsk)

tasks = lapply(otasks, as_task)
resamplings = lapply(otasks, as_resampling)
learner = lrn("classif.ranger")
```

```{r large_benchmarking-024_experiments-024}
#| echo: false

learner = lrn("classif.ranger")

if (file.exists(file.path(getwd(), "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(getwd(), "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(getwd(), "openml", "resamplings.rds"))
}

if (file.exists(file.path(getwd(), "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(getwd(), "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(getwd(), "openml", "tasks.rds"))
}
```
The next step is to define a benchmark design, where the `ranger` is applied to each task-resampling combination.
Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality, which creates an experiment design from the Cartesian product of all tasks, learners and resamplings.
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml()")` function which can be used in such a scenario, where tasks and resamplings are paired.

```{r large_benchmarking-025_experiments-025}
large_design = benchmark_grid_oml(tasks, learner, resamplings)
large_design
```

Because this is already a relatively large experiment (imagine if we used all tasks and added some tuning), we will use this as the starting point for illustrating how the R package `r ref_pkg("batchtools")` can be used to run such an experiment on a HPC cluster.

## Batchtools {#sec-batchtools}

In this section we are concerned the execution of machine learning benchmarks on high-performance computing (HPC) clusters.
We will start by giving a brief summary of the HPC basics and will then show how the R packages `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` simplify the interaction with such clusters.

### HPC Basics {#sec-hpc-basics}

<!-- Explain what a HPC cluster is -->
A HPC cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used for solving complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
A HPC cluster typically consists of multiple compute nodes, each with multiple CPU/GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously, hence making it useful for benchmarking.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a scheduling system like `r link("https://slurm.schedmd.com", "Slurm")` (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the computing resources.

@fig-hpc contains a rough sketch of a HPC architecture.
Multiple users -- in this case Ann and Bob -- can login into the head node (typically via `r link("https://en.wikipedia.org/wiki/Secure_Shell", "SSH")`) and add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue.
One such instruction will be referred to as a `r define("computational job")`.
The scheduling system controls when these computational jobs are executed.

```{r large_benchmarking-026_experiments-026}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster. Ann and Bob both have access to the cluster and can log in to the head node. There, they can submit jobs to the scheduling system, which adds them to its queue and determines when they are run."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

<!-- What do we want to do with the HPC cluster -->
Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. users rely on self-discipline to ensure a clean folder structure

In the next section, we will show how `r ref_pkg("batchtools")` mitigates these problems.


### Benchmarking on a HPC Cluster {#sec-hpc-benchmark}

As a guiding example, we use the random forest experiment where we have left off at the end of @sec-openml-collection.
We will first show how one could use `r ref_pkg("batchtools")` to execute such an experiment on a HPC cluster.
Afterwards, we will show how the `r ref_pkg("mlr3batchmark")` package can be used to simplify the process even further.


<!-- What we wanna do conceptually -->
Our goal now is to run the benchmark design shown below (originally defined at the end of @sec-openml-collection) on a HPC cluster.
You may imagine this to be a large study, such that a `benchmark()` call and parallelization via `r ref_pkg("future")` would not result in a reasonable runtime.

```{r large_benchmarking-027_experiments-027}
large_design
```

If we want to execute this benchmark design using `r ref_pkg("batchtools")`, we need to understand how the batchtools package conceptualizes a benchmark experiment.
The central concept is the experiment (or `r define("job", "Job or Experiment")`):

Experiment (or Job) $E$ is defined by applying Algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ with problem parameters $\xi_P$.

A benchmark experiments in batchtools then consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters.
Each experiment $E$ is independent of all other experiments and constitutes the basic level of computation that can be parallelized.

If we want to utilize batchtools to execute the `large_design` from above, we therefore have to translate the mlr3 resample experiments into the batchtools language.
There is not a single right way to do this, and we here show one solution.
In the introduction of this chapter, we have defined a ML benchmark experiment as running a number of resample experiments $E_{i, j} = (L_i, T_j, R_j)$ that are defined by a learner, task and resampling.
While it might seem natural to define one such resample experiment as a job, each resample experiment can be decomposed even further, namely into its iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ are number of iterations of resampling $R_j$.
We will therefore understand one resampling iteration $E^l_{i, j}$ as a job (aka. experiment).
Note that such a job does not have to coincide with the notion of a computational job defined in the previous section.
One computational job can consist of (sequentially) executing multiple jobs.
This can make sense, when the jobs are short, as submitting and running computational jobs on a HPC cluster comes with an overhead.

We will now continue with showing how to define the batchtools algorithms $A$, algorithm parameters $\xi_A$, problems $P$ and problem parameters $\xi_P$ in such a way that they allow to define each resampling iteration as a job.
The first step is always to create an (or load an existing) experiment registry using the function `r ref("batchtools::makeExperimentRegistry()")` (or `r ref("batchtools::loadRegistry()")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

:::{.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry.
The R package `r ref_pkg("batchtools")` also has a different registry object that can be constructed with `r ref("batchtools::makeRegistry()")` which we will not cover in this book.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry gives access to

* algorithms, problems, and job definitions
* log outputs and status of submitted, running, and finished jobs
* job results
* `r define("cluster function", "Batchtools object that allows to configure the interaction with the scheduling system.")`, which defines the interaction with the scheduling system

While the first three bullet points should be relatively clear, the fourth needs some explanation.
By configuring the scheduling system through a cluster function, it is possible to make the interaction with the scheduling system independent on the scheduling software.
We will come back to this later and show how to change it to work on a Slurm cluster.

We create a registry using a temporary directory by specifying `file.dir = NA` below, but we could also provide a proper path to be used as the registry folder.
We also set a global seed for the registry.

```{r large_benchmarking-028_experiments-028}
library(mlr3batchmark)
library(batchtools)

reg = makeExperimentRegistry(file.dir = NA, seed = 1)
```

When printing the registry, we see that (unsurprisingly) there are 0 algorithms, problems, and jobs registered.
We are also informed that the "Interactive" cluster function is used and about the working directory for the experiments.

```{r large_benchmarking-029_experiments-029}
reg
```

The next step is to populate the registry with algorithms and problems, which we will then use to define the jobs, i.e. the resampling iterations.
We can register a problem, by calling `r ref("batchtools::addProblem()")` whose main arguments beside the registry are:

* `name` to uniquely identify the problem
* `data` to represent the static data part of a problem
* `fun` takes in the `data` and problem parameters $\xi_P$ and returns a concrete problem instance

We register the first task-resampling combination of the `large_design` using the task ID as the name.
We define the problem parameter $\xi_P$ as the resampling iteration.
The problem `fun` takes in the static problem `data` -- a list with a task and resampling -- and the problem parameter $\xi_P$ -- the resampling iteration -- and returns the problem instance by adding the parameter to the list.

```{r large_benchmarking-030_experiments-030}
task = large_design$task[[1L]]
task
resampling = large_design$resampling[[1L]]

addProblem(
  name = task$id,
  data = list(task = task, resampling = resampling),
  fun = function(data, iteration, ...) {
    list(
      task = data$task,
      resampling = data$resampling,
      iteration = iteration
    )
  },
  reg = reg
)
```

:::{.callout-tip}
All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but we would not have to do so.
:::

When calling `r ref("batchtools::addProblem()")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

```{r large_benchmarking-031_experiments-031}
reg$problems
```

The next step is to register the algorithm we want to run, which we can do by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to uniquely identify the algorithm
* `fun` which takes in the problem instance and parameters $\xi_A$ and defines what is computed when running an experiment. Its output is the experiment result.

Our goal at the start was to define one job as one resampling iteration, so the algorithm should execute a single resampling iteration.
We will call the algorithm that we add `"run_learner"` and make the learner itself the parameter $\xi_A$.
It receives a problem instance and the learner, executes one iteration of a resampling and returns the learner's state and the predictions as the experiment result.
Note that could have also added one algorithm for each learner and included the respective hyperparameters as algorithm parameters $\xi_A$.
What is best, depends on the concrete problem at hand and -- as mentioned before -- there is no single best solution.

```{r large_benchmarking-032_experiments-032}
addAlgorithm(
  "run_learner",
  fun = function(instance, learner, ...) {
    library("mlr3verse")
    resampling = instance$resampling
    task = instance$task
    iteration = instance$iteration

    train_ids = resampling$train_set(iteration)
    test_ids = resampling$test_set(iteration)

    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    list(state = learner$state, prediction = prediction)
  },
  reg = reg
)

reg$algorithms
```

:::{.callout-note}
Both the problem and algorithm `fun` also receive other arguments during execution.
These must either be explicitly named in the definitions of the algorithm and problem `fun`, or the argument `...` must be present.
For a more detailed explanation, we refer to the `r ref_pkg("batchtools")` documentation.
:::

As we have now defined a problem and an algorithm, we can define experiments with concrete algorithm and problem parameters $\xi_A$ and $\xi_B$.
We can do this using the `r ref("batchtools::addExperiments()")` function, which takes in the arguments:

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames (or ‘data.table’). The name must match the algorithm name while the column names correspond to parameters of the algorithm.

We now add all 10 resampling iterations as experiments.
We use a robustified `ranger` as the learner and add experiments for all 10 resampling iterations,

```{r large_benchmarking-033_experiments-033}
l = as_learner(
  ppl("robustify") %>>% lrn("classif.ranger")
)
addExperiments(
  prob.designs = list(yeast = data.table(iteration = 1:10)),
  algo.designs = list(run_learner = data.table(learner = list(l))),
  reg = reg
)
```

Note that whenever an experiment is added, the global seed set for the registry is incremented and saved for the experiment to ensure reproducibility.

When printing the registry again, we confirm that the algorithm, problem and experiments (jobs) were added.

```{r large_benchmarking-034_experiments-034}
reg
```

To get more detailed information, we can use `r ref("batchtools::getJobTable()")`.
This returns a table summarizing information of all added jobs.
Among many other things, we see that each job has a unique ID with which it can be referenced in many batchtools functions.

```{r large_benchmarking-035_experiments-035}
job_table = getJobTable(reg = reg)


job_table[1:2, c("job.id", "algorithm", "algo.pars", "problem", "prob.pars")]
```

The complete workflow of adding algorithms, problems, and experiments is summarized in the image below.

```{r large_benchmarking-037_experiments-037}
#| label: fig-batchtools-illustration
#| fig-cap: "Illustration of batchtools problem, algorithm, and experiment"
#| fig-align: "center"
#| fig-alt: "A problem consists of a static data part and applies the problem function to this data part (and potentially problem parameters) to return a problem instance. The algorithm function takes in a problem instance (and potentially algorithm parameters), executes one job and returns its result."
#| echo: false
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

We have defined 10 jobs, each representing one resampling iteration and it is time to translate these jobs into computational jobs, which includes:

1. specifying resource requirements for each computational
1. (optionally) grouping multiple jobs into one computational job.

The resource requirements that can be specified depend on the cluster function that is set in the registry.
We earlier left the cluster function at its default value, which was the "Interactive" cluster function that is primarily intended for exploration of the package and testing of experiments.

Because we now want to submit the jobs to the scheduling system, the time has come to properly configure it.
We assume for this example that we are operating a Slurm cluster.
Therefore, we set the cluster function to a simple Slurm cluster function that can be constructed using `r ref("batchtools::makeClusterFunctionsSlurm()")`.

```{r large_benchmarking-038_experiments-038}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm(template = "slurm-simple")
```

```{r large_benchmarking-039_experiments-039}
#| echo: false
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

We update the `$cluster.function` field of the registry and save the registry, which is necessary in this case.

```{r large_benchmarking-040_experiments-040}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry()
```

Assuming that we are running this program on the head node of the Slurm cluster, we can call `r ref("batchtools::submitJobs()")` to add the previously defined experiments to the queue of the Slurm scheduler.
The most important arguments of this function besides the registry are:

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job, and
* `resources`, which is a named list specifying the resource requirements with which the computational jobs will be run.

The ids we refer to here are the ids that can e.g. be accessed as the `job.id` column from the job table obtained earlier.

```{r large_benchmarking-041_experiments-041}
job_table$job.id
```

In our case, we show how to group five jobs into one computational job.

```{r large_benchmarking-042_experiments-042}
chunks = data.table(job.id = 1:10, chunk = rep(1:2, each = 5))
chunks
```

In addition to the grouping, we also specify the number of CPUs per computational job to 1, the wall time to 1 hour and the RAM limit to 8 GB.

```{r}
#| eval: false
submitJobs(
  ids = chunks,
  resources = list(ncpus = 1, walltime = 3600, memory = 8000),
  reg = reg
)
```



```{r large_benchmarking-043_experiments-043}
#| echo: false
#| output: false
submitJobs(ids = chunks, reg = reg)
waitForJobs(reg = reg)
```

Once the jobs are correctly submitted, it is time to wait until their execution is finished.
While the computational jobs are being processed, `r ref_pkg("batchtools")` exposes many useful functions that allow the user to access information about the submitted, running, expired, or failed computational jobs.
This includes e.g. `r ref("batchtools::getStatus()")` to get the status of the running job, `r ref("batchtools::showLog()")` to inspect the logs or `r ref("batchtools::grepLogs()")` to search the log files.

:::{.callout-tip}
A good approach to submit comptuational jobs is do this from an R Session that is running persistently through multiple SSH sessions.
One option is to use TMUX (Terminal Multiplexer).
:::

When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using the `r ref("batchtools::loadResult()")` function which takes in a job id as argument `id`, as well as a registry.
It outputs the objects returned by the `fun` of the algorithm (added by `r ref("addAlgorithm")`), which is a list containing the learner's state and the prediction for a specific resampling iteration.

```{r large_benchmarking-045_experiments-045}
results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
names(results[[1L]])
results[[1L]]$prediction
```

If we wanted to run the whole benchmark experiment defined in `large_design`, we would simply have to add the remaining five problems and then add the experiments analogously to how we have just shown.
What we will do instead, is show how this whole process can be simplified using the `r ref_pkg("mlr3batchmark")` package.
For this, we will start with a new registry.
```{r large_benchmarking-046_experiments-046}
reg = makeExperimentRegistry(NA, seed = 1)
```

The whole process, of adding the algorithm, problems, and experiments can be done by swapping the common `r ref("mlr3::benchmark()")` call with the `r ref("mlr3batchmark::batchmark()")` function.

```{r large_benchmarking-047_experiments-047}
#| output: false
batchmark(large_design)
```

This does something similar as the steps that we he executed manually before.
When printing the registry, we see that one algorithm, six problems, and 60 experiments are registered.

```{r large_benchmarking-048_experiments-048}
reg
```

We can now submit them like before using `r ref("batchtools::submitJobs()")`.
Once they are done, we can either load the individual results using `r ref("batchtools::loadResult()")`, or the complete `r ref("BenchmarkResult")` by calling `r ref("mlr3batchmark::reduceBatchmarkResult()")`.


```{r large_benchmarking-049_experiments-049}
#| echo: false
if (file.exists(file.path(getwd(), "openml", "bmr_large.rds"))) {
  bmr = readRDS(file.path(getwd(), "openml", "bmr_large.rds"))
} else {
  bmr = benchmark(large_design)
  saveRDS(bmr, file.path(getwd(), "openml", "bmr_large.rds"))
}

```


```{r large_benchmarking-050_experiments-050}
#| eval: false
bmr = reduceBatchmarkResult()
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark()")` on the `large_design` above.

```{r large_benchmarking-051_experiments-051}
bmr
```

<!-- TODO: Render as pdf and do own review -->
<!-- TODO: Check that spelling is correct and put everything through deepl. -->
<!-- TODO: Send it to Marc, Michel, Martin and Bernd -->

## Conclusion

In this chapter we have learned about two tools -- OpenML and batchtools -- that both aid in the design and execution of (large-scale) benchmark experiments.
OpenML is a well-structured repository for machine learning research data that -- among many other things -- allows to access, share and filter datasets, tasks, and task collections.
Furthermore, we have shown how the R package `r ref_pkg("batchtools")` and its mlr3 connector `r ref_pkg("mlr3batchmark")` can considerably simplify the execution of large-scale benchmark experiments on HPC clusters.

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | OpenML Dataset                                                      |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | OpenML Task                                                         |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | OpenML Collection                                                   |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new                                                        |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a Problem                                                  |
| `r ref("addAlgorithm()")`           |-                           | Register an algorithm                                               |
| `r ref("addExperiments()")`         |-                           | Regiter experiments using existing algorithms and problems          |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduling system                                |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getJobStatus()")`           |-                           | Get the status of existing jobs                                     |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |

:Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-ls-benchmarking}

### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")` and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.

## Exercises

1. In this exercise, we will learn how to work with OpenML
   a. Look at the OpenML dataset with ID 31 on the OpenML `r link("https://openml.org", "website")`.
      Then load it into R and create an mlr3 task from it.
   b. One OpenML dataset can be associated with many tasks.
      Find one classification task that is associated with the dataset from the previous exercise (you can use the `r ref("list_oml_tasks()")` function for that).
      Construct an mlr3 task and resampling from this OpenML task.
   c. List 10 OpenML datasets that have less than 5000 observations, no missing values, and less than 10 features.
1. In this exercise, we are investigating the effect of the number of folds on the performance estimate and we will execute the experiment using `r ref_pkg("batchtools")`.
   We are therefore changing the perspective and consider the resampling method the algorithm.
   a. Create an experiment registry.
   b. Add a problem to the registry that accepts problem parameters `learner_id`, `task_id` and `measure_id` and returns a list with the corresponding learner, task, and measure, retrieved from the mlr3 dictionary.
   c. Add an algorithm for the cross-validation method that takes in parameter `folds`, evaluates the resampling and returns the scalar performance measure.
   d. Add an experiment using the penguins task, the `classif.rpart` learner and the accuracy measure.
      Add experiments for 2, 5, and 10 folds and repeat each experiment 12 times (read the documentation of `r ref("addExperiments")` on how to add repetitions of an experiment).
   e. Test one of the defined jobs using the batchtools function `r ref("testJob")`.
   f. Set the `cluster.function` of the registry to multicore `r ref("makeClusterFunctionsMulticore")` and save the registry.
   g. Submit the jobs to the cluster and wait until they are finished. Chunk the jobs so that 3 computational jobs run for roughly the same amount of time.
   h. Load and plot the experiment results using a box plot that compares the results for the different number of folds.
      (Optional: Explain the results)
