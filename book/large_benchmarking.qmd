---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
    (ABSTRACT START) In the field of machine learning, benchmark experiments are used to evaluate the performance of algorithms.
    Conducting such experiments involves evaluating algorithms on diverse datasets, which can be acquired through resources such as [OpenML](http://www.openml.org/), which is a platform for sharing machine learning research data.
    The first section of this chapter shows how to work with OpenML using the interface package [`mlr3oml`](https://github.com/mlr-org/mlr3oml).
    However, running large-scale experiments requires not only datasets, but also significant computational resources, and it can be beneficial to leverage High-Performance Computing (HPC) clusters to speed up the experiment execution.
    In the second section, we will show how the R package [`batchtools`](https://github.com/mllg/batchtools) and its mlr3 integration [`mlr3batchmark`](https://github.com/mlr-org/mlr3batchmark) can considerably simplify the process of working on HPC clusters.
    Once the experiments are complete, statistical analysis is required to extract insights from the results.
    In the third section of this chapter, we show how to do this using the [`mlr3benchmark`](https://github.com/mlr-org/mlr3benchmark) package. (ABSTRACT END)
---

# Large-Scale Benchmark Experiments {#sec-large-benchmarking}

{{< include _setup.qmd >}}

<!-- This chapter is a little annoying to render, because of the dependencies in the chunks. -->
<!-- If weird errors happen, delete the cache and render again. -->

```{r large_benchmarking-001_experiments-001}
#| output: false
#| echo: false
#| eval: true
#| cache: false
lgr::get_logger("mlr3oml")$set_threshold("off")

rebuild_cache = FALSE

if (rebuild_cache) {
  unlink(here::here("book", "openml"), recursive = TRUE)
  dir.create(here::here("book", "openml"))
}

options(mlr3oml.cache = here::here("book", "openml", "cache"))

library(mlr3batchmark)
library(batchtools)
```

In the world of machine learning, there are many methods that are hard to evaluate using mathematical analysis alone.
Even if formal analysis is successful, it is often an open questions whether real-world datasets satisfy the necessary assumptions for the theorems to hold.
Consequently, researchers resort to conducting benchmark experiments to answer fundamental scientific questions.
Such studies evaluate different algorithms on a wide range of datasets, with the aim of identifying which method performs best.
These empirical investigations are essential for understanding the capabilities and limitations of existing methods and for developing new and improved approaches.
To carry them out effectively, researchers need access to datasets, spanning a wide range of domains and problem types.
This is because one can only draw conclusions about the kind of datasets on which the benchmark study was conducted.
Fortunately, there are several online resources available for acquiring such datasets, with `r link("https://www.openml.org/", "OpenML")` [@openml2013] being a popular choice.
In the first part of this chapter, we will show how to use OpenML through its `r ref_pkg("mlr3oml")` integration.

However, evaluating algorithms on such a large scale requires significant computational resources.
For this reason, researchers often utilize High-Performance Computing (HPC) clusters, which allow them to conveniently execute multiple experiments in parallel.
The R package `r ref_pkg("batchtools")` [@batchtools] is a tool for managing and executing experiments on such clusters.
In the second section we will show how to use batchtool and its mlr3 integration `r ref_pkg("mlr3batchmark")`.

Once the experiments are complete, statistical analysis is used to answer the original scientific question that motivated the benchmark experiment in the first place.
We will show how to conduct such statistical analysis using the `r ref_pkg("mlr3benchmark")` package.

A common design is to compare a set of Learners $L_1, \ldots L_n$ by evaluating their performance on a set of tasks $T_1, \ldots, T_k$ which each have an associated resampling $R_1, \ldots, R_k$.
The performance is evaluated using measures $M_k$, where $k = 1, \ldots, m$.
We here focus on the most frequent case of a full experimental grid design, where each resample experiment is defined by a triple $E_{i, j} = (L_i, T_j, R_j)$.
Running the benchmark experiment consists of running each resample experiment and then evaluating the predictions using the measures $M_k$.
The execution of a single resample experiment can again be subdivided into computationally independent resampling iterations $E^l_{i, j}$, c.f. @sec-resampling.

@tbl-ml-benchmark illustrates a full grid design and should seem familiar, as it is the result of calling the `$aggregate()` method of a `r ref("BenchmarkResult")`.
Note that the evaluated performance measures $m_{i, j}$ are vectors if $m > 1$.

| Algorithm   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a full experimental grid design. {#tbl-ml-benchmark}

As a guiding example throughout this chapter, we will compare the random forest implementation in `r ref_pkg("ranger")` with the logistic regression method.
As biomedical research increasingly relies on machine learning methods, this question holds significant practical importance and has already been studied by @couronne2018random.

The following example compares the two methods using a simple holdout resampling and three classification tasks that come with `r ref_pkg("mlr3")`.
@sec-performance already covered how to conduct such benchmark experiments and we recommend revisiting this chapter if anything is unclear.
The metric of choice is the classification accuracy.
Note that we "robustify" both learners to work on a wide range of tasks.

<!-- FIXME: refernce to robustify when preprocessing chapter is ready -->

```{r large_benchmarking-002_experiments-002}
#| warning: false
lrn_logreg = lrn("classif.log_reg")
lrn_logreg = as_learner(ppl("robustify", learner = lrn_logreg) %>>% lrn_logreg)
lrn_logreg$id = "logreg"
lrn_ranger = lrn("classif.ranger")
lrn_ranger = as_learner(ppl("robustify", learner = lrn_ranger) %>>% lrn_ranger)
lrn_ranger$id = "ranger"

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "spam")),
  list(lrn_logreg, lrn_ranger),
  rsmp("holdout")
)

bmr = benchmark(design)

result = bmr$aggregate(msr("classif.acc"))

result[, .(task_id, learner_id, classif.acc)]
```

The random forest outperforms the logistic regression on two out of three datasets.
In the subsequent sections, we will show how to turn this example into a more realistic experiment using the tools mentioned earlier.

## Getting Data with OpenML {#sec-openml}

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of datasets and tasks is essential.
It is therefore helpful to

1. have convenient access to a large collection of datasets and be able to filter them for specific properties, and
1. be able to easily share these with others, so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.

OpenML is a platform that facilitates the sharing and dissemination of machine learning research data and satisfies these two desiderata.
Like mlr3, it is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface [@feurer2021openml] or from Java.
Its goal is to make it easier for researchers to find the data they need to answer the questions they have.
Its design was guided by the `r link("https://www.go-fair.org/fair-principles/", "FAIR")` principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The purpose of these principles is to make scientific data more easily discoverable and reusable.
More concretely, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible through a REST API or the web interface.

In this section we will cover some of the main features of OpenML and how to use them via the  `r ref_pkg("mlr3oml")` interface package.
OpenML supports different types of objects and we will cover the following:

*   `r define("OpenML **Dataset**")`: (Usually tabular) data with additional metadata.
    The latter includes for example a description of the data and a licence.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("mlr3::DataBackend")`.
    As most OpenML datasets also have a designated target column, they can often directly be converted to an `r ref("mlr3::Task")`.
*   `r define("OpenML **Task**")`: A machine learning task, i.e. a concrete problem specification on an OpenML dataset.
    This includes splits into train and test sets, thereby differing from the mlr3 task definition.
    For this reason it can be converted to both a `r ref("mlr3::Task")` and corresponding instantiated `r ref("mlr3::Resampling")`.
*   `r define("OpenML **Task Collection**")`: A container object that allows to group tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 [@bischl2021openml], which is a curated collection of classification tasks.

While OpenML also supports other objects such as representations of algorithms (flows) and experiment results (runs), they are not covered in this chapter.
For more information about these features, we refer to the OpenML `r link("htts://openml.org", "website")` or the documentation of the `r ref_pkg("mlr3oml")` package.

### Dataset {#sec-openml-dataset}

To illustrate the OpenML dataset class, we will use the dataset with `r link("https://openml.org/d/1590", "ID 1590")` -- the well-known adult data.
Such an ID can either be found by searching for objects on the OpenML website, or through the REST API.
This will be covered in more detail in @sec-openml-filtering.

We load the object into R using `r ref("mlr3oml::odt()")`, which returns an object of class `r ref("OMLData")`.

```{r large_benchmarking-003_experiments-003}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about `r odata$nrow` adults -- such as their age or education -- and the goal is usually to predict the *class* variable, which indicates whether a person has an income above 50K dollars per year.
The `r ref("OMLData")` object not only contains the data itself, but comes with additional metadata that is accessible through its fields.

```{r large_benchmarking-004_experiments-004}
odata$license
```

The actual data can be accessed through `$data`.

```{r large_benchmarking-005_experiments-005}
head(odata$data)
```

:::{.callout-tip}
When working with OpenML objects, these are downloaded in a piecemeal fashion from the OpenML server.
This way you can, e.g., access the metadata without loading the data set.
While accessing the `$data` slot in the above example, the download is automatically triggered, the downloaded data is imported in R, and the `data.frame` gets stored in the `odata` object.
All subsequent accesses to `$data` will be transparently redirected to the in-memory `data.frame`.
Additionally, many objects can be permanently cached on the file system.
This which can be enabled by setting the option `mlr3oml.cache` to either `TRUE` or a specific path to be used as the cache folder.
:::

After we have loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The class that comes closest to the OpenML dataset is the `r ref("mlr3::DataBackend")` (see @sec-backends) and it is possible to convert `r ref("OMLData")` objects by calling `r ref("as_data_backend()")`.
This is a recurring theme throughout this section: OpenML and mlr3 objects are well interoperable.

```{r large_benchmarking-006_experiments-006}
backend = as_data_backend(odata)
backend
```

We can create an mlr3 task from the backend via `r ref("as_task_classif()")`.

```{r large_benchmarking-007_experiments-007}
task = as_task_classif(backend, target = "class")
task
```
Alternatively, as the OpenML adult dataset comes with a default target, you can also directly convert to a task with the appropriate type:
```{r}
task = as_task(odata)
```

### Task {#sec-openml-task}

OpenML tasks are built on top of OpenML datasets and additionally specify the target variable, features, and the train-test splits.
Similar to mlr3, OpenML has different types of tasks, such as regression or classification.
A task associated with the adult data from earlier has ID `r link("https://openml.org/t/359983", "359983")`.
We can load the object using the `r ref("mlr3oml::otsk()")` function, which returns an `r ref("OMLTask")` object.

```{r large_benchmarking-010_experiments-010}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-011_experiments-011}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.
In mlr3 terms, these are the instantiation of a mlr3 `r ref("Resampling")` on a specific `r ref("Task")`.

```{r large_benchmarking-012_experiments-012}
head(otask$task_splits)
```

The OpenML task can be converted to both an mlr3 `r ref("Task")` and a `r ref("ResamplingCustom")` instantiated on the task.
To convert to the former we can use `r ref("as_task()")`.

```{r large_benchmarking-013_experiments-013}
task = as_task(otask)
task
```

The accompanying resampling can be created using `r ref("as_resampling()")`.

```{r large_benchmarking-014_experiments-014}
resampling = as_resampling(otask)
resampling
```

:::{.callout-tip}
As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors, e.g. `tsk("oml", task_id = 359983)`.
:::


### Filtering of Data and Tasks {#sec-openml-filtering}

Besides working with objects with known IDs, another important question is how to find relevant IDs.
Because objects on OpenML have strict metadata, they can be filtered w.r.t these properties.
This is possible through either the website or the REST API.
In addition, the website also supports targeted text queries to search for specific datasets such as the "adult" data from earlier.

The `r ref("list_oml_data()")` function allows to filter datasets for specific properties.
As an example, we might only be interested in comparing the random forest and the logistic regression on datasets with less than 4 features and 100 to 1000 observations.
By setting `number_classes` to 2, we only receive datasets where the default target has two different values.
To keep the output readable, we only show the first 5 results from that query.

```{r large_benchmarking-008_experiments-008}
#| eval: false
odatasets = list_oml_data(
  limit = 5,
  number_features = c(1, 4),
  number_instances = c(100, 1000),
  number_classes = 2
)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "odatasets_list.rds")
if (rebuild_cache) {
  odatasets = list_oml_data(
    limit = 5,
    number_features = c(1, 4),
    number_instances = c(100, 1000)
  )
  saveRDS(odatasets, path)
} else {
  odatasets = readRDS(path)
}
```


The table below confirms that indeed only datasets with the specified properties were returned.
We only show a subset of the columns for readability.

```{r large_benchmarking-009_experiments-009}
odatasets[, .(data_id, NumberOfClasses, NumberOfFeatures, NumberOfInstances)]
```

Besides datasets, it is also possible to filter tasks.
This can be done using  `r ref("list_oml_tasks()")`, which works analogously to the previous example.

We could now start looking at the returned IDs in more detail to verify whether they are suitable for our purposes.
This process can be tedious, as some datasets have quirks to look out for.
A solution to this problem is to use an existing curated task collection, which we will cover next.

### Task Collection {#sec-openml-collection}

The OpenML task collection is a container object which bundles existing tasks.
This allows for the creation of `r define("benchmark suites")`, which are curated collections of tasks, satisfying certain quality criteria.
One example for such a benchmark suite is the `r link("https://www.openml.org/search?type=study&study_type=task&id=99", "OpenML CC-18")`, which contains curated classification tasks [@bischl2021openml].
Other collections available on OpenML include the `r link("https://www.openml.org/search?type=study&study_type=task&id=271", "AutoML benchmark")` [@amlb2022] or a `r link("https://www.openml.org/search?type=study&study_type=task&id=304", "benchmark for tabular deep learning")` [@grinsztajn2022why].


```{r large_benchmarking-019_experiments-019}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML).
# This is why we load it from disk
path = here::here("book", "openml", "otask_collection.rds")
if (rebuild_cache) {
  otask_collection = ocl(id = 99)
  otask_collection$task_ids
  saveRDS(otask_collection, path)
} else {
  otask_collection = readRDS(path)
}
```

We can create an `r ref("OMLCollection")` using `r ref("mlr3oml::ocl()")`.
We see that the CC-18 contains 72 classification tasks on different datasets.

```{r large_benchmarking-020_experiments-020}
#| eval: false
otask_collection = ocl(id = 99)
```

```{r large_benchmarking-021_experiments-021}
otask_collection
```

The contained tasks can be accessed through `$task_ids`.

```{r}
head(otask_collection$task_ids)
```

We will now define our experimental design using tasks from the CC-18.
If we wanted to get all tasks and resamplings, we could achieve this using the converters `r ref("as_tasks()")` and `r ref("as_resamplings()")`.
However, as the CC-18 contains not only binary classification tasks we use `r ref("list_oml_tasks()")` to subset the collection further.
We pass the task IDs from the CC-18 as argument `task_id` and request the number of classes to be 2.

```{r}
#| eval: false
binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids, number_classes = 2)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "binary_cc18.rds")
if (rebuild_cache) {
  binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids, number_classes = 2)
  saveRDS(binary_cc18, path)
} else {
  binary_cc18 = readRDS(path)
}
```

In order to keep the runtime in later examples reasonable, we only use the first six tasks.

```{r}
head(binary_cc18[, .(task_id, name, NumberOfClasses)])
ids = binary_cc18$task_id[1:6]
```

We now define the learners, tasks, and resamplings for the experiment.
In addition to the random forest and the logistic regression, we also include a featureless learner as a baseline.

```{r large_benchmarking-023_experiments-023}
#| eval: false
library(mlr3misc)
otasks = lapply(ids, otsk)

tasks = lapply(otasks, as_task)
resamplings = lapply(otasks, as_resampling)

learners = list(lrn_logreg, lrn_ranger, lrn("classif.featureless", id = "featureless"))
```

```{r large_benchmarking-024_experiments-024}
#| echo: false
learners = c(lrn_logreg, lrn_ranger, lrn("classif.featureless", id = "featureless"))
if (rebuild_cache) {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, here::here("book", "openml", "resamplings.rds"))

  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, here::here("book", "openml", "tasks.rds"))
} else {
  resamplings = readRDS(file.path(here::here(), "book", "openml", "resamplings.rds"))
  tasks = readRDS(here::here("book", "openml", "tasks.rds"))
}
```

To define the design table, we use `r ref("benchmark_grid()")` and set `paired` to `TRUE`, as the resamplings are instantiated on the tasks.

```{r large_benchmarking-025_experiments-025}
large_design = benchmark_grid(tasks, learners, resamplings, paired = TRUE)
head(large_design)
```

## Experiment Execution on HPC Clusters {#sec-hpc-exec}

Once an experimental design is finalized, the next step is to run it.
Parallelizing this step is conceptually straight-forward, because we are facing an embarrassingly parallel problem (see @sec-parallelization).
Not only are the resample experiments independent, but even their individual iterations are.
However, if the experiment is large, parallelization on a local machine as shown in @sec-parallelization might not be enough and using a distributed computing system, such as an HPC cluster, is recommended.
While access to HPC clusters is widespread, the effort required to work on these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework to conveniently run large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.

:::{.callout-note}
In @sec-parallel-resample we have already touched upon different parallelization backends.
The package `r ref_pkg("future")` also comes with a plan for `r ref_pkg("batchtools")`.
However, for larger experiments the additional control over the execution which `batchtools` offers is invaluable.
Therefore, we recommend the `"batchtools"` plan only for moderately sized experiments which complete within a couple of hours.
:::

<!-- We will now start by giving a brief summary of the HPC basics and will then show how to use `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")`. -->

### HPC Basics {#sec-hpc-basics}

A High Performance Computing cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used for solving complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
An HPC cluster typically consists of multiple compute nodes, each with multiple CPU/GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously.
We will leverage this capacity to parallelize the execution of the benchmark experiment.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a `r define("scheduling system")` like `r link("https://slurm.schedmd.com", "Slurm")` (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the computing resources.

@fig-hpc contains a rough sketch of an HPC architecture.
Multiple users can login into the head node (typically via `r link("https://en.wikipedia.org/wiki/Secure_Shell", "SSH")`) and add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue.
One such instruction will be referred to as a `r define("computational job")`.
The scheduling system controls when these computational jobs are executed.

```{r large_benchmarking-026_experiments-026}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster. Ann and Bob both have access to the cluster and can log in to the head node. There, they can submit jobs to the scheduling system, which adds them to its queue and determines when they are run."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. it is cumbersome to query the status of jobs and debug failures

In the following we will see how `r ref_pkg("batchtools")` mitigates these problems.

### Batchtools {#sec-batchtools}

#### Experiment Definition {#sec-batchtools-define}

<!-- What we wanna do conceptually -->
Our goal in this section is to run the benchmark design shown below (originally defined at the end of @sec-openml-collection) on an HPC cluster.

```{r large_benchmarking-027_experiments-027}
head(large_design)
```

::: {.callout-warning}
This part is considerably more complex than most parts of the mlr3book.
We will later see that much of the complexity can be avoided when using `r ref_pkg("mlr3batchmark")`.
:::

If we want to run this using `r ref_pkg("batchtools")`, we need to understand how the package defines benchmark experiments.
The central concept is the experiment or `r define("job", "Job or Experiment")`:

One replication of an experiment (or job) $E$ is defined by applying algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ with problem parameters $\xi_P$.

A benchmark experiment in batchtools then consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters.
Note that the problem parameters $\xi_P$ can e.g. useful in simulation studies but we won't use them here.
Each experiment $E$ is computationally independent of all other experiments and constitutes the basic level of computation batchtools can parallelize.
There is not a single right way to use batchtools and we here show one solution where we use the notion of an experiment replication to represent the resampling iterations.

In the introduction of this chapter, we have defined a benchmark experiment as running a number of resample experiments $E_{i, j} = (L_i, T_j, R_j)$ that are defined by a learner, task and resampling.
While it might seem natural to define one such triplet as a job, each resample experiment can be split up even further, namely into its iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ is the number of iterations of resampling $R_j$.
We will therefore understand one resampling iteration $E^l_{i, j}$ as a job (aka. experiment). ^[Note that such a job does not have to coincide with the notion of a computational job defined earlier, more on that in @sec-batchtools-submission.]
We will now show how to define the batchtools algorithm $A$, algorithm parameters $\xi_A$, and problem $P$ in such a way that they allow to define each resampling iteration as a job.

The first step is always to create an (or load an existing) experiment registry using the function `r ref("batchtools::makeExperimentRegistry()")` (or `r ref("batchtools::loadRegistry()")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

::: {.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry.
The R package `r ref_pkg("batchtools")` also has a different registry object that can be constructed with `r ref("batchtools::makeRegistry()")`, which we won't cover here.
It is a more generic registry that can be used in situations where the computation is not defined in terms of algorithms and problems.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry stores the

* algorithms, problems, and job definitions
* log outputs and status of submitted, running, and finished jobs
* job results
* `r define("cluster function", "Batchtools object that allows to configure the interaction with the scheduling system.")`, which defines the interaction with the scheduling system

While the first three bullet points should be relatively clear, the fourth needs some explanation.
By configuring the scheduling system through a cluster function, it is possible to make the interaction with the scheduling system independent of the scheduling software.
We will come back to this later and show how to change it to work on a Slurm cluster.

We create a registry using a temporary directory by specifying `file.dir = NA`, but we could also provide a proper path to be used as the registry folder.
Furthermore, we set the registry's `seed` to 1 and the `packages` to `r ref_pkg("mlr3verse")`, which will make the package available in all our experiments.

```{r large_benchmarking-028_experiments-028}
#| cache: false
library(batchtools)

reg = makeExperimentRegistry(
  file.dir = NA,
  seed = 1,
  packages = "mlr3verse"
)
```

When printing our newly created registry, we see that there are 0 problems, algorithms or jobs registered.
Among other things, we are informed that the "Interactive" cluster function is used and about the working directory for the experiments.

```{r large_benchmarking-029_experiments-029}
reg
```

The next step is to populate the registry with algorithms and problems, which we will then use to define the jobs, i.e. the resampling iterations.
We can register a problem by calling `r ref("batchtools::addProblem()")`, whose main arguments beside the registry are:

* `name` to uniquely identify the problem
* `data` to represent the static data part of a problem
* `fun`, which takes in the problem `data`, the parameters $\xi_P$ and the `job` definition, see `r ref("batchtools::makeJob()")` and returns a problem `instance`.

We register all task-resampling combinations of the `large_design` using the task ID as the name.^[The mlr3 task ID is not the same as the OpenML task ID.]
The problem `fun` takes in the static problem `data` and returns it as the problem `instance` as is.
If we were using problem parameters, we could modify the problem instance depending on their values.
In the code below, recall that the `tasks` and `resamplings` were originally used to define the `large_design`.

```{r large_benchmarking-030_experiments-030}
#| cache: false
#| output: false
for (i in seq_along(tasks)) {
  addProblem(
    name = tasks[[i]]$id,
    data = list(task = tasks[[i]], resampling = resamplings[[i]]),
    fun = function(data, ...) data,
    reg = reg
  )
}

```

::: {.callout-tip}
All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but would not have to do so.
:::

When calling `r ref("batchtools::addProblem()")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

The next step is to register the algorithm we want to run.
We can achieve this by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to uniquely identify the algorithm
* `fun`, which takes in the problem instance, the algorithm parameters $\xi_A$, and the `job` definition.
  It defines the computational steps of an experiment and its return value is the experiment result.

The algorithm function receives a list containing the task and resampling as the problem `instance`, the `learner_id` as the algorithm parameter, and the `job` object, which gives access to the experiment `$repl`.
The function creates the learner from the `learner_id`, executes the given resampling iteration, and then returns the trained learner and prediction object.

```{r large_benchmarking-032_experiments-032}
#| cache: false


addAlgorithm(
  "run_learner",
  fun = function(instance, learner_id, job, ...) {
    if (learner_id == "featureless") {
      learner = lrn("classif.featureless")
    } else {
      learner = switch(learner_id,
        logreg = lrn("classif.log_reg"),
        ranger = lrn("classif.ranger")
      )
      learner = as_learner(ppl("robustify", learner = learner) %>>% learner)
    }
    learner$id = learner_id
    # extract information from instance
    resampling = instance$resampling
    task = instance$task
    resample_iteration = job$repl

    # extract train/test split
    train_ids = resampling$train_set(resample_iteration)
    test_ids = resampling$test_set(resample_iteration)

    # train learner on train set, predict on test set
    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    # return trained learner and prediction
    list(learner = learner, prediction = prediction)
  },
  reg = reg
)

reg$algorithms
```


::: {.callout-warning}
The returned `learner` objects can become rather large and eat up a lot of disk space.
This can be made more efficient by, e.g., only returning the learner's `$state`, and removing the trained `$model` from the state.
These are rather internal data structures and you should know what you are doing when you are working with them.
The `r ref_pkg("mlr3batchmark")` package (see @sec-mlr3batchmark) implements this more efficiently.
:::

As we have now defined the problems and the algorithm, we can define concrete experiments using `r ref("batchtools::addExperiments()")`.
This function has arguments

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames. The name must match the algorithm name while the column names correspond to parameters of the algorithm.

* `repls`, the experiment replications. We use this this to represent the resample iterations. NB: This means that our replications are not iid., as the training sets of the different folds are overlapping.

In the code below, we add all resampling iterations (all resamplings use 10-fold cross-validation) for the six tasks as experiments.
Note that whenever an experiment is added, the current seed is assigned to the experiment and then incremented.

```{r large_benchmarking-033_experiments-033}
#| cache: false
#| output: false

learner_ids = list("ranger", "logreg", "featureless")
addExperiments(
  prob.designs = reg$prolems,
  algo.designs = list(run_learner = data.table(learner_id = learner_ids)),
  repls = 10
)
```

We confirm that the algorithm, problems, and experiments (jobs) were added succesfully.

```{r large_benchmarking-034_experiments-034}
reg
```



@fig-batchtools-illustration summarizes the interplay between the batchtools problems, algorithms, and experiments.

```{r large_benchmarking-037_experiments-037}
#| label: fig-batchtools-illustration
#| fig-cap: "Illustration the batchtools problem, algorithm, and experiment. "
#| fig-align: "center"
#| fig-alt: "A problem consists of a static data part and applies the problem function to this data part (and potentially problem parameters) to return a problem instance. The algorithm function takes in a problem instance (and potentially algorithm parameters), executes one job and returns its result."
#| echo: false
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

#### Job Submission {#sec-batchtools-submission}

We can summarize them defined experiments using `r ref("batchtools::summarizeExperiments()")`.
There are 30 jobs for each task, as there are 10 resampling iterations for each of the three learners.

```{r}
summarizeExperiments()
```
`r ref("batchtools::getJobTable()")` can be used to get more detailed information about the jobs.
Here, we only show a few selected columns for readability.
Among other things, we see that each job has a unique `job.id`.

```{r large_benchmarking-035_experiments-035}
#| cache: false
job_table = getJobTable(reg = reg)

head(job_table[, .(job.id, algorithm, algo.pars, problem, repl)])
```

Before submitting the jobs, it is recommended to test each algorithm individually using `r ref("batchtools::testJob()")`.
Another helpful function is `r ref("batchtools::findExperiments()")`, which allows to conveniently select a particular subset of jobs.
In the code below, we find all jobs using the featureless learner.

```{r}
#| cache: false
featureless_ids = findExperiments(algo.pars = (learner_id == "featureless"))
head(featureless_ids)
```

We test the first of these jobs and specify `external = TRUE` to run the test in an external R session.
```{r}
#| output: false
result = testJob(featureless_ids$job.id[[1L]], external = TRUE)
```

As we have configured everything correctly, the function returns the expected result.

```{r}
names(result)
```

However, if something goes wrong, `r ref_pkg("batchtools")` comes with a bunch of useful debugging utilities covered in @sec-batchtools-monitoring.

Once we are confident that the jobs are defined correctly, we can proceed with their submission, which requires:

1. specifying resource requirements for each computational job, and
1. (optionally) grouping multiple jobs into one computational job

Which resources can be configured depends on the cluster function that is set in the registry.
We earlier left it at its default value, which is the "Interactive" cluster function.
In the following we assume that we are working on a Slurm cluster.
Accordingly, we set the cluster function to a predefined "slurm-simple" cluster function, which can be constructed using `r ref("batchtools::makeClusterFunctionsSlurm()")`.

```{r large_benchmarking-038_experiments-038}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm(template = "slurm-simple")
```

```{r large_benchmarking-039_experiments-039}
#| echo: false
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

:::{.callout-tip}
It is possible to customize the cluster function.
More information is available in the documentation of the `r ref_pkg("batchtools")` package.
:::

We update the `$cluster.function` field of the registry and save it, which is necessary in this case.

```{r large_benchmarking-040_experiments-040}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry(reg)
```

The jobs are submitted to the scheduler via `r ref("batchtools::submitJobs()")`.
The most important arguments of this function besides the registry are:

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job.
  This can make sense when the jobs are short, as submitting and running computational jobs on an HPC cluster comes with an overhead (chunking, see @sec-parallelization).
* `resources`, which is a named list specifying the resource requirements for the submitted jobs.


We will chunk the jobs in such a way that all experiments on a given task are run sequentially in one computational job.
What is optimal depends on the concrete experiment and scheduling system.

```{r large_benchmarking-042_experiments-042}
chunks = data.table(job.id = job_table$job.id, chunk = rep(1:6, each = 30))
chunks[c(1, 30, 31, 60), ]
```

Furthermore, we specify the number of CPUs per computational job to 1, the wall time to 1 hour, and the RAM limit to 8 GB.

```{r}
#| eval: false
submitJobs(
  ids = chunks,
  resources = list(ncpus = 1, walltime = 3600, memory = 8000),
  reg = reg
)
```


:::{.callout-tip}
A good approach to submit computational jobs is to do this from an R session that is running persistently through multiple SSH sessions.
One option is to use TMUX (Terminal Multiplexer) or GNU Screen.
:::



```{r large_benchmarking-043_experiments-043}
#| echo: false
#| output: false
#| cache: false
if (rebuild_cache) {
  submitJobs(reg = reg)
  waitForJobs(reg = reg)
  results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
  saveRDS(results, here::here("book", "openml", "results.rds"))
} else {
  results = readRDS(here::here("book", "openml", "results.rds"))
}

```


#### Result Collection {#sec-batchtools-result}

Once the jobs are submitted, it is time to wait until their execution finishes.
Functions useful during this time (which e.g. allow to query the status of running jobs) are covered in @sec-batchtools-monitoring.
When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using `r ref("batchtools::loadResult()")`, which takes in a job id as argument `id`.
It outputs the objects returned by the algorithm function, which in our case is a list containing the trained learner and prediction object.



```{r large_benchmarking-045_experiments-045}
#| eval: false
results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))

name(results[[1L]])
```

In order to use mlr3's post-processing tools, we need to convert the results into a `r ref("BenchmarkResult")`.
We will first assemble the individual `r ref("ResampleResult")` objects and then combine them using `r ref("c()")`.

To define the resample results, we will essentially join three tables:

1. `design_table`, which will give access to the `r ref("Task")` and `r ref("Resampling")` for a given learner and task ID.
1. `job_table`, which maps the job ids to the learner and task ID.
1. `result_table`, which relates the job ids to the trained learner and prediction.

We create the design table using information from the `large_design`.

```{r}
design_table = data.table(
  learner_id = mlr3misc::map_chr(large_design$learner, "id"),
  task_id = mlr3misc::map_chr(large_design$task, "id"),
  task = large_design$task,
  resampling = large_design$resampling
)
head(design_table)
```

Next, we subset the job table and `r ref("batchtools::unwrap()")` it to unpack the list column `"algo.pars"`, resulting in the column `"learner_id"`.

```{r}
job_table2 = unwrap(job_table[, .(job.id, repl, algo.pars, task_id = problem)])
head(job_table2)
```

Then, we define the result table from the experiment `results`.

```{r}
result_table = data.table(
  job.id = job_table$job.id,
  learner = mlr3misc::map(results, "learner"),
  prediction = mlr3misc::map(results, "prediction")
)

head(result_table)
```

Finally, we merge the three tables.

```{r}
table = merge(design_table, job_table2, by = c("task_id", "learner_id"))
table = merge(table, result_table, by = "job.id")

head(table)
```

Using this table, we now loop over each `learner_id` and `task_id`, load the relevant objects from the table, and create the resample result object.


```{r}
learner_ids = unique(table$learner_id)
task_ids = unique(table$task_id)
resample_results = lapply(learner_ids, function(lid) {
  lapply(task_ids, function(tid) {
    tmp = table[learner_id == lid & task_id == tid, ]
    # the task and resampling are the same for all iterations, so we take the first
    # FIXME: as_resample_result
    ResampleResult$new(as_result_data(
      task = tmp$task[[1L]],
      learners = tmp$learner,
      resampling = tmp$resampling[[1L]],
      iterations = tmp$repl,
      predictions = list(test = tmp$prediction)
    ))
  })
})
```

Finally, we can combine the individual `r ref("ResampleResult")` objects into a benchmark result.
```{r}
resample_results = unlist(resample_results)
bmr = Reduce(c, resample_results)
bmr
```

In the next section, we will show how to simplify all of what we have just seen with the help of the `r ref_pkg("mlr3batchmark")` package.

#### Simplifying the Process using mlr3batchmark {#sec-mlr3batchmark}

In the previous section, it was demonstrated that conducting benchmark experiments in mlr3 using batchtools can be a relatively complex task.
However, this complexity can be significantly reduced by utilizing the `r ref_pkg("mlr3batchmark")` package, which seamlessly integrates the functionalities of both `r ref_pkg("mlr3")` and `r ref_pkg("batchtools")`.
Furthermore, it is also a more efficient solution than shown in the previous sections, where we have sacrificed some efficiency for simplicity.

As before, the first step is to create a new registry.
```{r large_benchmarking-046_experiments-046}
#| cache: false
reg1 = makeExperimentRegistry(NA, seed = 1, packages = "mlr3verse")
```

The whole process, of adding the algorithm, problems, and experiments can be achieved by simply replacing the `r ref("mlr3::benchmark()")` call with `r ref("mlr3batchmark::batchmark()")`.

```{r large_benchmarking-047_experiments-047}
#| output: false
batchmark(large_design, reg = reg1)
```


This does something similar as the steps that we did by hand before, i.e. register problems, algorithms, and experiments.
When printing the registry, we confirm that one algorithm, six problems, and 180 experiments are registered.

```{r large_benchmarking-048_experiments-048}
reg1
```

We can submit jobs as previously done using `r ref("batchtools::submitJobs()")`. Once the calculations are complete, we have the option to load individual results with `r ref("batchtools::loadResult()")` or obtain the full `r ref("BenchmarkResult")` by invoking `r ref("mlr3batchmark::reduceResultsBatchmark()")`.


```{r large_benchmarking-049_experiments-049}
#| echo: false
path = here::here("book", "openml", "bmr_large.rds")
if (rebuild_cache) {
  batchmark(large_design)
  submitJobs()
  waitForJobs()
  bmr = reduceResultsBatchmark(reg = reg1)
  saveRDS(bmr, path)
} else {
  bmr = readRDS(path)
}

```


```{r large_benchmarking-050_experiments-050}
#| eval: false
bmr = reduceResultsBatchmark(reg1)
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark()")` on the `large_design`.

```{r large_benchmarking-051_experiments-051}
bmr
```

#### Job Monitoring and Error Handling {#sec-batchtools-monitoring}

In any large scale experiment many things can and will go wrong, even if we test our jobs beforehand using `r ref("batchtools::testJob()")` as recommended earlier.
The cluster might have an outage, jobs may run into resource limits or crash, subtle bugs in your code could be triggered or any other error condition might arise.
In these situations it is important to quickly determine what went wrong and to recompute only the minimal number of required jobs.

We will illustrate some tools provided by `r ref_pkg("batchtools")` using the `lrn("classif.debug")` (see @sec-error-handling), where we set the error probability during training to 1.
We use the penguins task and a holdout resampling.


```{r}
#| cache: false
#| output: false
reg2 = makeExperimentRegistry(NA, seed = 1, packages = "mlr3verse")
lrn_debug = lrn("classif.debug", error_train = 1)
design = benchmark_grid(tsk("penguins"), lrn_debug, rsmp("holdout"))
batchmark(design, reg = reg2)
submitJobs(1, reg = reg2)
```

In case we suspect that something went wrong, the first thing to do is to run `r ref("getStatus()")` to display a summary of the current state of the system.

```{r}
getStatus()
```
The status message shows that the job could not be executed successfully.
To get the IDs of all jobs that failed due to an error we can use `r ref("findErrors()")` and to retrieve the actual error message, we can use `r ref("getErrorMessages()")`.

```{r}
findErrors(reg = reg2)
getErrorMessages(reg = reg2)
```
If we want to peek into the R log file of a job to see more context for the error we can use `r ref("showLog()")` which opens a pager or use `r ref("getLog()")` to get the log as character vector.
You can also grep for messages using `r ref("grepLogs()")`.

Finally, there might be some errors that cannot be avoided, e.g. when a learning algorithm fails on a specific resampling iterations.
It is generally advised to register a fallback learner to deal with such situations in a statistically sound fashion.
We have already covered this in @sec-error-handling.

## Statistical Analysis {#sec-benchmark-analysis}

Once we successfully executed the benchmark experiment, we can proceed with its analysis.
The package `r ref("mlr3benchmark")` provides infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.
Before we can use these methods, we have to convert the benchmark result to a `r ref("mlr3benchmark::BenchmarkAggr")` using `r ref("as_benchmark_aggr()")`.
We can then perform a pairwise comparison using `$friedman_posthoc()`.

```{r performance-047}
library("mlr3benchmark")
bma = as_benchmark_aggr(bmr, measures = msr("classif.acc"))
bma$friedman_posthoc()
```

These results would indicate a statistically significant difference between the `"featureless"` learner and `"ranger"`, assuming a 95% confidence level.

The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:

```{r performance-048}
autoplot(bma, type = "cd")
```

While our experiment did now show a significant difference between the random forest and the logistic regression, the former has a lower rank on average.
This is in line with the large benchmark study conducted by @couronne2018random, where the random forest outperformed the logistic regression in 69% of 243 real world datasets.


## Conclusion

In this chapter, we have explored how to conduct large scale machine learning experiments using mlr3.
We have shown how to acquire diverse datasets from OpenML through the `r ref_pkg("mlr3oml")` interface package.
Furthermore, we have learned how to execute large-scale experiments using the `r ref_pkg("batchtools")` package and its `r ref_pkg("mlr3batchmark")` integration.
Finally, we have demonstrated how to analyze the results using the `r ref_pkg("mlr3benchmark")` package, thereby extracting meaningful insights from the experiments.

The most important functions and classes we learned about are in @tbl-api-large-benchmarking alongside their R6 classes (if applicable).

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | Retrieve an OpenML Dataset                                          |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | Retrieve an OpenML Task                                             |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | Retrieve an OpenML Collection                                       |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new registry                                               |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a new Problem                                              |
| `r ref("addAlgorithm()")`           |-                           | Register a  new algorithm                                           |
| `r ref("addExperiments()")`         |-                           | Register experiments using existing algorithms and problems         |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduler                                        |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getStatus()")`              |-                           | Get the status of the computation                                   |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |
| `r ref("findExperiments()")`        |-                           | Find specific experiments                                           |
| `r ref("grepLogs()")`               |-                           | Search the log files                                                |
| `r ref("summarizeExperiments()")`   |-                           | Summarize defined experiments                                       |
| `r ref("getLog()")`                 |-                           | Get a specific log file                                             |
| `r ref("showLog()")`                |-                           | Open a specific log file                                            |
| `r ref("findErrors()")`             |-                           | Find ids of failed jobs                                             |
| `r ref("getErrorMessages()")`       |-                           | Get error messages                                                  |
| `r ref("makeJob()")`                |-                           | Create a job object                                                 |
| `r ref("as_result_data()")`         | `r ref("ResultData")`      | Create a `ResultData` object                                        |
| `r ref("as_resample_result()")`     | `r ref("ResampleResult")`  | Create a `ResampleResult` object                                    |

:Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-large-benchmarking}


<!-- TODO: Update the table -->


### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")`, and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold, and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.
- Explore the `r link("https://docs.openml.org/", "OpenML documentation")`.

## Exercises

The goal of this exercise is to repeat the previous experiment for regression tasks.
