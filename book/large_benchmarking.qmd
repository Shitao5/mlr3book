---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
    In the field of machine learning, benchmark experiments are used to evaluate the performance of algorithms and to answer scientific questions.
    Conducting such experiments involves evaluating algorithms on diverse sets of datasets, which can be acquired through resources such as [OpenML](http://www.openml.org/), which is a platform for sharing machine learning research data.
    The first section of this chapter shows how to work with OpenML using the interface package [`mlr3oml`](https://github.com/mlr-org/mlr3oml).
    However, running large-scale experiments requires not only datasets, but also significant computational resources, and it is recommended to leverage High-Performance Computing (HPC) clusters to speed up the experiment execution.
    In the second section, we will show how the R package [`batchtools`](https://github.com/mllg/batchtools) and its mlr3 integration [`mlr3batchmark`](https://github.com/mlr-org/mlr3batchmark) can considerably simplify the process of working on HPC clusters.
    Once the experiments are complete, statistical analysis is required to extract insights from the results.
    In the third section of this chapter, we show hod to do this using the [`mlr3benchmark`](https://github.com/mlr-org/mlr3benchmark) package.
---

# Large-Scale Benchmark Experiments {#sec-large-benchmarking}

{{< include _setup.qmd >}}

<!-- This chapter is a little annoying to render, because of the dependencies in the chunks. -->
<!-- If weird errors happen, delete the cache and render again. -->

```{r large_benchmarking-001_experiments-001, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
#| cache: false
set.seed(1)
lgr::get_logger("mlr3oml")$set_threshold("off")

rebuild_cache = FALSE

if (rebuild_cache) {
  unlink(here::here("book", "openml"), recursive = TRUE)
  dir.create(here::here("book", "openml"))
}

options(mlr3oml.cache = here::here("book", "openml", "cache"))

library(mlr3batchmark)
library(batchtools)
```

In the world of machine learning, there are many algorithms that are difficult to evaluate using mathematical methods alone.
Even if mathematical analysis is successful, it is often an open questions whether real-world datasets satisfy the necessary assumptions for the theorems to hold.
As a solution, researchers often resort to conducting large-scale benchmark experiments to answer fundamental scientific questions about these algorithms.
Benchmark experiments involve evaluating a variety of machine learning algorithms on a wide range of datasets, with the aim of identifying which algorithms perform best under the given circumstances.
These experiments are essential for understanding the capabilities and limitations of different algorithms and for developing new and improved approaches.
specific
To carry out benchmark experiments effectively, researchers require access to a diverse set of datasets, spanning a wide range of domains and problem types.
This is because one can only draw conclusions about the kind of datasets on which the benchmark study was conducted.
Fortunately, there are several online resources available for acquiring such datasets, with `r link("https://www.openml.org/", "OpenML")` [@openml2013] being a popular choice.
OpenML provides a vast collection of machine learning research data, making it a valuable resource for researchers looking to conduct large-scale benchmark experiments.
In the first part of this chapter, we will show how to use OpenML through its `r ref_pkg("mlr3oml")` integration.

However, evaluating algorithms on such a large scale requires significant computational resources.
To address this, researchers often utilize High-Performance Computing (HPC) clusters, which allow them to conveniently execute multiple experiments in parallel.
The R package `r ref_pkg("batchtools")` [@batchtools] is a tool for managing and executing experiments on such clusters.
In the second section we will show how it can be used to execute large machine learning experiments.
We will show how to do this with batchtools only, but will also cover how its mlr3 connector `r ref_pkg("mlr3batchmark")` can simplify the process even further.

Finally, once the experiments are complete, statistical analysis is used to extract meaningful insights from the results.
This analysis allows researchers to answer the original scientific question that motivated the benchmark experiment in the first place, providing valuable information for both theoretical and practical applications of machine learning.
We will show how to conduct such an analysis using the `r ref_pkg("mlr3benchmark")` package.

A common benchmarking scenario in machine learning is to compare a set of Learners $L_1, \ldots L_n$ by evaluating their performance on a set of tasks $T_1, \ldots, T_k$ which each have an associated resampling $R_1, \ldots, R_k$ and - for the sake of simplicity - a *single* performance measure $M$.
Such experiments can, e.g., answer which learner performs best on the given tasks w.r.t. measure $M$.
We here focus on the most frequent case of a full experimental grid design where each resample experiment is defined by the triple $E_{i, j} = (L_i, T_j, R_j)$.
Running the benchmark experiment consists of running each resample experiment and then evaluating the predictions using the measure $M$.
Running a resample experiment can again be subdivided into independent resampling iterations $E^l_{i, j}$.
For a more in-depth coverage of resampling and its purpose, we refer to @sec-resampling.
@tbl-ml-benchmark illustrates such an experiment design and should seem familiar, as it is the result of calling the `$aggregate()` method on a `r ref("BenchmarkResult")`.

| Algorithm   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a common setup for machine learning benchmark experiments. {#tbl-ml-benchmark}

@sec-performance already covered how to create and run such an experiment with `r ref_pkg("mlr3")`.
We will now briefly revisit how to do this but recommend going back to @sec-benchmarking if anything is unclear.

As a guiding example throughout this whole chapter, we will compare the random forest implementation in `r ref_pkg("ranger")` with the logistic regression method.
The following code compares a logistic regression with a random forest using a simple holdout resampling and three classification tasks that come with `r ref_pkg("mlr3")`.
The metric of choice is the classification accuracy.

```{r large_benchmarking-002_experiments-002}
#| warning: false
library(mlr3learners)

lrn_logreg = lrn("classif.log_reg")
lrn_logreg = as_learner(ppl("robustify", learner = lrn_logreg) %>>% lrn_logreg)
lrn_logreg$id = "logreg"
lrn_ranger = lrn("classif.ranger")
lrn_ranger = as_learner(ppl("robustify", learner = lrn_ranger) %>>% lrn_ranger)
lrn_ranger$id = "ranger"

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "spam")),
  list(lrn_logreg, lrn_ranger),
  rsmp("holdout")
)

bmr = benchmark(design)

result = bmr$aggregate(msr("classif.acc"))

result[, .(task_id, learner_id, classif.acc)]
```

:::{.callout-note}
The `r ref("benchmark()")` function can work on arbitrary designs, and `r ref("benchmark_grid()")` is just a helper function to create an exhaustive grid design.
If you want to evaluate a custom design, just provide a `data.table` with the columns `task`, `learner` and `resampling` for the corresponding objects.
Each row represents a resample experiment to execute.
You can combine the resulting `r ref("BenchmarkResult")` with any `r ref("BenchmarkResult")` or `r ref("ResampleResult")` using the combine function `r ref("c()")`.
Just keep in mind that special care is required during the analysis of non-exhaustive grid designs, e.g., plots or aggregated performance values might be misleading.
:::

The goal in this chapter can be summarized as turning this short example into a more realistic benchmark experiment.

## Getting Data with OpenML {#sec-openml}

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of datasets and tasks is essential.
It is therefore helpful to

1. have convenient access to a large collection of datasets and be able to filter them for specific properties, and
1. be able to easily share these with others, so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.

OpenML is a platform that facilitates the sharing and dissemination of machine learning research data and satisfies the two desiderata from above.
Like mlr3, it is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface [@feurer2021openml] or from Java.
Its goal is to make it easier for researchers to find the data they need to answer the questions they have.
Its design was guided by the `r link("https://www.go-fair.org/fair-principles/", "FAIR")` principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The purpose of these principles is to make scientific data more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.
More concretely, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible through a REST API or the web interface.

In this section we will cover some of the main features of OpenML and how to use them via the  `r ref_pkg("mlr3oml")` interface package.
OpenML supports different types of objects. 
The following is a summary of the OpenML entities that we will cover:

*   `r define("OpenML **Dataset**")`: (Usually tabular) data with additional metadata.
    The latter includes for example a description of the data, a licence, and meta-features.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("mlr3::DataBackend")`.
    As most OpenML datasets also have a designated target column, they can often directly be converted to a `r ref("mlr3::Task")`.
*   `r define("OpenML **Task**")`: A machine learning task, i.e. a concrete problem specification on an OpenML dataset.
    This includes splits into training and test sets, thereby differing from the mlr3 `r ref("Task")` definition.
    For this reason it can be converted to both a `r ref("mlr3::Task")` and corresponding instantiated `r ref("mlr3::Resampling")`.
*   `r define("OpenML **Task Collection**")`: A container object that allows to group tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 [@bischl2021openml], which is a curated collection of classification tasks.

While OpenML also supports other objects such as representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
For more information about these features, we refer to the [OpenML website](htts://openml.org) or the documentation of the `r ref_pkg("mlr3oml")` package.

### Dataset {#sec-openml-dataset}

To illustrate the OpenML dataset class, we will use the dataset with `r link("https://openml.org/d/1590", "ID 1590")` -- the well-known adult data.
We load it into R using `r ref("mlr3oml::odt()")`, which returns an object of class `r ref("OMLData")`.

```{r large_benchmarking-003_experiments-003}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about `r odata$nrow` adults -- such as their age or education -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*, which is also the default target variable.
The `r ref("OMLData")` object not only contains the data itself, but comes with additional metadata that is accessible through its fields.
This includes a licence or data qualities, which are meta features of the datasets.

```{r large_benchmarking-004_experiments-004}
odata$license
head(odata$qualities)
```

The actual data can be accessed through the `$data` field.

```{r large_benchmarking-005_experiments-005}
head(odata$data)
```

:::{.callout-tip}
When working with OpenML objects, these are downloaded piecemeal from the OpenML server.
This way you can, e.g., access the metadata without loading the data set.
While accessing the `$data` slot in the above example, the download is automatically triggered, the downloaded data is imported in R and the `data.frame` gets stored in the `odata` object.
All subsequent accesses to `$data` will be transparently redirected to the in-memory `data.frame`.
Additionally, many objects can be permanently cached on the file system. 
This which can be enabled by setting the option `mlr3oml.cache` to either `TRUE` or a specific path to be used as the cache folder.
:::

After we have loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The mlr3 class that comes closest to the OpenML dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert `r ref("OMLData")` objects by calling `r ref("as_data_backend()")`.
This is a recurring theme throughout this section: OpenML and mlr3 objects are well interoperable.

```{r large_benchmarking-006_experiments-006}
backend = as_data_backend(odata)
backend
```

If we wanted to use this dataset as a task in our comparison of the random forest with the logistic regression, we could now create an mlr3 `r ref("Task")`.

```{r large_benchmarking-007_experiments-007}
task = as_task_classif(backend, target = "class")
task
```
Alternatively, as the OpenML adult dataset comes with a default target, you can also directly convert to a task with the appropriate type:
```{r}
task = as_task(odata)
```

### Task {#sec-openml-task}

Instead of using OpenML datasets, it is also possible to use OpenML tasks, which are built on top of the former.
Thereby, not only the dataset, but also the target variable, features, and the train train-test splits are specified and can easily be shared with others.
Similar to mlr3, OpenML has different types of tasks, such as regression or classification.

A task associated with the adult data from earlier is `r link("https://openml.org/t/359983", "task 359983")`.
We can load it using the `r ref("mlr3oml::otsk()")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590.
The printed output also tells us the task type, the target variable^[This may differ from the default target used in the previous section.] and the estimation procedure.

```{r large_benchmarking-010_experiments-010}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-011_experiments-011}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.
In mlr3 terms, these are the instantiation of a mlr3 `r ref("Resampling")` on a specific task.

```{r large_benchmarking-012_experiments-012}
head(otask$task_splits)
```

The OpenML task can be converted to both an mlr3 `r ref("Task")` and a `r ref("ResamplingCustom")` instantiated on the task.
For the first of these we can use the `r ref("as_task()")` converter function.

```{r large_benchmarking-013_experiments-013}
task = as_task(otask)
task
```

The accompanying resampling can be created using the `r ref("as_resampling()")` converter.

```{r large_benchmarking-014_experiments-014}
resampling = as_resampling(otask)
resampling
```

:::{.callout-tip}
As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors, i.e. `tsk("oml", task_id = 359983)`.
:::


### Filtering {#sec-openml-filtering}

Besides working with with known IDs, another important question is how to find objects that satisfy specific properties.
Because objects on OpenML have strict metadata, they can be filtered.

We might for example only be interested in comparing the random forest and the logistic regression on datasets with less than 4 features and 100 to 1000 observations.
The `r ref("mlr3oml::list_oml_data()")` function allows to search for datasets.
By setting `number_classes = 2` we only receive datasets where the default target has two different values.
To keep the output readable, we only show the first 5 results from that query by setting the argument `limit`.

```{r large_benchmarking-008_experiments-008}
#| eval: false
odatasets = list_oml_data(
  limit = 5,
  number_features = c(1, 4),
  number_instances = c(100, 1000), 
  number_classes = 2
)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "odatasets_list.rds")
if (rebuild_cache) {
  odatasets = list_oml_data(
    limit = 5,
    number_features = c(1, 4),
    number_instances = c(100, 1000)
  )
  saveRDS(odatasets, path)
} else {
  odatasets = readRDS(path)
}
```


By looking at the table we confirm that indeed only datasets with the specified properties were returned.
We only show a subset of the columns for readability.

```{r large_benchmarking-009_experiments-009}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

:::{.callout-tip}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data()")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML `r link("https://www.openml.org/apis", "API documentation")`.
:::

Besides datasets, it is also possible to filter tasks.
This can be achieved using the  `r ref("mlr3oml::list_oml_tasks()")` and would work analogously to the example above.
<!-- As an example, suppose we are only interested in binary classification problems. -->
<!---->
<!-- ```{r large_benchmarking-018_experiments-018} -->
<!-- #| eval: false -->
<!-- otasks = list_oml_tasks( -->
<!--   type = "classif", -->
<!--   number_classes = 2, -->
<!--   limit = 5 -->
<!-- ) -->
<!-- ``` -->
<!---->
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| output: false -->
<!---->
<!-- path = here::here("book", "openml", "otasks_list.rds") -->
<!-- if (rebuild_cache) { -->
<!--   otasks = list_oml_tasks( -->
<!--     type = "classif", -->
<!--     number_classes = 2, -->
<!--     limit = 5 -->
<!--   ) -->
<!--   saveRDS(otasks, path) -->
<!-- } else { -->
<!--   otasks = readRDS(path) -->
<!-- } -->
<!-- ``` -->
<!---->
<!---->
<!---->
<!-- We limit the response to 5 and show only selected columns for readability. -->
<!---->
<!-- ```{r} -->
<!-- otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)] -->
<!-- ``` -->
<!---->
<!---->
:::{.callout-tip}
These filtering operations are also available on the OpenML [website](https://openml.org).
:::

We could now start looking at the returned IDs in more detail to verify whether they are suitable for our purposes.
Some datasets come with quirks to look out for and this process can be tedious.
A solution to this problem is to already use a predefined curated task collection, which we will cover next.

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the OpenML task collection, which bundles existing task IDs in a container object.
This allows for the creation of `r define("benchmark suites")`, which are curated collections of tasks, satisfying certain quality criteria.
Many research areas have agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time.
One example for such a benchmark suite is the `r link("https://www.openml.org/search?type=study&study_type=task&id=99", "OpenML CC-18")` which contains curated classification tasks [@bischl2021openml].
Other collections available on OpenML include the `r link("https://www.openml.org/search?type=study&study_type=task&id=271", "AutoML benchmark")` [@amlb2022] or a `r link("https://www.openml.org/search?type=study&study_type=task&id=304", "benchmark for tabular deep learning")` [@grinsztajn2022why].


```{r large_benchmarking-019_experiments-019}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML).
# This is why we load it from disk
path = here::here("book", "openml", "otask_collection.rds")
if (rebuild_cache) {
  otask_collection = ocl(id = 99)
  otask_collection$task_ids
  saveRDS(otask_collection, path)
} else {
  otask_collection = readRDS(path)
}
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl()")` function.
The printed output informs us that the CC-18 contains 72 classification tasks on different datasets.

```{r large_benchmarking-020_experiments-020}
#| eval: false
otask_collection = ocl(id = 99)
```

```{r large_benchmarking-021_experiments-021}
otask_collection
```

The tasks that are contained in the CC-18 are accessible via `$task_ids`.

```{r}
head(otask_collection$task_ids)
```

Coming back to our original goal of comparing the `r ref_pkg("ranger")` random forest implementation with the logistic regression, the next step is to create a benchmark design.
To do so, we first need to create the tasks and resamplings from the task collection.
If we wanted to get all tasks and resamplings, we could achieve this using the converters `r ref("as_tasks()")` and `r ref("as_resamplings()")`.
However, the CC-18 does not only contain binary classification tasks.
We can use the `r ref("list_oml_tasks()")` function introduced in the previous section to filter the CC-18 further.
We pass the task IDs from the CC-18 as argument `task_id` and request the number of classes to be 2.

```{r}
#| eval: false
binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids, number_classes = 2)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "binary_cc18.rds")
if (rebuild_cache) {
  binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids, number_classes = 2)
  saveRDS(binary_cc18, path)
} else {
  binary_cc18 = readRDS(path)
}
```

In order to keep the runtime in later examples low, we only use the first six tasks.

```{r}
head(binary_cc18[, .(task_id, name, NumberOfClasses)])
ids = binary_cc18$task_id[1:6]
```

As learners in our benchmark design, we use the robustified logistic regression and random forest which we have defined in the introduction.

```{r large_benchmarking-023_experiments-023}
#| eval: false
otasks = lapply(ids, otsk)

tasks = lapply(otasks, as_task)
resamplings = lapply(otasks, as_resampling)
learners = list(lrn_logreg, lrn_ranger)
```

```{r large_benchmarking-024_experiments-024}
#| echo: false
learners = list(lrn_logreg, lrn_ranger)
if (rebuild_cache) {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, here::here("book", "openml", "resamplings.rds"))

  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, here::here("book", "openml", "tasks.rds"))
} else {
  resamplings = readRDS(file.path(here::here(), "book", "openml", "resamplings.rds"))
  tasks = readRDS(here::here("book", "openml", "tasks.rds"))
}
```

The next step is to define a benchmark design, where the random forest and logistic regression algorithms are applied to each task-resampling combination.
Because the tasks and resamplings are paired, i.e. the resamplings are instantiated on the tasks, we set `paired` to` TRUE` when calling`r ref("benchmark_grid")`.

```{r large_benchmarking-025_experiments-025}
# FIXME: Change this once available in mlr3
large_design = benchmark_grid_oml(tasks, learners, resamplings)
large_design
```

## Experiment Execution on HPC Clusters {#sec-hpc-exec}

Once an experiment design is finalized, the next step is to run it.
Such experiments are conceptually straight-forward to parallelize, because we are facing an embarrassingly parallel problem (see @sec-parallelization for an explanation).
Not only are the resample experiments independent, but even their individual iterations are.
However, if the experiment is large, parallelization on a local machine as shown in @sec-parallelization might not be enough and using a distributed computing system, such as an HPC cluster, is recommended.
While access to HPC clusters is widespread, the effort required to work on these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework to conveniently run large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.

:::{.callout-note}
The package `r ref_pkg("future")` comes with a plan for `r ref_pkg("batchtools")`.
However, for larger experiments the additional control over the execution which `batchtools` offers is invaluable.
Therefore, we recommend the `"batchtools"` plan only for moderately sized experiments which complete within a couple of hours.
:::

We will now start by giving a brief summary of the HPC basics and will then show how the R packages `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` simplify working on clusters.

### HPC Basics {#sec-hpc-basics}

A High Performance Computing cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used for solving complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
An HPC cluster typically consists of multiple compute nodes, each with multiple CPU/GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously.
We will leverage this capacity to parallelize the execution of the benchmark experiment.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a scheduling system like `r link("https://slurm.schedmd.com", "Slurm")` (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the computing resources.

@fig-hpc contains a rough sketch of an HPC architecture.
Multiple users -- in this case Ann and Bob -- can login into the head node (typically via `r link("https://en.wikipedia.org/wiki/Secure_Shell", "SSH")`) and add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue.
One such instruction will be referred to as a `r define("computational job")`.
The scheduling system controls when these computational jobs are executed.

```{r large_benchmarking-026_experiments-026}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster. Ann and Bob both have access to the cluster and can log in to the head node. There, they can submit jobs to the scheduling system, which adds them to its queue and determines when they are run."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. it is cumbersome to query the status of jobs and debug failures

In the following we will show how `r ref_pkg("batchtools")` mitigates these problems.

### Batchtools {#sec-batchtools}

#### Experiment Definition {#sec-batchtools-define}

<!-- What we wanna do conceptually -->
In the following our goal is to run the benchmark design shown below (originally defined at the end of @sec-openml-collection) on an HPC cluster.

```{r large_benchmarking-027_experiments-027}
large_design
```

If we want to execute this experiment design using `r ref_pkg("batchtools")`, we need to understand how the batchtools package conceptualizes benchmark experiments.
The central concept is the experiment (or `r define("job", "Job or Experiment")`):

Experiment (or Job) $E$ is defined by applying Algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ with problem parameters $\xi_P$.

A benchmark experiment in batchtools then consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters.
Each experiment $E$ is independent of all other experiments and constitutes the basic level of computation that can be parallelized.
Note that the problem parameters $\xi_P$ can e.g. be useful for simulation studies, where datasets with different parameters are sampled.
It is also possible to run multiple repetitions of an experiment, which is useful when they are stochastic.
To be able to use batchtools for our purpose we have to translate the mlr3 benchmark experiment into the batchtools language.
There is not a single right way to do this and here we show a solution where we slightly abuse the problem parameter $\xi_P$.

In the introduction of this chapter, we have defined a benchmark experiment as running a number of resample experiments $E_{i, j} = (L_i, T_j, R_j)$ that are defined by a learner, task and resampling.
While it might seem natural to define one such triplet as a job, each resample experiment can be split up even further, namely into its iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ are number of iterations of resampling $R_j$.
We will therefore understand one resampling iteration $E^l_{i, j}$ as a job (aka. experiment).
Note that such a job does not have to coincide with the notion of a computational job defined in the previous section.
One computational job can consist of (sequentially) executing multiple jobs.
This can make sense when the jobs are short, as submitting and running computational jobs on an HPC cluster comes with an overhead.
We will now show how to define the batchtools algorithm $A$, algorithm parameters $\xi_A$, problem $P$ and problem parameters $\xi_P$ in such a way that they allow to define each resampling iteration as a job.

The first step is always to create an (or load an existing) experiment registry using the function `r ref("batchtools::makeExperimentRegistry()")` (or `r ref("batchtools::loadRegistry()")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

:::{.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry.
The R package `r ref_pkg("batchtools")` also has a different registry object that can be constructed with `r ref("batchtools::makeRegistry()")` which we will not cover in this book.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry stores the

* algorithms, problems, and job definitions
* log outputs and status of submitted, running, and finished jobs
* job results
* `r define("cluster function", "Batchtools object that allows to configure the interaction with the scheduling system.")`, which defines the interaction with the scheduling system

While the first three bullet points should be relatively clear, the fourth needs some explanation.
By configuring the scheduling system through a cluster function, it is possible to make the interaction with the scheduling system independent of the scheduling software.
We will come back to this later and show how to change it to work on a Slurm cluster.

We create a registry using a temporary directory by specifying `file.dir = NA`, but we could also provide a proper path to be used as the registry folder.
Furthermore, we set the registry's `seed` to 1 and the `packages` to mlr3verse, which will load the package in all our experiments.
<!-- FIXME: No libra -->

```{r large_benchmarking-028_experiments-028}
#| cache: false
library(batchtools)

reg = makeExperimentRegistry(
  file.dir = NA,
  seed = 1,
  packages = "mlr3verse"
)
```

When printing our newly created registry, we see that there are 0 algorithms, problems, and jobs registered.
Among other things, we are informed that the "Interactive" cluster function is used and about the working directory for the experiments.

```{r large_benchmarking-029_experiments-029}
reg
```

The next step is to populate the registry with algorithms and problems, which we will then use to define the jobs, i.e. the resampling iterations.
We can register a problem, by calling `r ref("batchtools::addProblem()")`, whose main arguments beside the registry are:

* `name` to uniquely identify the problem
* `data` to represent the static data part of a problem
* `fun`, which takes in the `data` and problem parameters $\xi_P$ and returns a concrete problem instance

We register all task-resampling combination of the `large_design` using the task ID as the name.
The problem parameters $\xi_P$ will only contain the resampling `iteration`.
The problem `fun` takes in the static problem `data` -- a list with a task and resampling -- and the problem parameter $\xi_P$ -- the resampling iteration -- and returns the problem instance by merely adding the parameter to the list.

```{r large_benchmarking-030_experiments-030}
#| cache: false
#| output: false
for (i in seq_along(tasks)) {
  addProblem(
    name = tasks[[i]]$id,
    data = list(task = tasks[[i]], resampling = resamplings[[i]]),
    fun = function(data, iteration, ...) {
      list(
        task = data$task,
        resampling = data$resampling,
        iteration = iteration
      )
    },
    reg = reg
  )
}

```

:::{.callout-tip}
All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but we would not have to do so.
:::

When calling `r ref("batchtools::addProblem()")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

```{r large_benchmarking-031_experiments-031}
reg$problems
```

The next step is to register the algorithm we want to run, which we can do by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to uniquely identify the algorithm
* `fun`, which takes in the problem instance and parameters $\xi_A$ and defines the computational steps of an experiment.
  Its output is the experiment result.

Our batchtools algorithm will execute one resampling iteration.
We will call the algorithm `"run_learner"` and make the learner itself a parameter $\xi_A$.
The algorithm function receives a problem instance and the learner, executes one iteration of a resampling and returns a list containing the learner and the prediction object.
What exactly should be returned depends on your specific problem and available memory.

```{r large_benchmarking-032_experiments-032}
#| cache: false
addAlgorithm(
  "run_learner",
  fun = function(instance, learner, ...) {
    library("mlr3verse")
    resampling = instance$resampling
    task = instance$task
    iteration = instance$iteration

    train_ids = resampling$train_set(iteration)
    test_ids = resampling$test_set(iteration)

    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    list(learner = learner, prediction = prediction)
  },
  reg = reg
)

reg$algorithms
```

:::{.callout-note}
Both the problem and algorithm `fun` also receive other arguments during execution.
These must either be explicitly named in the definitions of the algorithm and problem `fun`, or the argument `...` must be present.
For a more detailed explanation, we refer to the `r ref_pkg("batchtools")` documentation.
:::

As we have now defined a problem and an algorithm, we can define experiments with concrete algorithm and problem parameters $\xi_A$ and $\xi_B$.
We can do this using the `r ref("batchtools::addExperiments()")` function, which takes in the arguments:

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames. The name must match the algorithm name while the column names correspond to parameters of the algorithm.

The Cartesian product of algorithm and problem definitions is then added to the experiment.

In the following code, we add all 10 resampling iterations (all resamplings use 10-fold) for the six tasks as experiments.
We do this for both the random forest and logistic regression learner.
Note that whenever an experiment is added, the current seed is assigned to the experiment and then incremented.

```{r large_benchmarking-033_experiments-033}
#| cache: false
#| output: false
addExperiments(
  prob.designs = setNames(lapply(reg$problems, function(i) data.table(iteration = 1:10)), reg$problems),
  algo.designs = list(run_learner = data.table(learner = list(lrn_logreg, lrn_ranger))),
  reg = reg
)
```

When printing the registry we confirm that the algorithm, problem and experiments (jobs) were added.

```{r large_benchmarking-034_experiments-034}
reg
```


The complete workflow of adding problems, algorithms, and experiments is summarized in @fig-batchtools-illustration.

```{r large_benchmarking-037_experiments-037}
#| label: fig-batchtools-illustration
#| fig-cap: "Illustration the batchtools problem, algorithm, and experiment. "
#| fig-align: "center"
#| fig-alt: "A problem consists of a static data part and applies the problem function to this data part (and potentially problem parameters) to return a problem instance. The algorithm function takes in a problem instance (and potentially algorithm parameters), executes one job and returns its result."
#| echo: false
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

#### Job Submission {#sec-batchtools-submission}

Once the experiments are registered, the next step is to submit them.
We can summarize the defined experiments using `r ref("batchtools::summarizeExperiments()")`.
There are 20 jobs for each task, as there are 10 resampling iterations for each learner.

```{r}
summarizeExperiments()
```
We can use `r ref("batchtools::getJobTable()")` to get more detailed information about the defined jobs.
Among many other things, we see that each job has a unique ID.
We only show a few selected columns for readability.

```{r large_benchmarking-035_experiments-035}
#| cache: false
job_table = getJobTable(reg = reg)


head(job_table[, .(job.id, algorithm, algo.pars, problem, prob.pars)])
```


It is now time to translate these jobs into computational jobs, which requires:

1. specifying resource requirements for each computational, and
1. (optionally) grouping multiple jobs into one computational job.

The resource requirements that can be specified depend on the cluster function that is set in the registry.
We earlier left it at its default value, which is the "Interactive" cluster function.
In the following we assume that we are working on a Slurm cluster.
We therefore set the cluster function to a predefined "slurm-simple" cluster function, which can be constructed using `r ref("batchtools::makeClusterFunctionsSlurm()")`.

```{r large_benchmarking-038_experiments-038}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm(template = "slurm-simple")
```

```{r large_benchmarking-039_experiments-039}
#| echo: false
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

:::{.callout-tip}
It is possible to customize the cluster function.
More information is available in the documentation of the `r ref_pkg("batchtools")` package.
:::

We update the `$cluster.function` field of the registry and save it, which is necessary in this case.

```{r large_benchmarking-040_experiments-040}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry(reg)
```

We are now almost ready to submit the jobs to the cluster.
Before doing so, it is recommended to test each algorithm individually.
For this the `r ref("batchtools::testJob()")` function can be used.
Another helpful function is `r ref("batchtools::findExperiments()")`, which can be employed to conveniently select a particular subset of jobs.
It returns the IDs of all experiments that match the given criteria.
The following code snippets illustrates their usage, by first finding all jobs of the logistic regression method and then testing one of them.

```{r}
#| cache: false
logreg_ids = findExperiments(algo.pars = (learner$id == "logreg"))
head(logreg_ids)
```

We test the first of these jobs and specify `external = TRUE`, to run the test in an external R session.
```{r}
#| output: false
result = testJob(logreg_ids$job.id[[1L]], external = TRUE)
```

We have configured everything correctly and the function therefore returned the expected result.

```{r}
names(result)
```



However, if something goes wrong, `r ref_pkg("batchtools")` comes with a bunch of useful debugging utilities, some of which we will cover in @sec-batchtools-monitoring.

Once we are confident that the jobs are defined correctly, we can proceed with adding the jobs to the queue of the Slurm scheduler.
This is done by calling `r ref("batchtools::submitJobs()")`.
The most important arguments of this function besides the registry are:

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job, and
* `resources`, which is a named list specifying the resource requirements with which the computational jobs will be run.


Here we will chunk the jobs in such a way that all experiments on a given task are run sequentially in one computational job.
What is optimal depends on the concrete experiment and scheduling system.

```{r large_benchmarking-042_experiments-042}
chunks = data.table(job.id = job_table$job.id, chunk = rep(1:6, each = 20))
chunks[c(1, 20, 21, 40), ]
```

In addition to the grouping, we also specify the number of CPUs per computational job to 1, the wall time to 1 hour, and the RAM limit to 8 GB.

```{r}
#| eval: false
submitJobs(
  ids = chunks,
  resources = list(ncpus = 1, walltime = 3600, memory = 8000),
  reg = reg
)
```


:::{.callout-tip}
A good approach to submit computational jobs is do this from an R Session that is running persistently through multiple SSH sessions.
One option is to use TMUX (Terminal Multiplexer).
:::





```{r large_benchmarking-043_experiments-043}
#| echo: false
#| output: false
#| cache: false
if (rebuild_cache) {
  submitJobs(reg = reg)
  waitForJobs(reg = reg)
  results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
  saveRDS(results, here::here("book", "openml", "results.rds"))
} else {
  results = readRDS(here::here("book", "openml", "results.rds"))
}

```


#### Result Collection {#sec-batchtools-result}

Once the jobs are submitted, it is time to wait until their execution finishes.
Functions useful during this time will be covered in @sec-batchtools-monitoring.
When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using `r ref("batchtools::loadResult()")`, which takes in a job id as argument `id`.
It outputs the objects returned by the algorithm function, which in our case is a list containing the learner and prediction for the given resampling iteration.

```{r large_benchmarking-045_experiments-045}
#| eval: false
results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
```

```{r}
names(results[[1L]])
results[[1L]]$prediction
results[[1L]]$learner
```

In order to use mlr3's post-processing tools, we need to assemble the results into a `r ref("BenchmarkResult")`.
To do so, we will first assemble the individual `r ref("ResampleResult")` objects and then combine them using `r ref("c()")`.
For that, we need to group the job ids according to the resampling iteration.
We prepare the necessary information in the code below.
Here the variable `iter` is the resampling iteration and `rr_no` enumerates the resample experiments.

```{r}
job_table$learner_id = sapply(job_table$algo.pars, function(p) p$learner$id)
job_table$iter = sapply(job_table$prob.pars, function(p) p$iteration)

tbl = job_table[, 
  .(job.id, iter, rr_no = rep(.GRP, times = .N)), 
  by = list(problem, learner_id)]

head(tbl)
```

For each resample experiment, we now load all the prediction and learners.
In order to create the `r ref("ResampleResult")` object, we also need to create the task and resampling.
One way to achieve this is by creating a batchtools `Job` object, by passing the respective `job.id` to `r ref("makeJob")`.
This object stores all the information needed to run the given job in the first place.
It therefore also contains the resampling and task.

```{r}
resample_results = lapply(unique(tbl$rr_no), function(i) {
  tbl_subset = tbl[rr_no == i, .(job.id, iter)]
  ids = tbl_subset$job.id
  learners = lapply(results[ids], function(x) x$learner)
  predictions = lapply(results[ids], function(x) x$prediction)
  # We only access data from the job that is the same for all `ids`. 
  job = makeJob(ids[1L])

  ResampleResult$new(as_result_data(
    task = job$problem$data$task, 
    learners = learners,
    resampling = job$problem$data$resampling,
    iterations = tbl_subset$iter,
    predictions = predictions,
    store_backends = FALSE
  ))
})
```

Finally, we can combine the individual resample results into a benchmark result.
```{r}
bmr = Reduce(c, resample_results)
bmr
```

In the next section we will show how to simplify all of what we have just seen with the help of the `r ref_pkg("mlr3batchmark")` package.

#### Simplifying the Process using mlr3batchmark {#sec-mlr3batchmark}

First, we create a new registry.
```{r large_benchmarking-046_experiments-046}
#| cache: false
reg1 = makeExperimentRegistry(NA, seed = 1, packages = "mlr3verse")
```

The whole process, of adding the algorithm, problems, and experiments can achieved, by simply replacing the common `r ref("mlr3::benchmark()")` call with `r ref("mlr3batchmark::batchmark()")`.

```{r large_benchmarking-047_experiments-047}
#| output: false
batchmark(large_design, reg = reg1)
```


This does something similar as the steps that we had to conduct manually before, i.e. register problems, algorithms, and experiments.
When printing the registry, we see that one algorithm, six problems, and 120 experiments are registered.

```{r large_benchmarking-048_experiments-048}
reg1
```

We can now submit them like before using `r ref("batchtools::submitJobs()")`.
Once they are done, we can either load the individual results using `r ref("batchtools::loadResult()")`, or the complete `r ref("BenchmarkResult")` by calling `r ref("mlr3batchmark::reduceBatchmarkResult()")`.


```{r large_benchmarking-049_experiments-049}
#| echo: false
path = here::here("book", "openml", "bmr_large.rds")
if (rebuild_cache) {
  bmr = benchmark(large_design)
  saveRDS(bmr, path)
} else {
  bmr = readRDS(path)
}

```


```{r large_benchmarking-050_experiments-050}
#| eval: false
bmr = reduceResultsBatchmark(reg1)
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark()")` on the `large_design` above.

```{r large_benchmarking-051_experiments-051}
bmr
```



#### Job Monitoring and Error Handling {#sec-batchtools-monitoring}

In any large scale experiment many things can and will go wrong, even if we test our jobs beforehand using `r ref("testJob()")` as recommended earlier.
The cluster might have an outage, jobs may run into resource limits or crash, subtle bugs in your code could be triggered or any other error condition might arise.
In these situations it is important to quickly determine what went wrong and to recompute only the minimal number of required jobs.

We will illustrate some of the tools provided by `r ref_pkg("batchtools")` using the `lrn("classif.debug")` (see @sec-error-handling), where we set the error probability during training to 1.
We use the penguins task and a holdout resampling.


```{r}
#| cache: false
#| output: false
reg2 = makeExperimentRegistry(NA, seed = 1, packages = "mlr3verse")
lrn_debug = lrn("classif.debug", error_train = 1)
design = benchmark_grid(tsk("penguins"), lrn_debug, rsmp("holdout"))
batchmark(design, reg = reg2)
submitJobs(1, reg = reg2)
```

After you have submitted jobs and suspect that something is going wrong, the first thing to do is to run `r ref("getStatus()")` to display a summary of the current state of the system.
```{r}
getStatus()
```
The status message shows that the job could not be executed successfully.
To get the IDs of all jobs that failed due to an error we can use `r ref("findErrors()")` and to retrieve the actual error message, we can use `r ref("getErrorMessages()")`.

```{r}
findErrors(reg = reg2)
getErrorMessages(reg = reg2)
```
If we want to peek into the R log file of a job to see more context for the error we can use `r ref("showLog()")` which opens a pager or use `r ref("getLog()")` to get the log as character vector:
```{r}
tail(getLog(id = 1, reg = reg2))
```

You can also grep for messages using `r ref("grepLogs()")`.
<!---->
<!-- ```{r} -->
<!-- grepLogs(pattern = "debug", ignore.case = TRUE, reg = reg2) -->
<!-- ``` -->

Finally, there might be some errors that cannot be avoided, e.g. when a learning algorithm fails on a specific resampling iterations.
It is generally advised to register a fallback learner to deal with such situations in a statistically sound fashion.
We have already covered this in section @sec-error-handling.

## Statistical Analysis {#sec-benchmark-analysis}


The package `r ref("mlr3benchmark")` provides infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.
Before we can use these methods, we have to convert the benchmark result to a `r ref("BenchmarkAggr")`.

<!-- #| FIXME: as_benchmark_aggr -->
```{r performance-047}
library("mlr3benchmark")
bma = as.BenchmarkAggr(bmr, measures = msr("classif.acc"))
bma$friedman_test()
```
These results indicate a statistically significant difference between the random forest and the logistic regression.

:::{.callout-tip}
When comparing more than two learners the `$friedman_posthoc()` can be used for a pairwise comparison.
:::

The ranks that were used to conduct the friedman test can be accessed via `$rank_data()`.
We specify `minimize` as `FALSE`, as we are analyzing the classification accuracy.

```{r}
bma$rank_data(minimize = FALSE)
```

We can visualize the results using the `r ref("autoplot()")` function.
By default, it creates a boxplot that is grouped by task.

```{r}
autoplot(bmr)
```

We see that the random forest outperforms the logistic regression for all datasets.
This result is in line with the results from a large benchmark study conducted by @couronne2018random that investigated the same question.

## Conclusion

In this chapter, we have explored the importance of benchmark experiments in machine learning and how to conduct them effectively.
We have shown how to acquire diverse datasets from OpenML through the `r ref_pkg("mlr3oml")` interface package.
Furthermore, we have learned how to execute large-scale experiments using the `r ref_pkg("batchtools")` package and its `r ref_pkg("mlr3batchmark")` integration.
Finally, we have demonstrated how to analyze the results using the `r ref_pkg("mlr3benchmark")` package, thereby extracting meaningful insights from the experiments.

The most important functions and classes we learned about are in @tbl-apis-large-benchmarking alongside their R6 classes (if applicable).

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | OpenML Dataset                                                      |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | OpenML Task                                                         |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | OpenML Collection                                                   |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new                                                        |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a Problem                                                  |
| `r ref("addAlgorithm()")`           |-                           | Register an algorithm                                               |
| `r ref("addExperiments()")`         |-                           | Regiter experiments using existing algorithms and problems          |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduling system                                |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getStatus()")`              |-                           | Get the status of existing jobs                                     |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |
| `r ref("findExperiments()")`        |-                           | Find specific experiments                            |
| `r ref("grepLogs()")`               |-                           | Search the Log files                            |
| `r ref("summarizeExperiments()")`               |-                           | Summarize defined experiments                            |
| `r ref("getLog()")`                 |-                           | Get a specific log file                            |
| `r ref("showLog()")`                |-                           | Open a specific log file                            |
| `r ref("findErrors()")`             |-                           | Find failed job ids                            |
| `r ref("getErrorMessages()")`             |-                           | Get error messages                            |
| `r ref("makeJob()")`                |-                           | Create a job object                            |
| `r ref("as_result_data()")`         | `r ref("ResultData")`        | Create an `ResultData` object                             |
| `r ref("as_resample_result()")`         | `r ref("ResampleResult")`       | Create a `ResampleResult` object                             |

:Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-large-benchmarking}


<!-- TODO: Update the table -->


### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")` and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.
- Explore the `r link("https://docs.openml.org/", "OpenML documentation")`.

## Exercises

The goal of this exercise is to repeat the previous experiment for regression tasks.

TODO:


<!-- 1. Look at the X Benchmark Suite and filter all datasets with less than Y observations. Create the tasks and resamplings. Compare 3 learnersj -->
<!-- 1. Execute the experiment using batchmark -->
<!-- 1. Analyze an plot the results using mlr3benchmark -->

<!-- 1. In this exercise, we will learn how to work with OpenML -->
<!--    a. Look at the OpenML dataset with ID 31 on the OpenML `r link("https://openml.org", "website")`. -->
<!--       Then load it into R and create an mlr3 task from it. -->
<!--    b. One OpenML dataset can be associated with many tasks. -->
<!--       Find one classification task that is associated with the dataset from the previous exercise (you can use the `r ref("list_oml_tasks()")` function for that). -->
<!--       Construct an mlr3 task and resampling from this OpenML task. -->
<!--    c. List 10 OpenML datasets that have less than 5000 observations, no missing values, and less than 10 features. -->
<!-- 1. In this exercise, we are investigating the effect of the number of folds on the performance estimate and we will execute the experiment using `r ref_pkg("batchtools")`. -->
<!--    We are therefore changing the perspective and consider the resampling method the algorithm. -->
<!--    a. Create an experiment registry. -->
<!--    b. Add a problem to the registry that accepts problem parameters `learner_id`, `task_id` and `measure_id` and returns a list with the corresponding learner, task, and measure, retrieved from the mlr3 dictionary. -->
<!--    c. Add an algorithm for the cross-validation method that takes in parameter `folds`, evaluates the resampling and returns the scalar performance measure. -->
<!--    d. Add an experiment using the penguins task, the `classif.rpart` learner and the accuracy measure. -->
<!--       Add experiments for 2, 5, and 10 folds and repeat each experiment 12 times (read the documentation of `r ref("addExperiments")` on how to add repetitions of an experiment). -->
<!--    e. Test one of the defined jobs using the batchtools function `r ref("testJob")`. -->
<!--    f. Set the `cluster.function` of the registry to multicore `r ref("makeClusterFunctionsMulticore")` and save the registry. -->
<!--    g. Submit the jobs to the cluster and wait until they are finished. Chunk the jobs so that 3 computational jobs run for roughly the same amount of time. -->
<!--    h. Load and plot the experiment results using a box plot that compares the results for the different number of folds. -->
<!--       (Optional: Explain the results) -->

<!-- TODO: Modify the exercise. Compare 3 learners and then add a third exercise for the statistical analysis. -->
