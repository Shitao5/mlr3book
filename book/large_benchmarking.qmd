---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
    In the field of machine learning, benchmark experiments are used to evaluate the performance of algorithms and to answer scientific questions.
    Conducting such experiments involves evaluating algorithms on a diverse set of datasets, which can be acquired through resources such as [OpenML](http://www.openml.org/).
    The first section of this chapter shows how to work with OpenML using the interface package [`mlr3oml`](https://github.com/mlr-org/mlr3oml).
    However, running large-scale experiments requires significant computational resources, and it is recommended to leverage High-Performance Computing (HPC) clusters to speed up the experiment execution.
    In the second section, we will show how the R package [`batchtools`](https://github.com/mllg/batchtools) and its mlr3 integration [`mlr3batchmark`](https://github.com/mlr-org/mlr3batchmark) can considerably simplify this process.
    Once the experiments are complete, statistical analysis is used to extract insights from the results.
    We will show how to do this with the [`mlr3benchmark`](https://github.com/mlr-org/mlr3benchmark) package.
---

# Large-Scale Benchmark Experiments {#sec-large-benchmarking}

{{< include _setup.qmd >}}

<!-- This chapter is a little annoying to render, because of the dependencies in the chunks. -->
<!-- If weird errors happen, delete the cache and render again. -->

```{r large_benchmarking-001_experiments-001, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = here::here("book", "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

In the world of machine learning, there are many algorithms that are difficult to evaluate using mathematical methods alone.
As a result, researchers often resort to conducting large-scale benchmark experiments to answer fundamental scientific questions about these algorithms.
Benchmark experiments involve evaluating a variety of machine learning algorithms on a wide range of datasets, with the aim of identifying which algorithms perform best under specific circumstances.
These experiments are essential for understanding the capabilities and limitations of different algorithms and for developing new and improved approaches.

To carry out benchmark experiments effectively, researchers require access to a diverse set of datasets, spanning a wide range of domains and problem types.
This is because one can only draw conclusions about the kind of datasets on which the benchmark study was conducted.
Fortunately, there are several online resources available for acquiring such datasets, with `r link("https://www.openml.org/", "OpenML")` [@openml2013] being a popular choice.
OpenML provides a vast collection of machine learning research data, making it a valuable resource for researchers looking to conduct large-scale benchmark experiments.
In the first part of this chapter, we will show how to use OpenML through its mlr3 interface `r ref_pkg("mlr3oml")`.

However, evaluating algorithms on such a large scale requires significant computational resources.
To address this, researchers often utilize High-Performance Computing (HPC) clusters, which allow them to execute multiple experiments in parallel.
The R package `r ref_pkg("batchtools")` [@batchtools] is a tool for managing and executing experiments on HPC clusters.
In the second section we will show how it can be used to execute large machine learning experiments.
We will show how to do this with batchtools only, but will also cover how its mlr3 integration `r ref_pkg("mlr3batchmark")` can simplify the process even further.

Finally, once the experiments are complete, statistical analysis is used to extract meaningful insights from the results.
This analysis allows researchers to answer the original scientific question that motivated the benchmark experiment in the first place, providing valuable information for both theoretical and practical applications of machine learning.
We will show how to conduct such an analysis using the `r ref_pkg("mlr3benchmark")` package.

A common benchmarking scenario in machine learning is to compare a set of Learners $L_1, \ldots L_n$ by evaluating their performance on a set of tasks $T_1, \ldots, T_k$ which each have an associated resampling $R_1, \ldots, R_k$ and - for the sake of simplicity - a *single* performance measure $M$.
Such experiments can, e.g., answer which learner performs best on the given tasks w.r.t. measure $M$.
We here focus on the most frequent case of a full experimental grid design where each resampling experiment is defined by the triple $E_{i, j} = (L_i, T_j, R_j)$.
Running the benchmark experiment consists of running each resample experiment and then evaluating the predictions using the measure $M$.
Note that each resampling experiment can be subdivided into independent resampling iterations $E^l_{i, j}$ which will be relevant when parallelizing the experiment.
For a more in-depth coverage of resampling and its purpose, we refer to @sec-resampling.
@tbl-ml-benchmark illustrates such an experiment design.

| Algorithm   | Problem      | Metric     |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment. {#tbl-ml-benchmark}

@tbl-ml-benchmark should seem familiar, as it is the result of calling the `$aggregate()` method on a `r ref("BenchmarkResult")`.
@sec-performance already covered how to create and run such an experiment with `r ref_pkg("mlr3")`.
We will now briefly revisit how do do this but recommend reviewing this information if things are unclear.
As a guiding example throughout this whole chapter, we will compare the random forest implementation in `r ref_pkg("ranger")` with the logistic regression method [@couronne2018random].


The following code compares a logistic regression with a random forest using a simple holdout resampling and three classification tasks that come with `r ref_pkg("mlr3")`.
The metric of choice is the classification accuracy (`msr("classif.acc")`).

```{r large_benchmarking-002_experiments-002}
#| warning: false
library(mlr3learners)

lrn_logreg = as_learner(ppl("robustify") %>>%lrn("classif.log_reg"))
lrn_logreg$id = "logreg"
lrn_ranger = as_learner(ppl("robustify") %>>%lrn("classif.ranger"))
lrn_ranger$id = "ranger"

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  list(lrn_logreg, lrn_ranger),
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

result = bmr$aggregate(msr("classif.acc"))

result[, .(task_id, learner_id, classif.acc)]
```

:::{.callout-note}
The `r ref("benchmark()")` function can work on arbitrary designs, and `r ref("benchmark_grid()")` is just a helper function to create an exhaustive grid design.
If you want to evaluate a custom design, just provide a `data.table` with the columns `task`, `learner` and `resampling` for the corresponding objects.
Each row represents a resampling experiment to execute.
You can combine the resulting `r ref("BenchmarkResult")` with any `r ref("BenchmarkResult")` or `r ref("ResampleResult")` using the combine function `r ref("c()")`.
Just keep in mind that special care is required during the analysis of non-exhaustive grid designs, e.g., plots or aggregated performance values might be misleading.
:::

## OpenML {#sec-openml}

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of datasets and tasks is essential.
It is therefore helpful to

1. have convenient access to a large collection of datasets and tasks and be able to filter them for specific properties, and
1. be able to easily share these with others, so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.

OpenML is a platform that facilitates the sharing and dissemination of machine learning research data and satisfies the two desiderata mentioned above.
Like mlr3, it is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface [@feurer2021openml] or from Java.
Its goal is to make it easier for researchers to find the data they need to answer the questions they have.
Its design was guided by the `r link("https://www.go-fair.org/fair-principles/", "FAIR")` principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The purpose of these principles is to make scientific data more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.

More concretely, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible through a REST API or the web interface.

In this section we will cover the main features of OpenML and how to use them via the  `r ref_pkg("mlr3oml")` interface package.
This will give you an idea of what you can do with the platform and how to do it.
OpenML supports different types of objects. The following is a summary of the OpenML entities that we will cover:

*   `r define("OpenML **Dataset*8")`: (Usually tabular) data with additional metadata.
    The latter includes e.g. a description of the data and its properties, a licence, and meta-features of the dataset.
    When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("mlr3::DataBackend")`.
    As most OpenML datasets also have a designated target column, they can often directly be converted to a `r ref("mlr3::Task")`.
*   `r define("OpenML **Task**")`: A machine learning task, i.e. a concrete problem specification on an OpenML dataset.
    This includes splits into training and a test set, which differs from the mlr3 `r ref("Task")` definition.
    For this reason it can be converted to both a `r ref("mlr3::Task")` and corresponding `r ref("mlr3::Resampling")`.
*   `r define("OpenML **Task Collection**")`: A container object that allows to group tasks.
    This allows the creation of benchmark suites, such as the OpenML CC-18 [@bischl2021openml], which is a curated collection of classification tasks.

Note that while OpenML also supports other objects such as representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
For more information about these features, we refer to the [OpenML website](htts://openml.org) or the documentation of the `r ref_pkg("mlr3oml")` package.

We will cover the three object types listed above.

### Dataset {#sec-openml-dataset}

As mentioned in the introduction, our guiding use case will be the comparison of the `ranger` algorithm with a logistic regression.
Our first goal is therefore to access datasets that can be used in the comparison.

As an example, we will focus on the OpenML dataset with `r link("https://openml.org/d/1590", "ID 1590")` -- the well-known adult data.
We load it into R using the function `r ref("mlr3oml::odt()")`, which returns an object of the class `r ref("OMLData")`.

```{r large_benchmarking-003_experiments-003}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about `r odata$nrow` adults -- such as their age or education -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*.
The `r ref("OMLData")` object not only contains the data itself, but comes with additional metadata that is accessible through its fields.
This might include a licence or data qualities, which are properties of the datasets.

```{r large_benchmarking-004_experiments-004}
odata$license
head(odata$qualities)
```

The actual data can be accessed through the field `$data`.

```{r large_benchmarking-005_experiments-005}
head(odata$data)
```

:::{.callout-tip}
When working with OpenML objects, these are downloaded piecemeal from the OpenML server.
This way you can, e.g., access the metadata without loading the data set.
While accessing the `$data` slot in the above example, the download is automatically triggered, the downloaded data is imported in R and the `data.frame` gets stored in the `odata` object.
All subsequent accesses to `$data` will be transparently redirected to the in-memory `data.frame`.
Additionally, many objects can be permanently cached on the file system. 
This which can be enabled by setting the option `mlr3oml.cache` to either `TRUE` or a specific path to be used as the cache folder.
:::

After we have loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The mlr3 class that comes closest to the OpenML dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend()")`.
This is a recurring theme throughout this section: OpenML and mlr3 objects are well interoperable.

```{r large_benchmarking-006_experiments-006}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with a logistic regression, we could now create an mlr3 `r ref("Task")` from the data backend and use the mlr3 toolbox as usual.

```{r large_benchmarking-007_experiments-007}
task = as_task_classif(backend, target = "class")
task
```
Alternatively, as the OpenML adult dataset comes with a default target, you can also directly convert to a task with the appropriate type:
```{r}
task = as_task(odata)
```

### Task {#sec-openml-task}

While the previous section showed how to access and convert OpenML datasets, we will now focus on OpenML tasks that are built on top of the former.
While using datasets from OpenML makes it easy for us to share with others which datasets was used, this is not enough to ensure reproducibility.
It is also important to e.g. know which target variable, features, or train-test split was used.
Other important aspects include e.g. the choice of features and how the data was split into training and test sets.
Similar to mlr3, OpenML has different types of tasks, such as regression or classification.
Unlike mlr3, the task also comes with a concrete data-split, which would be a `r ref("Resampling")` in mlr3 terms.

The task associated with the adult data from earlier has task `r link("https://openml.org/t/359983", "ID 359983")`.
We can load it using the `r ref("mlr3oml::otsk()")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590.
The printed output also tells us the task type, the target variable^[This may differ from the default target used in the previous section.] and the estimation procedure, which is comparable with a mlr3 `r ref("Resampling")`.

```{r large_benchmarking-010_experiments-010}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-011_experiments-011}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.
In mlr3 terms, these are the instantiation of a mlr3 `r ref("Resampling")` on a specific task.

```{r large_benchmarking-012_experiments-012}
head(otask$task_splits)
```

The OpenML task can be converted to either an mlr3 `r ref("Task")` or an instantiated `r ref("ResamplingCustom")`.
For the first of these we can use the `r ref("as_task()")` converter function.

```{r large_benchmarking-013_experiments-013}
task = as_task(otask)
task
```

The accompanying resampling (instantiated on the task created above) can be created using the `r ref("as_resampling()")` converter.

```{r large_benchmarking-014_experiments-014}
resampling = as_resampling(otask)
resampling
```

:::{.callout-tip}
As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors, i.e. `tsk("oml", task_id = 359983)`.
:::


### Filtering {#sec-openml-filtering}

TODO: uebergang und die section in die story einbinden.

In the last sections, we have learned how to work with the OpenML dataset and task.
Our original goal was to conduct a benchmark study that compares the random forest with the logistic regression method.
An important question is therefore how to find relevant datasets and tasks for such a study.
Because datasets on OpenML have such strict metadata, it allows for filtering the existing dataset.
The `r ref("list_oml_data")` function allows to execute such requests from R.

We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r large_benchmarking-008_experiments-008}
#| eval: false
odatasets = list_oml_data(
  limit = 5,
  number_features = c(1, 4),
  number_instances = c(100, 1000)
)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "odatasets_list.rds")
if (!file.exists(path)) {
  odatasets = list_oml_data(
    limit = 5,
    number_features = c(1, 4),
    number_instances = c(100, 1000)
  )
  saveRDS(odatasets, path)
} else {
  odatasets = readRDS(path)
}
```


By looking at the table we confirm that indeed only datasets with the specified properties were returned.
Here, we only show a subset of the columns for readability.

```{r large_benchmarking-009_experiments-009}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

:::{.callout-tip}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data()")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML `r link("https://www.openml.org/apis", "API documentation")`.
:::

Analogously to datasets, it is also possible to filter tasks.
As an example, suppose we are only interested in binary classification problems.
We can run this query analogously to querying datasets by using the `r ref("mlr3oml::list_oml_tasks()")` function and specifying the `task_type` and `number_classes` arguments.

```{r large_benchmarking-018_experiments-018}
#| eval: false
otasks = list_oml_tasks(
  type = "classif",
  number_classes = 2,
  limit = 5
)
```

```{r}
#| echo: false
#| output: false

path = here::here("book", "openml", "otasks_list.rds")
if (!file.exists(path)) {
  otasks = list_oml_tasks(
    type = "classif",
    number_classes = 2,
    limit = 5
  )
  saveRDS(otasks, path)
} else {
  otasks = readRDS(path)
}
```



We limit the response to 5 and show only selected columns for readability.

```{r}
otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```


:::{.callout-tip}
These filtering operations can also be conducted through the [OpenML website](https://openml.org).
:::

We could now start looking at the resulting datasets and tasks (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.
However, an alternative method is to use already existing benchmark suites from OpenML, which will be the topic of the next section.

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the OpenML task collection, which bundles existing OpenML tasks in a container object.
This allows for the creation of `r define("benchmark suites")`, which are curated collections of tasks, usually satisfying certain quality criteria.
Many research areas have such agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time.
One example for such a benchmark suite is the `r link("https://www.openml.org/search?type=study&study_type=task&id=99", "OpenML CC-18")` which contains curated classification tasks [@bischl2021openml].
Other collections available on OpenML include the `r link("https://www.openml.org/search?type=study&study_type=task&id=271", "AutoML benchmark")` [amlb2022] or a `r link("https://www.openml.org/search?type=study&sort=tasks_included&study_type=task&id=304", "benchmark for tabular deep learning")` [@grinsztajn2022why].


```{r large_benchmarking-019_experiments-019}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML).
# This is why we load it from disk
otask_collection = readRDS(here::here("book", "openml", "otask_collection.rds"))
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output informs us that the  contains 72 classification tasks on different datasets.

```{r large_benchmarking-020_experiments-020}
#| eval: false
otask_collection = ocl(id = 99)
```

```{r large_benchmarking-021_experiments-021}
otask_collection
```

The task IDs that are bundled in this benchmark suite are accessible via the `$task_ids` field.

```{r}
otask_collection$task_ids
```

Coming back to our original goal of comparing the `ranger` random forest implementation with the logistic regression, the next step is to create a benchmark design.
To do so, we first need to create the tasks and resamplings from the task collection.
If we wanted to get all tasks and resamplings, we could achieve this using the converters `r ref("as_tasks()")` and `r ref("as_resamplings()")`.
However, the CC-18 does not only contain binary classification tasks.
We can use the `r ref("list_oml_tasks()")` function from the previous section to do this.

```{r}
#| eval: false
binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids)
```

```{r}
#| echo: false
#| output: false
path = here::here("book", "openml", "binary_cc18.rds")
if (!file.exists(path)) {
  binary_cc18 = list_oml_tasks(task_id = otask_collection$task_ids)
  saveRDS(binary_cc18, path)
} else {
  binary_cc18 = readRDS(path)
}
```

In order to keep the runtime in later examples manageable, we only use the first six tasks.

```{r}
head(binary_cc18[, .(task_id, name, NumberOfClasses)])

binary_ids = binary_cc18[NumberOfClasses == 2, task_id]

head(binary_ids)
ids = binary_ids[1:6]
```



We use the robustified logistic regression and random forest which we have already created earlier.

<!-- And also because the albert dataset fails ... -->

```{r large_benchmarking-023_experiments-023}
#| eval: false
otasks = lapply(binary_ids, otsk)

tasks = lapply(otasks, as_task)
resamplings = lapply(otasks, as_resampling)
learners = list(lrn_logreg, lrn_ranger)
```

```{r large_benchmarking-024_experiments-024}
#| echo: false
learners = list(lrn_logreg, lrn_ranger)
if (file.exists(file.path(here::here(), "book", "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(here::here(), "book", "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(ids, function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(here::here(), "book", "openml", "resamplings.rds"))
}

if (file.exists(file.path(here::here(), "book", "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(here::here(), "book", "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(ids, function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(here::here(), "book", "openml", "tasks.rds"))
}
```

The next step is to define a benchmark design, where the random forest and logistic regression algorithms are applied to each task-resampling combination.
Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality, which creates an experiment design from the Cartesian product of all tasks, learners and resamplings.
<!-- FIXME: Use mlr3::benchmark_grid(..., paired = TRUE) once available -->
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml()")` function which can be used in such a scenario, where tasks and resamplings are paired.

```{r large_benchmarking-025_experiments-025}
large_design = benchmark_grid_oml(tasks, learners, resamplings)
large_design
```

This is now a relatively large experiment (especially if we used all tasks and added some tuning) and in @sec-hpc-exec we will show how to run it.
Before doing so, we will now cover how to find relevant datasets and tasks on OpenML.

## Experiment Execution on HPC Clusters {#sec-hpc-exec}

Once an experiment design like in @tbl-ml-benchmark is finalized, the next step is to run it in.
Such experiments are conceptually straight-forward to parallelize, because we are facing an embarrassingly parallel problem.
Not only are the resampling experiments independent, but even the individual resampling iterations do not need to share information.
However, if the experiment is large, parallelization on a local machine (see @sec-parallelization) might not be enough and using a distributed computing system, such as an HPC cluster, is recommended.
While access to HPC clusters is widespread, the effort required to work on these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework to conveniently run large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.

:::{.callout-note}
The package `r ref_pkg("future")` comes with a plan for `r ref_pkg("batchtools")`.
However, for larger experiments the additional control over the execution which `batchtools` offers is invaluable.
Therefore, we recommend the `"batchtools"` plan only for moderately sized experiments which complete within a couple of hours.
:::

We will start by giving a brief summary of the HPC basics and will then show how the R packages `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` simplify the interaction with such clusters.

### HPC Basics {#sec-hpc-basics}

A HPC cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve.
They are used for solving complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
A HPC cluster typically consists of multiple compute nodes, each with multiple CPU/GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously, hence making it useful for benchmarking.
The most important difference between such a cluster and a personal computer (PC), is that one has no direct control over the nodes, but has to instead work with a scheduling system like `r link("https://slurm.schedmd.com", "Slurm")` (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that manages the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the computing resources.

@fig-hpc contains a rough sketch of a HPC architecture.
Multiple users -- in this case Ann and Bob -- can login into the head node (typically via `r link("https://en.wikipedia.org/wiki/Secure_Shell", "SSH")`) and add their computational workloads of the form "Execute Computation X using Resources Y for Z amount of time" to the queue.
One such instruction will be referred to as a `r define("computational job")`.
The scheduling system controls when these computational jobs are executed.

```{r large_benchmarking-026_experiments-026}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster. Ann and Bob both have access to the cluster and can log in to the head node. There, they can submit jobs to the scheduling system, which adds them to its queue and determines when they are run."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

<!-- What do we want to do with the HPC cluster -->
Common challenges when interoperating with a scheduling systems are that

1. the code to submit a job depends on the scheduling system
1. code that runs locally has to be adapted to run on the cluster
1. the burden is on the user to account for seeding to ensure reproducibility
1. query the state 

In the next section, we will show how `r ref_pkg("batchtools")` mitigates these problems.


### Batchtools {#sec-batchtools}

As a guiding example, we use the random forest experiment that we have defined in @sec-openml-collection.
We will first show how the R package `r ref_pkg("batchtools")` can be used to run this experiment on a HPC cluster.
Afterwards, we will show how the `r ref_pkg("mlr3batchmark")` package simplifies this even further.

##### Experiment Definition {#sec-batchtools-define}

<!-- What we wanna do conceptually -->
In the following our goal is to run the benchmark design shown below (originally defined at the end of @sec-openml-collection) on a HPC cluster.

```{r large_benchmarking-027_experiments-027}
large_design
```

If we want to execute this benchmark design using `r ref_pkg("batchtools")`, we need to understand how the batchtools package conceptualizes a benchmark experiment.
The central concept is the experiment (or `r define("job", "Job or Experiment")`):

Experiment (or Job) $E$ is defined by applying Algorithm $A$ using algorithm parameters $\xi_A$ to problem $P$ with problem parameters $\xi_P$.

A benchmark experiments in batchtools then consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters.
Each experiment $E$ is independent of all other experiments and constitutes the basic level of computation that can be parallelized.
Note that the problem parameters $\xi_P$ can e.g. be useful for simulation studies, where datasets with different parameters are sampled.

If we want to utilize batchtools to execute the `large_design` from above, we therefore have to translate the mlr3 resample experiments into the batchtools language.
There is not a single right way to do this, and we here show one solution, where we slightly abuse the problem parameter $\xi_P$ for our purposes.
In the introduction of this chapter, we have defined a ML benchmark experiment as running a number of resample experiments $E_{i, j} = (L_i, T_j, R_j)$ that are defined by a learner, task and resampling.
While it might seem natural to define one such resample experiment as a job, each resample experiment can be decomposed even further, namely into its iterations $E^1_{i, j}, ..., E_{i, j}^{n_j}$, where $n_j$ are number of iterations of resampling $R_j$.
We will therefore understand one resampling iteration $E^l_{i, j}$ as a job (aka. experiment).
Note that such a job does not have to coincide with the notion of a computational job defined in the previous section.
One computational job can consist of (sequentially) executing multiple jobs.
This can make sense when the jobs are short, as submitting and running computational jobs on a HPC cluster comes with an overhead.

We will now continue with showing how to define the batchtools algorithms $A$, algorithm parameters $\xi_A$, problems $P$ and problem parameters $\xi_P$ in such a way that they allow to define each resampling iteration as a job.
The first step is always to create an (or load an existing) experiment registry using the function `r ref("batchtools::makeExperimentRegistry()")` (or `r ref("batchtools::loadRegistry()")`).
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder in the file system.

:::{.callout-note}
Whenever we refer to a registry in this section, we mean an experiment registry.
The R package `r ref_pkg("batchtools")` also has a different registry object that can be constructed with `r ref("batchtools::makeRegistry()")` which we will not cover in this book.
:::

<!-- So why do we need this registry? -->

Among other things, the experiment registry gives access to

* algorithms, problems, and job definitions
* log outputs and status of submitted, running, and finished jobs
* job results
* `r define("cluster function", "Batchtools object that allows to configure the interaction with the scheduling system.")`, which defines the interaction with the scheduling system

While the first three bullet points should be relatively clear, the fourth needs some explanation.
By configuring the scheduling system through a cluster function, it is possible to make the interaction with the scheduling system independent on the scheduling software.
We will come back to this later and show how to change it to work on a Slurm cluster.

We create a registry using a temporary directory by specifying `file.dir = NA` below, but we could also provide a proper path to be used as the registry folder.
We also set a global seed for the registry.

```{r large_benchmarking-028_experiments-028}
#| cache: false
library(batchtools)

reg = makeExperimentRegistry(file.dir = NA, seed = 1)
```

When printing the registry, we see that (unsurprisingly) there are 0 algorithms, problems, and jobs registered.
We are also informed that the "Interactive" cluster function is used and about the working directory for the experiments.

```{r large_benchmarking-029_experiments-029}
reg
```

The next step is to populate the registry with algorithms and problems, which we will then use to define the jobs, i.e. the resampling iterations.
We can register a problem, by calling `r ref("batchtools::addProblem()")` whose main arguments beside the registry are:

* `name` to uniquely identify the problem
* `data` to represent the static data part of a problem
* `fun` takes in the `data` and problem parameters $\xi_P$ and returns a concrete problem instance

We register all task-resampling combination of the `large_design` using the task ID as the name.
We define the problem parameter $\xi_P$ as the resampling iteration.
The problem `fun` takes in the static problem `data` -- a list with a task and resampling -- and the problem parameter $\xi_P$ -- the resampling iteration -- and returns the problem instance by adding the parameter to the list.

```{r large_benchmarking-030_experiments-030}
#| cache: false
for (i in seq_along(tasks)) {
  addProblem(
    name = tasks[[i]]$id,
    data = list(task = tasks[[i]], resampling = resamplings[[i]]),
    fun = function(data, iteration, ...) {
      list(
        task = data$task,
        resampling = data$resampling,
        iteration = iteration
      )
    },
    reg = reg
  )
}

```

:::{.callout-tip}
All batchtools functions that interoperate with a registry, take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but we would not have to do so.
:::

When calling `r ref("batchtools::addProblem()")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

```{r large_benchmarking-031_experiments-031}
reg$problems
```

The next step is to register the algorithm we want to run, which we can do by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to uniquely identify the algorithm
* `fun` which takes in the problem instance and parameters $\xi_A$ and defines what is computed when running an experiment. Its output is the experiment result.

Our goal at the start was to define one job as one resampling iteration, so the algorithm should execute a single resampling iteration.
We will call the algorithm that we add `"run_learner"` and make the learner itself the parameter $\xi_A$.
It receives a problem instance and the learner, executes one iteration of a resampling and returns the learner's state and the predictions as the experiment result.
The return value should therefore contain all the relevant information, but should also not contain too much data, in order to avoid bloating the usually limited memory on HPC clusters.
Note that could have also added one algorithm for each learner and included the respective hyperparameters as algorithm parameters $\xi_A$.
What is best, depends on the concrete problem at hand and there is no single best solution.

```{r large_benchmarking-032_experiments-032}
#| cache: false
addAlgorithm(
  "run_learner",
  fun = function(instance, learner, ...) {
    library("mlr3verse")
    resampling = instance$resampling
    task = instance$task
    iteration = instance$iteration

    train_ids = resampling$train_set(iteration)
    test_ids = resampling$test_set(iteration)

    learner$train(task, row_ids = train_ids)
    prediction = learner$predict(task, row_ids = test_ids)

    list(state = learner$state, prediction = prediction)
  },
  reg = reg
)

reg$algorithms
```

:::{.callout-note}
Both the problem and algorithm `fun` also receive other arguments during execution.
These must either be explicitly named in the definitions of the algorithm and problem `fun`, or the argument `...` must be present.
For a more detailed explanation, we refer to the `r ref_pkg("batchtools")` documentation.
:::

As we have now defined a problem and an algorithm, we can define experiments with concrete algorithm and problem parameters $\xi_A$ and $\xi_B$.
We can do this using the `r ref("batchtools::addExperiments()")` function, which takes in the arguments:

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames (or ‘data.table’). The name must match the algorithm name while the column names correspond to parameters of the algorithm.

The Cartesian product of algorithm and problem definitions is then added to the experiment.

In the following code, we add all 10 resampling iterations for the six tasks as experiments.
We do this for both the logistic regression and ranger learner.

```{r large_benchmarking-033_experiments-033}
#| cache: false
addExperiments(
  prob.designs = setNames(lapply(reg$problems, function(i) data.table(iteration = 1:10)), reg$problems),
  algo.designs = list(run_learner = data.table(learner = list(lrn_logreg, lrn_ranger))),
  reg = reg
)
```

Note that whenever an experiment is added, the global seed set for the registry is incremented and saved for the experiment to ensure reproducibility.

When printing the registry again, we confirm that the algorithm, problem and experiments (jobs) were added.

```{r large_benchmarking-034_experiments-034}
reg
```

We can summarize the defined experiments using `r ref("summarizeExperiments()")`.
There are 20 experiments for each task, i.e. 10 resampling iterations for both learners respectively.

```{r}
summarizeExperiments()
```


#### Job Submission {#sec-batchtools-submission}

Once the experiments are added to the experiment registry, the next step is to submit them.
To get more detailed information about the jobs we have defined, we can use `r ref("batchtools::getJobTable()")`.
This returns a table summarizing information of all added jobs.
Among many other things, we see that each job has a unique ID with which it can be referenced in many batchtools functions.

```{r large_benchmarking-035_experiments-035}
#| cache: false
job_table = getJobTable(reg = reg)


job_table[1:2, c("job.id", "algorithm", "algo.pars", "problem", "prob.pars")]
```

The complete workflow of adding algorithms, problems, and experiments is summarized in the image below.

```{r large_benchmarking-037_experiments-037}
#| label: fig-batchtools-illustration
#| fig-cap: "Illustration of batchtools problem, algorithm, and experiment"
#| fig-align: "center"
#| fig-alt: "A problem consists of a static data part and applies the problem function to this data part (and potentially problem parameters) to return a problem instance. The algorithm function takes in a problem instance (and potentially algorithm parameters), executes one job and returns its result."
#| echo: false
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```

We have defined 10 jobs, each representing one resampling iteration and it is time to translate these jobs into computational jobs, which includes:

1. specifying resource requirements for each computational
1. (optionally) grouping multiple jobs into one computational job.

The resource requirements that can be specified depend on the cluster function that is set in the registry.
We earlier left the cluster function at its default value, which was the "Interactive" cluster function that is primarily intended for exploration of the package and testing of experiments.

Because we now want to submit the jobs to the scheduling system, the time has come to properly configure it.
We assume for this example that we are operating a Slurm cluster.
Therefore, we set the cluster function to a simple Slurm cluster function that can be constructed using `r ref("batchtools::makeClusterFunctionsSlurm()")`.

```{r large_benchmarking-038_experiments-038}
#| eval: false
slurm_fn = makeClusterFunctionsSlurm(template = "slurm-simple")
```

```{r large_benchmarking-039_experiments-039}
#| echo: false
slurm_fn = makeClusterFunctionsInteractive()
slurm_fn$name = "Slurm"
```

We update the `$cluster.function` field of the registry and save the registry, which is necessary in this case.

```{r large_benchmarking-040_experiments-040}
#| eval: false
reg$cluster.function = slurm_fn
saveRegistry()
```

We are now almost ready to submit the jobs to the cluster.
Before submitting all jobs to the batch system, we encourage you to test each algorithm individually.
Or sometimes you want to submit only a subset of experiments because the jobs vastly differ in runtime.
Another reoccurring task is the collection of results for only a subset of experiments.
For all these use cases, `r ref("findExperiments()")` can be employed to conveniently select a particular subset of jobs.
It returns the IDs of all experiments that match the given criteria.
To test an experiment, the `r ref("testJob")` function can be used.
To illustrate the usage of these functions, we will select an experiment and test it in an external R session, by specifying `external = TRUE`.
```{r}
#| cache: false
logreg_ids = findExperiments(algo.pars = (learner$id == "logreg"))
head(logreg_ids)
```

```{r}
#| output: false
testJob(logreg_ids$job.id[[1L]], external = TRUE)
```



If something goes wrong, `batchtools` comes with a bunch of useful debugging utilities, see @sec-batchtools-monitoring.
If everything turns out fine, we can proceed with the calculation.

Assuming that we are running this program on the head node of the Slurm cluster, we can call `r ref("batchtools::submitJobs()")` to add the previously defined experiments to the queue of the Slurm scheduler.
The most important arguments of this function besides the registry are:

* `ids`, which are either a vector of job ids to submit, or a data frame with columns `job.id` and `chunk`, which allows to group multiple jobs into one computational job, and
* `resources`, which is a named list specifying the resource requirements with which the computational jobs will be run.

The ids we refer to here are the ids that can e.g. be accessed as the `job.id` column from the job table obtained earlier.

```{r large_benchmarking-041_experiments-041}
head(job_table$job.id)
```

In this example will chunk the jobs in such a way, that all experiments on a given task are chunked together.
What is optimal depends on the used cluster.

```{r large_benchmarking-042_experiments-042}
chunks = data.table(job.id = 1:120, chunk = rep(1:6, each = 20))
head(chunks)
```

In addition to the grouping, we also specify the number of CPUs per computational job to 1, the wall time to 1 hour and the RAM limit to 8 GB.

```{r}
#| eval: false
submitJobs(
  ids = chunks,
  resources = list(ncpus = 1, walltime = 3600, memory = 8000),
  reg = reg
)
```


:::{.callout-tip}
Once some jobs have already finished, the function `r ref("estimateRuntimes()")` can be used, to estimate the runtimes.
If not all jobs can be submitted at the same time, functions like `r ref("findNotSubmitted()")` or `r ref("findDone()")` can be useful.
:::




```{r large_benchmarking-043_experiments-043}
#| echo: false
#| output: false
#| cache: false
if (!file.exists(here::here("book", "openml", "results.rds"))) {
  submitJobs(reg = reg)
  waitForJobs(reg = reg)
  results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
  saveRDS(results, here::here("book", "openml", "results.rds"))
} else {
  results = readRDS(here::here("book", "openml", "results.rds"))
}

```


#### Result Collection {@sec-batchtools-result}

Once the jobs are correctly submitted, it is time to wait until their execution is finished.
While the computational jobs are being processed, `r ref_pkg("batchtools")` exposes many useful functions that allow the user to access information about the submitted, running, expired, or failed computational jobs.
This includes e.g. `r ref("batchtools::getStatus()")` to get the status of the running job, `r ref("batchtools::showLog()")` to inspect the logs or `r ref("batchtools::grepLogs()")` to search the log files.

:::{.callout-tip}
A good approach to submit comptuational jobs is do this from an R Session that is running persistently through multiple SSH sessions.
One option is to use TMUX (Terminal Multiplexer).
:::

When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using the `r ref("batchtools::loadResult()")` function which takes in a job id as argument `id`, as well as a registry.
It outputs the objects returned by the `fun` of the algorithm (added by `r ref("addAlgorithm")`), which is a list containing the learner's state and the prediction for a specific resampling iteration.

```{r large_benchmarking-045_experiments-045}
#| eval: false
results = lapply(job_table$job.id, function(i) loadResult(i, reg = reg))
```

```{r}
names(results[[1L]])
results[[1L]]$prediction
```



In order to use the visualization and analysis tools that are provided by mlr3, we need to create a `r ref("BenchmarkResult")` object.
While it is possible to do this by hand, it is much more convenient to use the `r ref_pkg("mlr3batchmark")` package to simplify the process..

#### Simplifying the Process using mlr3batchmark {#sec-mlr3batchmark}


First, we create a new registry.
```{r large_benchmarking-046_experiments-046}
reg1 = makeExperimentRegistry(NA, seed = 1)
```

The whole process, of adding the algorithm, problems, and experiments can be done by swapping the common `r ref("mlr3::benchmark()")` call with the `r ref("mlr3batchmark::batchmark()")` function.

```{r large_benchmarking-047_experiments-047}
#| output: false
batchmark(large_design)
```


This does something similar as the steps that we he executed manually before.
When printing the registry, we see that one algorithm, six problems, and 120 experiments are registered.

```{r large_benchmarking-048_experiments-048}
reg1
```

We can now submit them like before using `r ref("batchtools::submitJobs()")`.
Once they are done, we can either load the individual results using `r ref("batchtools::loadResult()")`, or the complete `r ref("BenchmarkResult")` by calling `r ref("mlr3batchmark::reduceBatchmarkResult()")`.


```{r large_benchmarking-049_experiments-049}
#| echo: false
if (file.exists(here::here("book", "openml", "bmr_large.rds"))) {
  bmr = readRDS(here::here("book", "openml", "bmr_large.rds"))
} else {
  bmr = benchmark(large_design)
  saveRDS(bmr, here::here("book", "openml", "bmr_large.rds"))
}

```


```{r large_benchmarking-050_experiments-050}
#| eval: false
bmr = reduceBatchmarkResult()
```

The resulting object is the same (except for seeding) that we would have obtained by calling `r ref("benchmark()")` on the `large_design` above.

```{r large_benchmarking-051_experiments-051}
bmr
```



#### Job Monitoring, Debugging and Error Handling {#sec-batchools-monitoring}

In any large scale experiment many things can and will go wrong, even if we test our jobs beforehand using `r ref("testJob")` as recommended earlier.
The cluster might have an outage, jobs may run into resource limits or crash, subtle bugs in your code could be triggered or any other error condition might arise.
In these situations it is important to quickly determine what went wrong and to recompute only the minimal number of required jobs.

We will illustrate some of the tools provided by `r ref_pkg("batchtools")` using the `lrn("classif.debug")`, where we set the error probability during training to 1.
We use the penguins task and the


```{r}
#| cache: false
reg2 = makeExperimentRegistry(NA, seed = 1)

lrn_debug = lrn("classif.debug", error_train = 1)

design = benchmark_grid(tsk("penguins"), lrn_debug, rsmp("holdout"))


batchmark(design)

submitJobs()
```

After you have submitted jobs and suspect that something is going wrong, the first thing to do is to run `r ref("getStatus()")` to display a summary of the current state of the system.
```{r}
getStatus()
```
The status message shows that two of the jobs could not be executed successfully.
To get the IDs of all jobs that failed due to an error we can use `r ref("findErrors()")` and to retrieve the actual error message, we can use `r ref("getErrorMessages()")`.

```{r}
findErrors()
getErrorMessages()
```
If we want to peek into the R log file of a job to see more context for the error we can use [`showLog()`](https://mllg.github.io/batchtools/reference/showLog) which opens a pager or use [`getLog()`](https://mllg.github.io/batchtools/reference/showLog) to get the log as character vector:
```{r}
tail(getLog(id = 1))
```

You can also grep for messages.

```{r}
grepLogs(pattern = "debug", ignore.case = TRUE)
```

Finally, there might be some errors that cannot be handled, e.g. when a learning algorithm fails on a specific resampling iterations.
For this reason, it is generally advised to register a fallback learner.
We have already covered this in section @sec-error-handling.

## Statistical Analysis {#sec-benchmark-analysis}

As we have now successfully conducted our experiment, the next step is the analysis of results.
As a first step, we visualize the results using the `r ref("autoplot")` function.

```{r}
autoplot(bmr)
```

The package `r ref("mlr3benchmark")` we previously used for ranking also provides infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.

```{r}
library("mlr3benchmark")




```




`$friedman_posthoc()` can be used for a pairwise comparison:

```{r performance-047}
library(mlr3benchmark)
bma = as.BenchmarkAggr(bmr, measures = msr("classif.acc"))
bma$friedman_test()
```


:::{.callout-tip}
When comparing more than 2 learners, the `$friedman_posthoc()` method can be used for pairwise comparisons.
:::


These results would indicate a statistically significant difference between the `"featureless"` learner and `"ranger"`, assuming a 95% confidence level.

The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:

<!-- ```{r performance-048} -->
<!-- autoplot(bma, type = "cd") -->
<!-- ``` -->

Similar to the test output before, this visualization leads to the conclusion that the `"featureless"` learner and `"ranger"` are significantly different, whereas the critical rank difference of 1.66 is not exceed for the comparison of the `"featureless"` learner, `"rpart"` and `"ranger"`, respectively.

The resuls are inline with the findings from @couronne2018random.


## Conclusion

In this chapter we have learned about two tools -- OpenML and batchtools -- that both aid in the design and execution of (large-scale) benchmark experiments.
OpenML is a well-structured repository for machine learning research data that -- among many other things -- allows to access, share and filter datasets, tasks, and task collections.
Furthermore, we have shown how the R package `r ref_pkg("batchtools")` and its mlr3 connector `r ref_pkg("mlr3batchmark")` can considerably simplify the execution of large-scale benchmark experiments on HPC clusters.
TODO: Update this

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | OpenML Dataset                                                      |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | OpenML Task                                                         |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | OpenML Collection                                                   |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new                                                        |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("addProblem()")`             |-                           | Register a Problem                                                  |
| `r ref("addAlgorithm()")`           |-                           | Register an algorithm                                               |
| `r ref("addExperiments()")`         |-                           | Regiter experiments using existing algorithms and problems          |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduling system                                |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("getJobStatus()")`           |-                           | Get the status of existing jobs                                     |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |
| `r ref("findExperiments()")` |-                           | Find specific experiments                            |

TODO: Update the table

:Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-ls-benchmarking}

### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")` and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.

## Exercises

The goal of this exercise is to repeat the previous experiment for regression tasks.

1. In this exercise, we will learn how to work with OpenML
   a. Look at the OpenML dataset with ID 31 on the OpenML `r link("https://openml.org", "website")`.
      Then load it into R and create an mlr3 task from it.
   b. One OpenML dataset can be associated with many tasks.
      Find one classification task that is associated with the dataset from the previous exercise (you can use the `r ref("list_oml_tasks()")` function for that).
      Construct an mlr3 task and resampling from this OpenML task.
   c. List 10 OpenML datasets that have less than 5000 observations, no missing values, and less than 10 features.
1. In this exercise, we are investigating the effect of the number of folds on the performance estimate and we will execute the experiment using `r ref_pkg("batchtools")`.
   We are therefore changing the perspective and consider the resampling method the algorithm.
   a. Create an experiment registry.
   b. Add a problem to the registry that accepts problem parameters `learner_id`, `task_id` and `measure_id` and returns a list with the corresponding learner, task, and measure, retrieved from the mlr3 dictionary.
   c. Add an algorithm for the cross-validation method that takes in parameter `folds`, evaluates the resampling and returns the scalar performance measure.
   d. Add an experiment using the penguins task, the `classif.rpart` learner and the accuracy measure.
      Add experiments for 2, 5, and 10 folds and repeat each experiment 12 times (read the documentation of `r ref("addExperiments")` on how to add repetitions of an experiment).
   e. Test one of the defined jobs using the batchtools function `r ref("testJob")`.
   f. Set the `cluster.function` of the registry to multicore `r ref("makeClusterFunctionsMulticore")` and save the registry.
   g. Submit the jobs to the cluster and wait until they are finished. Chunk the jobs so that 3 computational jobs run for roughly the same amount of time.
   h. Load and plot the experiment results using a box plot that compares the results for the different number of folds.
      (Optional: Explain the results)

TODO: Modify the exercise. Compare 3 learners and then add a third exercise for the statistical analysis.
