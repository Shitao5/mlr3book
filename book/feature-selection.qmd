# Feature Selection {#sec-feature-selection}

{{< include _setup.qmd >}}

`r authors("Feature Selection")`

`r index('Feature selection')`, also known as variable or descriptor selection, is the process of finding an optimal subset of features to use with a given task and learner, which can:

* improve predictive performance, since we reduce overfitting on irrelevant features;
* make models more robust by not relying on noisy features;
* simplify models for easier interpretation;
* quicken model fitting, e.g. for model updates; and
* reduce prediction time.

However, as with hyperparameter optimization (HPO), these objectives will not necessarily be optimized by the same `r define('optimal set')` of features and thus feature selection is often `r index('multi-objective')`.

A particularly common use of feature selection is in `r define('dimension reduction')`, which is the process of selecting a small number of features from a `r index('high-dimensional')` dataset, i.e., a dataset with many features, which may even exceed the number of datapoints.
The majority of feature selection methods are model agnostic, i.e. they can be used with any learner, however some models perform specific feature selection methods implicitly (also known as 'embedded feature selection'), for example decision trees select particular features when choosing how to create the next split in a node.
There are many different approaches to identifying relevant features but in this chapter we will only focus on `r index('filter')` and `r index('wrapper')` methods [@guyon2003;@chandrashekar2014], which make use of `r mlr3filters` and `r mlr3fselect` respectively.

## Filters {#sec-fs-filter}

Applying a filter method is a preprocessing step that takes place *before* training a model.
In general, filter algorithms take a three step approach:

1. Calculate a numeric score for all $p$ features in the dataset, $\zeta_1,...,\zeta_p$
2. Rank numeric scores, $r_1,...,r_p$
3. EITHER:
   1. Select features based on the value of $\zeta$, i.e., select feature $i$ if $\zeta_i \in [\alpha, \beta]$, where $\alpha, \beta$ are some thresholds (possibly infinite); OR
   2. Select features based on the value of ranking, e.g., select the top five ranked features $r_1,..,r_5$ or the top 50% of features $r_1,...,r_{p/2}$

Let's consider this in a concrete example.
Say we want to select a subset of features in the `mtcars` dataset to predict `mpg` based on feature correlation.
First we would compute the correlation of all features to the target:

```{r}
# compute absolute correlations and remove correlation of mpg to mpg
#  and sort from largest to smallest
correlations = setNames(sort(abs(cor(mtcars[, "mpg"], mtcars)), TRUE),
  colnames(mtcars))[-1]
correlations
```

Then we could either select features where the correlation is greater than some value, say 0.5:

```{r}
names(correlations[correlations > 0.5])
```

Or we could select the most correlated features, say the top 5:

```{r}
names(correlations[1:5])
```

In practice, this is quite a simplistic example as correlations can only be computed for continuous features and the chosen thresholds were fairly arbitrary.

The correlation example demonstrates a `r define('univariate filter')`, which only considers the univariate relationship between each feature (separately) and the target variable.
The largest benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods (@sec-fs-wrapper).
However, more advanced `r define('multivariate filter')` methods, which consider the relationship between features as well as the target, usually perform better [@bommert2020].
Another special case is `r index('feature importance filters')` (@sec-fs-var-imp-filters) , which select features that are important according to the model induced by a selected `Learner`.
The advantage of these methods is that they are embedded in the given `Learner` and therefore do not *require* any further data preprocessing, however they are not model agnostic and may leave the user with less control over the filter process, so additional filtering *may* still be useful.

In this section we will describe how to use `r mlr3filters` to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.

Many filter methods are implemented in `r mlr3filters`, particularly common methods are:

* Correlation (`r ref("FilterCorrelation")`) -- Calculates Pearson, Kendall, or Spearman correlation between numeric features and targets.
* Information gain (`r ref("FilterInformationGain")`) -- Calculates mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature.
* Minimal joint mutual information maximization (JMIM) (`r ref("FilterJMIM")`) -- Minimizes joint information between selected features to avoid redundancy.
* Permutation score (`r ref("FilterPermutation")`) -- Calculates permutation feature importance (@sec-interpretation) with a given learner for each feature.
* AUC `r ref("FilterAUC")` -- Calculates area under the ROC curve calculated for each feature separately.

For a full list of all implemented filter methods we refer the reader to the `r link("https://mlr3filters.mlr-org.com", "mlr3filters website")`, which also shows the supported task and features types.

All filters have advantages and disadvantages.
@bommert2020 recommend to not rely on a single filter method but try several ones if the available computational resources allow.
If only a single filter method can be tested, the authors recommend to use a feature importance filter using random forest permutation importance (@sec-fs-var-imp-filters); JMIM and AUC filters also performed well in their comparison.

### Model-Agnostic Filters {#sec-fs-calc}

Similarly to other classes in `mlr3`, `r ref("Filter")`s are stored in the `r ref("mlr_filters")` dictionary and can be accessed with `r ref("flt()")`.
Filter algorithms are run by calling the `$calculate()` method on your chosen `Filter`.
For example, using the correlation filter on the `mtcars` dataset:

```{r}
library(mlr3verse)
# load filter
filter = flt("correlation", method = "spearman")
filter
# load task
task = tsk("mtcars")
# calculate and return scores
filter$calculate(task)
filter$scores
```

Note how we changed the parameter `method` in the 'usual way' by setting the desired value in construction.

### Model-Dependent Filters {#sec-fs-var-imp-filters}

Feature importance methods and embedded filters, are model-dependent as they depend on the model of choice, and the returned scores/features will depend on the algorithm, chosen hyperparameters, and underlying implementation.
Embedded filters are so-called as the filter algorithm is part of the model training process, a common example is decision trees, which selectively choose variables to split the model during training.
This can be thought of as a filter method as after training only a subset of the filters will have been used by the model.

```{r}
lrn = lrn("regr.rpart")
task = tsk("mtcars")
lrn$train(task)$model
```

In the example above, we can see the decision tree has only selected `r length(lrn$selected_features())` (``r lrn$selected_features()``) features out of the possible `r task$ncol`.

In contrast, some learners will calculate importance scores during training without explicitly selecting features.
For example, the magnitude of weights fit by a linear regression can be thought of as an importance method as the value of the fitted coefficients tells us how much a given variable influences the outcome.

```{r}
lrn = lrn("regr.lm")
task = tsk("mtcars")
# remove intercept and sort coefficients in decreasing order
scores = sort(abs(lrn$train(task)$model$coefficients), TRUE)[-1]
scores
```

In this example we can see all variables have been used but the absolute coefficients vary in size with ``r names(scores)[1]`` contributing the most to the final prediction and ``r names(scores)[10]`` contributing the least.

#### Feature Importance {.unnumbered .unlisted}

Feature importance filters are available for any `Learner` with the "importance" property:

```{r}
head(as.data.table(mlr_learners)[
  sapply(properties, function(x) "importance" %in% x)])
```

The `r ref("FilterImportance")` class is a wrapper that trains the provided `Learner` and then passes the importance scores to the `$scores` field:

```{r feature-selection-005}
lrn = lrn("regr.rpart")
task = tsk("mtcars")

# Manually training a learner and returning `$importance`
lrn$train(task)$importance()

# Using the filter method
filter = flt("importance", learner = lrn)
filter$calculate(task)
filter$scores
```

:::{.callout-tip}
Some learners will have a choice of methods for feature importance, for example `classif.ranger` has four possible choices:

```{r}
lrn("regr.ranger")$param_set$levels$importance
```
:::

#### Embedded Filters {.unnumbered .unlisted}

Embedded filters are available for any `Learner` with the "selected_features" property:

```{r feature-selection-004}
head(as.data.table(mlr_learners)[
  sapply(properties, function(x) "selected_features" %in% x)])
```

The `r ref("FilterSelectedFeatures")` class is a wrapper that trains the provided `Learner` and then passes the selected features to the `$scores` field.
By example, again using `mtcars` and `regr.rpart`:

```{r feature-selection-007}
# Looking at selected features from the trained learner
lrn$selected_features()

# Now with the filter method
filter = flt("selected_features", learner = lrn)
filter$calculate(task)
filter$scores
```

Contrary to other filter methods, embedded methods just return scores of either 1 for selected features or 0 fordropped features.

### Selecting Features from Filters

Once scores have been calculated, the next step is to select features based on the score value or ranking.
For embedded methods, selecting features is trivial as scores are either 1 or 0, so we would simply select the columns of a task corresponding to the selected features:

```{r feature-selection-009}
task = tsk("mtcars")
filter = flt("selected_features", learner = lrn("regr.rpart"))
filter$calculate(task)
# $select requires vector of column names
task$select(names(which(filter$scores == 1)))
task$feature_names
```

For other filter methods, the choice of threshold for scores or rankings is arbitrary.
So returning to our example from the top of the chapter using the correlation filter with `mtcars`:

```{r feature-selection-010}
task = tsk("mtcars")
filter = flt("correlation")
filter$calculate(task)

# we could select top 5 features
task$clone()$select(names(filter$scores[1:5]))$feature_names

# or all features with correlation > 0.5
task$clone()$select(names(filter$scores > 0.5))
task$feature_names
```

The process of calculating scores, thresholding and selecting features, and filtering the task, can be simplified using `r ref("mlr_pipeops_filter")`, which we will discuss after @sec-pipelines.
Making use of this pipeline also allows hyperparameter optimization allowing us to automate which features to select.
<!-- FIXME: ADD REFERENCE BELOW -->
We will return to this in detail in FIXME!!!, for now here is a brief preview where we create a `r ref("Graph")` that we then tune to identify the optimal number of features to select:

```{r feature-selection-012}
library(mlr3verse)
task = tsk("mtcars")

# create filter PipeOp
graph = as_learner(po("filter", filter = flt("correlation")) %>>%
  po("learner", lrn("regr.rpart")))
graph$param_set$values$correlation.filter.nfeat = to_tune(1, task$ncol)

tt = tune(tnr("random_search"), task, graph, rsmp("holdout"), term_evals = 10)
tt$result
```

In this example we find that '`r tt$result$correlation.filter.nfeat`' is the optimal number of features.

## Wrapper Methods {#sec-fs-wrapper}

Wrapper-based feature selection is very similar to hyperparameter optimization (@sec-optimization) with the major difference being that instead of testing different hyperparameter configurations, we instead test different feature configuration, i.e., different subsets of features.
Common, simple wrappers can involve sequential approaches, e.g. `r index('sequential forward selection')`, where features are iteratively added to the model, as well as parallel methods, e.g. by evaluating random feature subsets.

Given the similarity to HPO, wrappers can be 'tuned' manually or automatically, which is all handled by the `r mlr3fselect` package.
We recommend reading @sec-optimization before this section as the design interface for wrappers is very similar to tuners and so we do not repeat common technical details.

Methods and classes in `r mlr3fselect` can be mapped to those in `r mlr3tuning` (@tbl-fselect-tuning) and so if you have read the previous chapter the code in this section will seem very familiar to you.
Concretely we can select feature selection algorithms with `fs` and then either manually construct an instance (single- or multi-objective) with `fsi`, or automatically start the selection process with `fselect`.

| mlr3tuning method | mlr3fselect method | Purpose |
| ----------------- | ------------------ | ------- |
| `r ref("Tuner")`/`r ref("tnr")` | `r ref("FSelector")`/`r ref("fs")` | `fs` loads the chosen wrapper as an `FSelector` |
| `TuningInstanceX`/`r ref("ti")` | `FSelectInstanceX`/`r ref("fsi")` | `fsi` constructs `r ref("FSelectInstanceSingleCrit")` or `r ref("FSelectInstanceMultiCrit")` depending on selected measures |
| `r ref("tune")`   | `r ref("fselect")`  | Automatically construct an `FSelectInstance` then start the selection process |
| `r ref("AutoTuner")`/`r ref("auto_tuner")` | `r ref('AutoFSelector')`/`r ref("auto_fselector")` | `auto_fselector` constructs `AutoFSelector` for automated nested resampling during feature selection optimization |

: Comparison of mlr3tuning and mlr3fselect methods {#tbl-fselect-tuning}

### `fs` and `fsi`

Let's look at the 'manual' approach first with `fs` and `fsi` to find the optimal set of features using a random search (random subset of features in each iteration), a decision tree, holdout resampling in each fold, and terminating after 20 evaluations.

```{r}
library(mlr3fselect)

# subset features to ease visualization
task = tsk("penguins")
task$select(c("bill_depth", "bill_length", "body_mass", "flipper_length"))

# select feature selection method
fselector = fs("random_search")

# manually creating an FSelectInstance
instance = fsi(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 20)
)

# run the feature selection method
fselector$optimize(instance)
```

We can then view the optimal feature set with `$result_feature_set`:

```{r}
instance$result_feature_set
```

Note that `$result_feature_set` shows features in alphabetical order and not in the order selected.
We could instead view the `archive` to see which features were tried in each batch:

```{r}
dt = as.data.table(instance$archive)
dt[batch_nr %in% 1:5, 1:5]
```

`r mlr3fselect` uses the same terminators as `r mlr3tuning`, which are discussed fully in @sec-terminator.
`r mlr3fselect` includes the following `FSelector` methods:

* Random search (`r ref("FSelectorRandomSearch")`) -- Try random feature subsets until termination
* Exhaustive search(`r ref("FSelectorExhaustiveSearch")`) -- Try all possible feature subsets
* Sequential search (`r ref("FSelectorSequential")`) -- Sequentially add or remove features in each iteration
* Recursive feature elimination (`r ref("FSelectorRFE")`) -- Makes use of a learner's importance scores to iteratively remove features with low feature importance
* Design points (`r ref("FSelectorDesignPoints")`) -- Compares feature sets specified by the user
* Genetic search(`r ref("FSelectorGeneticSearch")`) -- A genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations
* Shadow variable search(`r ref("FSelectorShadowVariableSearch")`) -- Adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected
<!-- FIXME: SAY SEE DOCS FOR MORE DETAILS AND IN @guyon2003 -->

### `fselect` and `auto_fselector`
<!-- fixme: ping BB for image illustrating below -->
Using `fselect` can automate the selection procedure by automatically constructing the required instance object and then calling `$optimize`.
In the example below we use the sequential forward selection method, which does not require a terminator:

```{r}
instance = fselect(
  fselector = fs("sequential"),
  task =  task,
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)

instance$result_feature_set
```

This could then be passed back to our task for future model training:

```{r}
task$select(instance$result_feature_set)
```

Though as we have seen in @sec-autotuner, nested resampling is required in order to reduce bias when estimating model performance.
Similarly to HPO, this is possible with `r ref("AutoFSelector")`:
<!-- FIXME: BELOW USE FORWARD SELECTION AND SHOW OUTPUT INSTEAD -->
```{r feature-selection-030, warning=FALSE}
at = auto_fselector(
  fselector = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

design = benchmark_grid(tsk("sonar"),
  c(at, lrn("classif.log_reg")), rsmp("cv", folds = 3))
benchmark(design)$aggregate()
```

In the above example we compared the a logistic regression with and without feature selection, and find that feature selection successfully improved the model's accuracy.

### Multi-objective feature selection

Finally, it is also possible to optimize multiple measures simultaneously to find the `r index('pareto optimal')` solution, which in this case refers to the set of feature subsets for which any subset outside this set would decrease performance in at least one measure.
As in `r mlr3tuning`, optimization with multiple criteria is implemented in a special class, in this case `r ref("FSelectInstanceMultiCrit")` but once again this is handled automatically and no changes need to be made by the user, other than specifying multiple metrics:

```{r feature-selection-026}
instance = fselect(
  fselector = fs("exhaustive_search"),
  task = tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  # optimize for highest accuracy and lowest training
  measure = msrs(c("classif.acc", "time_train")),
  terminator = trm("evals", n_evals = 10)
)

as.data.table(instance$result)[, .(features, classif.acc, time_train)]
```

```{r}
#| label: fig-pareto
#| fig-cap: Pareto front of training time and accuracy. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.
#| fig-alt: Scatter plot with time_train on x-axis and classif.acc on y-axis. Purple dots represent simulated tested configurations of time_train vs. classif.acc and blue dots and a blue line along the bottom-left of the plot shows the Pareto front.
#| echo: false
library(ggplot2)
library(viridisLite)

ggplot(as.data.table(instance$archive), aes(x = time_train, y = classif.acc)) +
  geom_point(
    data = ,
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[1],
    alpha = 0.8,
    stroke = 0.5) +
  geom_step(
    data = instance$archive$best(),
    direction = "hv",
    colour = viridis(3, end = 0.8)[2],
    linewidth = 1) +
  geom_point(
    data = instance$archive$best(),
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[2],
    alpha = 0.8,
    stroke = 0.5) +
  theme_minimal()
```

## Conclusion

In this chapter, we learned how to perform feature selection with `mlr3`.
We introduced filter and wrapper methods, described their implementation in `r mlr3filters` and `r mlr3fselect`, and covered automated optimization of one and more performance measures.
@tbl-api-feature-selection gives an overview of the most important functions (S3) and classes (R6) used in this chapter.

If you are interested in learning more about feature selection methodology we recommend @chandrashekar2014 and @guyon2003, @bommert2020 also includes a good benchmark of filter methods.


| Function/method | Associated R6 Class | Summary |
| --- | --- | --- |
| `r ref("flt()")`   | `r ref("Filter")` | Selects features by calculating a score for each feature |
| `$calculate()`   | `r ref("Filter")` | Calculates scores on a given task |
| `r ref("fselect()")` | `r ref("FSelectInstanceSingleCrit")` or  `r ref("FSelectInstanceMultiCrit")` | Specifies a feature selection problem and stores the results |
| `r ref("fs()")` | `r ref("FSelector")` | Specifies a feature selection algorithm |
| `$optimize()` | `r ref("FSelector")` | Executes the features selection specified by the `FSelectInstance` with the algorithm specified by the `FSelector` |
| `r ref("auto_fselector()")` | `r ref("AutoFSelector")` | Defines a learner that includes feature selection |

:Core S3 'sugar' functions for feature selection in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-feature-selection}

## Exercises

1. Calculate a information gain filter on the `boston_housing` task.
2. Use the filter from the first exercise to select the five best features in the `boston_housing` data set.
3. Apply a backward selection wrapper to the `penguins` task with a classification tree learner (`classif.rpart`), holdout resampling, and accuracy measure.
4. Perform automated feature selection with the `spam` task, a logistic regression learner (`classif.log_reg`), random search (terminating after 15 iterations), holdout inner and outer resampling, and the `classif.bbrier` measure. Formally compare this to a decision tree without feature selection on their accuracy and Brier performance. Which performs better?
