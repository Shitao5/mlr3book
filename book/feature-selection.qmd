# Feature Selection {#sec-feature-selection}

{{< include _setup.qmd >}}

`r authors("Feature Selection")`

`r index('Feature selection')`, also known as variable or descriptor selection, is the process of finding an optimal subset of features to use with a given task and learner, which can:

* improve predictive performance, since we reduce overfitting on irrelevant features;
* make models more robust by not relying on noisy features;
* simplify models for easier interpretation;
* quicken model fitting, e.g. for model updates; and
* reduce prediction time.

However, these objectives will not necessarily be optimized by the same `r define('optimal set')` of features and thus feature selection is inherently `r index('multi-objective')`.
In this chapter, we will primarily focus on feature selection as a means of improving predictive performance, but we will also briefly cover optimization of multiple criteria (@sec-multicrit-featsel).

A particularly common use of feature selection is in `r define('dimension reduction')`, which is the process of selecting a small number of features from a `r index('high-dimensional')` dataset, i.e., a dataset with many features, which may even exceed the number of datapoints.
The majority of feature selection methods are model agnostic, i.e. they can be used with any learner, however some models perform specific feature selection methods implicitly (also known as 'embedded feature selection'), for example decision trees select particular features when choosing how to create the next split in a node.
There are many different approaches to identifying relevant features but in this chapter we will only focus on `r index('filter')` and `r index('wrapper')` methods [@guyon2003;@chandrashekar2014], which make use of `r mlr3filters` and `r mlr3fselect` respectively.

## Filters {#sec-fs-filter}

Applying a filter method is a preprocessing step that takes place *before* training a model.
In general, filter algorithms take a three step approach:

1. Calculate a numeric score for all $p$ features in the dataset, $\zeta_1,...,\zeta_p$
2. Rank numeric scores, $r_1,...,r_p$
3. EITHER:
   1. Select features based on the value of $\zeta$, i.e., select feature $i$ if $\zeta_i \in [\alpha, \beta]$, where $\alpha, \beta$ are some thresholds (possibly infinite); OR
   2. Select features based on the value of ranking, e.g., select the top five ranked features $r_1,..,r_5$ or the top 50% of features $r_1,...,r_{p/2}$

Let's consider this in a concrete example.
Say we want to select a subset of features in the `mtcars` dataset to predict `mpg` based on feature correlation.
First we would compute the correlation of all features to the target:

```{r}
# compute absolute correlations and remove correlation of mpg to mpg
#  and sort from largest to smallest
correlations = setNames(sort(abs(cor(mtcars[, "mpg"], mtcars)), TRUE),
  colnames(mtcars))[-1]
correlations
```

Then we could either select features where the correlation is greater than some value, say 0.5:

```{r}
names(correlations[correlations > 0.5])
```

Or we could select the most correlated features, say the top 5:

```{r}
names(correlations[1:5])
```

In practice, this is quite a simplistic example as correlations can only be computed for continuous features and the chosen thresholds were fairly arbitrary.

The correlation example demonstrates a `r define('univariate filter')`, which only considers the univariate relationship between each feature (separately) and the target variable.
The largest benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods (@sec-fs-wrapper).
However, more advanced `r define('multivariate filter')` methods, which consider the relationship between features as well as the target, usually perform better [@bommert2020].
Another special case is `r index('feature importance filters')` (@sec-fs-var-imp-filters) , which select features that are important according to the model induced by a selected `Learner`.
The advantage of these methods is that they are embedded in the given `Learner` and therefore do not *require* any further data preprocessing, however they are not model agnostic and may leave the user with less control over the filter process, so additional filtering *may* still be useful.

In this section we will describe how to use `r mlr3filters` to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.

Many filter methods are implemented in `r mlr3filters`, particularly common methods are:

* Correlation (`r ref("FilterCorrelation")`) -- Calculates Pearson, Kendall, or Spearman correlation between numeric features and targets.
* Information gain (`r ref("FilterInformationGain")`) -- Calculates mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature.
* Minimal joint mutual information maximization (JMIM) (`r ref("FilterJMIM")`) -- Minimizes joint information between selected features to avoid redundancy.
* Permutation score (`r ref("FilterPermutation")`) -- Calculates permutation feature importance (@sec-interpretation) with a given learner for each feature.
* AUC `r ref("FilterAUC")` -- Calculates area under the ROC curve calculated for each feature separately.

For a full list of all implemented filter methods we refer the reader to the `r link("https://mlr3filters.mlr-org.com", "mlr3filters website")`, which also shows the supported task and features types.

All filters have advantages and disadvantages.
@bommert2020 recommend to not rely on a single filter method but try several ones if the available computational resources allow.
If only a single filter method can be tested, the authors recommend to use a feature importance filter using random forest permutation importance (@sec-fs-var-imp-filters); JMIM and AUC filters also performed well in their comparison.

### Model-Agnostic Filters {#sec-fs-calc}

Similarly to other classes in `mlr3`, `r ref("Filter")`s are stored in the `r ref("mlr_filters")` dictionary and can be accessed with `r ref("flt()")`.
Filter algorithms are running by calling the `$calculate()` method on your chosen `Filter`.
For example, using the correlation on the `mtcars` dataset:

```{r}
library(mlr3verse)
# load filter
filter = flt("correlation", method = "spearman")
filter
# load task
task = tsk("mtcars")
# calculate and return scores
filter$calculate(task)
filter$scores
```

Note how we changed the parameter `method` in the 'usual way' by setting the desired value in construction.

### Model-Dependent Filters {#sec-fs-var-imp-filters}

Feature importance methods and embedded filters, are model-dependent as they depend on the model of choice, and the returned scores/features will depend on the algorithm, chosen hyperparameters, and underlying implementation.
Embedded filters are so-called as the filter algorithm is part of the model training process, a common example is decision trees, which selectively choose variables to split the model during training.
This can be thought of as a filter method as after training only a subset of the filters will have been used by the model.

```{r}
lrn = lrn("regr.rpart")
task = tsk("mtcars")
lrn$train(task)$model
```

In the example above, we can see the decision tree has only selected `r length(lrn$selected_features())` (``r lrn$selected_features()``) features out of the possible `r task$ncol`.

In contrast, some learners will calculate importance scores during training without explicitly selecting features.
For example, the magnitude of weights fit by a linear regression can be thought of as an importance method as the value of the fitted coefficients tells us how much a given variable influences the outcome.

```{r}
lrn = lrn("regr.lm")
task = tsk("mtcars")
# remove intercept and sort coefficients in decreasing order
scores = sort(abs(lrn$train(task)$model$coefficients), TRUE)[-1]
scores
```

In this example we can see all variables have been used but the absolute coefficients vary in size with ``r names(scores)[1]`` contributing the most to the final prediction and ``r names(scores)[10]`` contributing the least.

#### Feature Importance {.unnumbered .unlisted}

Feature importance filters are available for any `Learner` with the "importance" property:

```{r}
head(as.data.table(mlr_learners)[
  sapply(properties, function(x) "importance" %in% x)])
```

The `r ref("FilterImportance")` class is a wrapper that trains the provided `Learner` and then passes the importance scores to the `$scores` field:

```{r feature-selection-005}
lrn = lrn("regr.rpart")
task = tsk("mtcars")

# Manually training a learner and returning `$importance`
lrn$train(task)$importance()

# Using the filter method
filter = flt("importance", learner = lrn)
filter$calculate(task)
filter$scores
```

:::{.callout-tip}
Some learners will have a choice of methods for feature importance, for example `classif.ranger` has four possible choices:

```{r}
lrn("regr.ranger")$param_set$levels$importance
```
:::

#### Selected Features {.unnumbered .unlisted}

Selected features filters are available for any `Learner` with the "selected_features" property:

```{r feature-selection-004}
head(as.data.table(mlr_learners)[
  sapply(properties, function(x) "selected_features" %in% x)])
```

The `r ref("FilterSelectedFeatures")` class is a wrapper that trains the provided `Learner` and then passes the selected features to the `$scores` field.
By example, again using `mtcars` and `regr.rpart`:

```{r feature-selection-007}
# Looking at selected features from the trained learner
lrn$selected_features()

# Now with the filter method
filter = flt("selected_features", learner = lrn)
filter$calculate(task)
filter$scores
```

Contrary to other filter methods, embedded methods just return scores of either 1 for selected features or 0 fordropped features.

### Selecting Features from Filters

Once scores have been calculated, the next step is to select features based on the score value or ranking.
For embedded methods, selecting features is trivial as scores are either 1 or 0, so we would simply select the columns of a task corresponding to the selected features:

```{r feature-selection-009}
task = tsk("mtcars")
filter = flt("selected_features", learner = lrn("regr.rpart"))
filter$calculate(task)
# $select requires vector of column names
task$select(names(which(filter$scores == 1)))
task$feature_names
```

For other filter methods, the choice of threshold for scores or rankings is arbitrary.
So returning to our example from the top of the chapter using the correlation filter with `mtcars`:

```{r feature-selection-010}
task = tsk("mtcars")
filter = flt("correlation")
filter$calculate(task)

# we could select top 5 features
task$clone()$select(names(filter$scores[1:5]))$feature_names

# or all features with correlation > 0.5
task$clone()$select(names(filter$scores > 0.5))
task$feature_names
```

The process of calculating scores, thresholding and selecting features, and filtering the task, can be simplified using `r ref("mlr_pipeops_filter")`, which we will discuss after @sec-pipelines.
Making use of this pipeline also allows hyperparameter optimization allowing us to automate which features to select.
We will return to this in detail in FIXME!!!, for now here is a brief preview where we create a `r ref("Graph")` that we then tune to identify the optimal number of features to select:

```{r feature-selection-012}
library(mlr3verse)
task = tsk("mtcars")

# create filter PipeOp and set `filter.nfeat` to select top 3 features
graph = as_learner(po("filter", filter = flt("correlation"), filter.nfeat = 3) %>>%
  po("learner", lrn("regr.rpart")))
graph$param_set$values$correlation.filter.nfeat = to_tune(1, task$ncol)

tune(tnr("random_search"), task, graph, rsmp("holdout"), term_evals = 10)$result
```

## Wrapper Methods {#sec-fs-wrapper}

Wrapper methods work by fitting models on selected feature subsets and evaluating their performance.
This can be done in a sequential fashion, e.g. by iteratively adding features to the model in the so-called sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search.
Below, the use of these simple approaches is described in a common framework along with more advanced methods such as genetic search.
It is further shown how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.

In more detail, wrapper methods iteratively select features that optimize a performance measure.
Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated in resampling with respect to a selected performance measure.
The strategy that determines which feature subset is used in each iteration is given by the `FSelector` object.
A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively adds the feature that leads to the largest performance improvement.
Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method.
All wrapper methods are implemented via the package `r mlr3fselect`.
In this chapter, we cover how to

* instantiate an `FSelector` object,
* configure it, to e.g. respect a runtime limit or for different objectives,
* run it or fuse it with a `Learner` via an `AutoFSelector`.

::: {.callout-note}
Wrapper-based feature selection is very similar to hyperparameter optimization (@sec-optimization).
The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations.
We will see below, that we can even use the same terminators, that some feature selection algorithms are similar to tuners and that we can also optimize multiple performance measures with feature selection.
:::

### Simple Forward Selection Example {#sec-fs-wrapper-example}

We start with the simple example from above and do sequential forward selection with the penguins data:

```{r feature-selection-017, message=FALSE}
library("mlr3fselect")

# subset features to ease visualization
task = tsk("penguins")
task$select(c("bill_depth", "bill_length", "body_mass", "flipper_length"))

instance = fselect(
  fselector = fs("sequential"),
  task =  task,
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)
```

To show all analyzed feature subsets and the corresponding performance, we use `as.data.table(instance$archive)`.

```{r feature-selection-018}
dt = as.data.table(instance$archive)
dt[batch_nr == 1, 1:5]
```

We see that the feature `flipper_length` achieved the highest prediction performance in the first iteration and is thus selected.
In the second round, adding `bill_length` improves performance to over 90%:

```{r feature-selection-018-2}
dt[batch_nr == 2, 1:5]
```

However, adding a third feature does not improve performance

```{r feature-selection-018-3}
dt[batch_nr == 3, 1:5]
```

and the algorithm terminates.
To directly show the best feature set, we can use:

```{r feature-selection-019}
instance$result_feature_set
```

::: {.callout-note}
`instance$result_feature_set` shows features in alphabetical order and not in the order selected.
:::

Internally, the `fselect` function creates an `FSelectInstanceSingleCrit` object and executes the feature selection with an `FSelector` object, based on the selected method, in this example an `FSelectorSequential` object.
It uses the supplied resampling and measure to evaluate all feature subsets provided by the `FSelector` on the task.

At the heart of `r mlr3fselect` are the R6 classes:

* `FSelectInstanceSingleCrit`, `FSelectInstanceMultiCrit`: These two classes describe the feature selection problem and store the results.
* `FSelector`: This class is the base class for implementations of feature selection algorithms.

In the following two sections, these classes will be created manually, to learn more about the `r mlr3fselect` package.

### The `FSelectInstance` Classes

To create an `FSelectInstanceSingleCrit` object, we use the sugar function `r ref("fsi")`, which is short for `FSelectInstanceSingleCrit$new()` or `FSelectInstanceMultiCrit$new()`, depending on the selected measure(s):

```{r feature-selection-020}
instance = fsi(
  task = tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 20)
)
```

Note that we have not selected a feature selection algorithm and thus did not select any features, yet.
We have also supplied a so-called `Terminator`, which is used to stop the feature selection.
For the forward selection in the example above, we did not need a terminator because we simply tried all remaining features until the full model or no further performance improvement.
However, for other feature selection algorithms such as `r ref("FSelectorRandomSearch", text = "random search")`, a terminator is required.
The following terminator are available:

* Terminate after a given time (`r ref("TerminatorClockTime")`)
* Terminate after a given amount of iterations (`r ref("TerminatorEvals")`)
* Terminate after a specific performance is reached (`r ref("TerminatorPerfReached")`)
* Terminate when feature selection does not improve (`r ref("TerminatorStagnation")`)
* A combination of the above in an *ALL* or *ANY* fashion (`r ref("TerminatorCombo")`)

Above we used the sugar function `r ref("trm")` to select `r ref("TerminatorEvals")` with 20 evaluations.

To start the feature selection, we still need to select an algorithm which are defined via the `FSelector` class, described in the next section.

### The `FSelector` Class

The `FSelector` class is the base class for different feature selection algorithms.
The following algorithms are currently implemented in `r mlr3fselect`:

* Random search, trying random feature subsets until termination (`r ref("FSelectorRandomSearch")`)
* Exhaustive search, trying all possible feature subsets (`r ref("FSelectorExhaustiveSearch")`)
* Sequential search, i.e. sequential forward or backward selection (`r ref("FSelectorSequential")`)
* Recursive feature elimination, which uses learner's importance scores to iteratively remove features with low feature importance (`r ref("FSelectorRFE")`)
* Design points, trying all user-supplied feature sets (`r ref("FSelectorDesignPoints")`)
* Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (`r ref("FSelectorGeneticSearch")`)
* Shadow variable search, which adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected (`r ref("FSelectorShadowVariableSearch")`)

In this example, we will use a simple random search and retrieve it from the dictionary `r ref("mlr_fselectors")` with the `r ref("fs()")` sugar function, which is short for `FSelectorRandomSearch$new()`:

```{r feature-selection-021}
fselector = fs("random_search")
```

### Starting the Feature Selection

To start the feature selection, we pass the `FSelectInstanceSingleCrit` object to the `$optimize()` method of the initialized `FSelector` object:

```{r feature-selection-022, output=FALSE}
fselector$optimize(instance)
```

The algorithm proceeds as follows

1. The `FSelector` proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting `batch_size`.
1. For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.
1. All evaluations are stored in the archive of the `FSelectInstanceSingleCrit` object.
1. The terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is.
1. Determine the feature subset with the best observed performance.
1. Store the best feature subset as the result in the instance object.

The best feature subset and the corresponding measured performance can be accessed from the instance:

```{r feature-selection-023}
as.data.table(instance$result)[, .(features, classif.acc)]
```

As in the forward selection example above, one can investigate all resamplings which were undertaken, as they are stored in the archive of the `FSelectInstanceSingleCrit` object and can be accessed by using `as.data.table()`:

```{r feature-selection-024}
as.data.table(instance$archive)[, .(bill_depth, bill_length, body_mass, classif.acc)]
```

Now the optimized feature subset can be used to subset the task and fit the model on all observations:

```{r feature-selection-025, eval=FALSE}
task = tsk("penguins")
learner = lrn("classif.rpart")

task$select(instance$result_feature_set)
learner$train(task)
```

The trained model can now be used to make a prediction on external data.

::: {.callout-warning}
Predicting on observations present in the task used for feature selection should be avoided.
The model has seen these observations already during feature selection and therefore performance evaluation results would be over-optimistic.
Instead, to get unbiased performance estimates for the current task, nested resampling (see @sec-autofselect and @sec-nested-resampling) is required.
:::

### Optimizing Multiple Performance Measures {#sec-multicrit-featsel}

You might want to use multiple criteria to evaluate the performance of the feature subsets.
For example, you might want to select the subset with the highest classification accuracy and lowest time to train the model.
However, these two subsets will generally not coincide, i.e. the subset with highest classification accuracy will probably be another subset than that with lowest training time.
With `r mlr3fselect`, the result is the pareto-optimal solution, i.e. the best feature subset for each of the criteria that is not dominated by another subset.
For the example with classification accuracy and training time, a feature subset that is best in accuracy *and* training time will dominate all other subsets and thus will be the only pareto-optimal solution.
If, however, different subsets are best in the two criteria, both subsets are pareto-optimal.

We will expand the previous example and perform feature selection on the penguins dataset, however, this time we will use `r ref("FSelectInstanceMultiCrit")` to select the subset of features that has the highest classification accuracy and the one with the lowest time to train the model.

The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:

```{r feature-selection-026}
instance = fsi(
  task = tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msrs(c("classif.acc", "time_train")),
  terminator = trm("evals", n_evals = 5)
)
```
The function `r ref("fsi")` creates an instance of `FSelectInstanceMultiCrit` if more than one measure is selected.
We now create an `FSelector` and call the `$optimize()` function of the `FSelector` with the `FSelectInstanceMultiCrit` object, to search for the subset of features with the best classification accuracy and time to train the model.
This time, we use `r ref("FSelectorDesignPoints", text = "design points")` to manually specify two feature sets to try: one with only the feature `sex` and one with all features except `island`, `sex` and `year`.
We expect the sex-only model to train fast and the model including many features to be accurate.

```{r feature-selection-027, output=FALSE}
design = mlr3misc::rowwise_table(
  ~bill_depth, ~bill_length, ~body_mass, ~flipper_length, ~island, ~sex, ~year,
  FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,
  TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE
)

fselector = fs("design_points", design = design)
fselector$optimize(instance)
```

As above, the best feature subset and the corresponding measured performance can be accessed from the instance.
However, in this simple case, if the fastest subset is not also the best performing subset, the result consists of two subsets: one with the lowest training time and one with the best classification accuracy:

```{r feature-selection-029}
as.data.table(instance$result)[, .(features, classif.acc, time_train)]
```

As explained above, the result is the pareto-optimal solution.

### Automating the Feature Selection {#sec-autofselect}

The `AutoFSelector` class wraps a learner and augments it with an automatic feature selection for a given task.
Because the `AutoFSelector` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Below, a new learner is created.
This learner is then wrapped in a random search feature selector, which automatically starts a feature selection on the given task using an inner resampling, as soon as the wrapped learner is trained.
Here, the function `r ref("auto_fselector")` creates an instance of `AutoFSelector`, i.e. it is short for `AutoFSelector$new()`.

```{r feature-selection-030}
at = auto_fselector(
  fselector = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 10)
)
at
```

We can now, as with any other learner, call the `$train()` and `$predict()` method.
This time however, we pass it to `r ref("benchmark()")` to compare the optimized feature subset to the complete feature set.
This way, the `AutoFSelector` will do its resampling for feature selection on the training set of the respective split of the outer resampling.
The learner then undertakes predictions using the test set of the outer resampling.
Here, the outer resampling refers to the resampling specified in `benchmark()`, whereas the inner resampling is that specified in `auto_fselector()`.
This is called nested resampling (@sec-nested-resampling) and yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.

In the call to `benchmark()`, we compare our wrapped learner `at` with a normal logistic regression `lrn("classif.log_reg")`.
For that, we create a benchmark grid with the task, the learners and a 3-fold cross validation on the `r ref("mlr_tasks_sonar", text = "sonar")` data.

```{r feature-selection-031, warning=FALSE}
grid = benchmark_grid(
  task = tsk("sonar"),
  learner = list(at, lrn("classif.log_reg")),
  resampling = rsmp("cv", folds = 3)
)

bmr = benchmark(grid)
```

Now, we compare those two learners regarding classification accuracy and training time:

```{r feature-selection-032}
aggr = bmr$aggregate(msrs(c("classif.acc", "time_train")))
as.data.table(aggr)[, .(learner_id, classif.acc, time_train)]
```

We can see that, in this example, the feature selection improves prediction performance but also drastically increases the training time, since the feature selection (including resampling and random search) is part of the model training of the wrapped learner.

## Conclusion

In this chapter, we learned how to perform feature selection with mlr3.
We introduced filter and wrapper methods, combined feature selection with pipelines, learned how to automate the feature selection and covered the optimization of multiple performance measures.
@tbl-api-feature-selection gives an overview of the most important functions (S3) and classes (R6) used in this chapter.

| S3 function | R6 Class | Summary |
| --- | --- | --- |
| `r ref("flt()")`   | `r ref("Filter")` | Selects features by calculating a score for each feature |
| `Filter$calculate()`   | `r ref("Filter")` | Calculates scores on a given task |
| `r ref("fselect()")` | `r ref("FSelectInstanceSingleCrit")` or  `r ref("FSelectInstanceMultiCrit")` | Specifies a feature selection problem and stores the results |
| `r ref("fs()")` | `r ref("FSelector")` | Specifies a feature selection algorithm |
| `FSelector$optimize()` | `r ref("FSelector")` | Executes the features selection specified by the `FSelectInstance` with the algorithm specified by the `FSelector` |
| `r ref("auto_fselector()")` | `r ref("AutoFSelector")` | Defines a learner that includes feature selection |

:Core S3 'sugar' functions for feature selection in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-feature-selection}


### Resources{.unnumbered .unlisted}
* A list of implemented filters in the `r mlr3filters` package is provided on the `r link("https://mlr3filters.mlr-org.com", "mlr3filters website")`.
* A summary of wrapper-based feature selection with the `r mlr3fselect` package is provided in the `r link("https://cheatsheets.mlr-org.com/mlr3fselect.pdf", "mlr3fselect cheatsheet")`.
* An overview of feature selection methods is provided by @chandrashekar2014.
* A more formal and detailed introduction to filters and wrappers is given in @guyon2003.
* @bommert2020 perform a benchmark of filter methods.
* Filters can be used as part of a machine learning pipeline (@sec-pipelines).
* Filters can be optimized with hyperparameter optimization (@sec-optimization).

## Exercises

1. Calculate a correlation filter on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
2. Use the filter from the first exercise to select the five best features in the `mtcars` data set.
3. Apply a backward selection to the `r ref("mlr_tasks_penguins", text = "penguins")` data set with a classification tree learner `"classif.rpart"` and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example. Answer the following questions:
    a. Do the selected features differ?
    b. Which feature selection method achieves a higher classification accuracy?
    c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?
4. Automate the feature selection as in @sec-autofselect with the `r ref("mlr_tasks_spam", text = "spam")` data set and a logistic regression learner (`"classif.log_reg"`). Hint: Remember to call `library("mlr3learners")` for the logistic regression learner.
