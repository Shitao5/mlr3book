<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 9&nbsp; Preprocessing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" rel="next">
<link href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter7/sequential_pipelines.html">Pipelines and Preprocessing</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter9/preprocessing.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#data-cleaning" id="toc-data-cleaning" class="nav-link active" data-scroll-target="#data-cleaning"><span class="header-section-number">9.1</span> Data Cleaning</a></li>
  <li><a href="#factor-encoding" id="toc-factor-encoding" class="nav-link" data-scroll-target="#factor-encoding"><span class="header-section-number">9.2</span> Factor Encoding</a></li>
  <li><a href="#sec-preprocessing-missing" id="toc-sec-preprocessing-missing" class="nav-link" data-scroll-target="#sec-preprocessing-missing"><span class="header-section-number">9.3</span> Missing Values</a></li>
  <li><a href="#sec-prepro-robustify" id="toc-sec-prepro-robustify" class="nav-link" data-scroll-target="#sec-prepro-robustify"><span class="header-section-number">9.4</span> Pipeline Robustify</a></li>
  <li><a href="#sec-prepro-scale" id="toc-sec-prepro-scale" class="nav-link" data-scroll-target="#sec-prepro-scale"><span class="header-section-number">9.5</span> Transforming Features and Targets</a></li>
  <li><a href="#functional-feature-extraction" id="toc-functional-feature-extraction" class="nav-link" data-scroll-target="#functional-feature-extraction"><span class="header-section-number">9.6</span> Functional Feature Extraction</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9.7</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">9.8</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">9.9</span> Citation</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter9/preprocessing.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter9/preprocessing.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-preprocessing" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p><strong>Janek Thomas</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH</em> <br><br></p>
<p><a href="../chapter7/sequential_pipelines.html"><span>Chapter&nbsp;7</span></a> and <a href="../chapter8/non-sequential_pipelines_and_tuning.html"><span>Chapter&nbsp;8</span></a> provided a technical introduction to <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a>, this chapter will now demonstrate how to use those pipelines to tackle common problems when preprocessing data for ML, including factor encoding, imputation of missing values, feature and target transformations, and functional feature extraction. Feature selection, an important preprocessing method, is covered in <a href="../chapter6/feature_selection.html"><span>Chapter&nbsp;6</span></a>.</p>
<p>In this book, preprocessing refers to everything that happens with <em>data</em> before it is used to fit a model, while postprocessing encompasses everything that occurs with <em>predictions</em> after the model is fitted.</p>
<div class="page-columns page-full"><p>Data cleaning is an important part of preprocessing that involves the removal of errors, noise, and redundancy in the data; we only consider data cleaning very briefly as it is usually performed outside of <code>mlr3</code> on the raw dataset.</p><div class="no-row-height column-margin column-container"><span class="">Data Cleaning</span></div></div>
<div class="page-columns page-full"><p>Another aspect of preprocessing is feature engineering, which covers all other transformations of data before it is fed to the machine learning model, including the creation of features from possibly unstructured data, such as written text, sequences or images. The goal of feature engineering is to enable the data to be handled by a given learner, and/or to further improve predictive performance. It is important to note that feature engineering helps mostly for simpler algorithms, while highly complex models usually gain less from it and require little data preparation to be trained. Common difficulties in data that can be solved with feature engineering include features with skewed distributions, high cardinality categorical features, missing observations, high dimensionality and imbalanced classes in classification tasks. Deep learning has shown promising results in automating feature engineering, however, its effectiveness depends on the complexity and nature of the data being processed, as well as the specific problem being addressed. Typically it can work well with natural language processing and computer vision problems, while for standard tabular data, tree-based ensembles such as a random forest or gradient boosting are often still superior (and easier to handle). However, tabular deep learning approaches are currently catching up quickly. Hence, manual feature engineering is still often required but with <code>mlr3pipelines</code>, which can simplify the process as much as possible.</p><div class="no-row-height column-margin column-container"><span class="">Feature Engineering</span></div></div>
<p>As we work through this chapter we will use an adapted version of the Ames housing data <span class="citation" data-cites="de2011ames">(<a href="../references.html#ref-de2011ames" role="doc-biblioref">De Cock 2011</a>)</span>. We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset. The modified version is shipped with <a href="https://mlr3data.mlr-org.com"><code>mlr3data</code></a> and the code to recreate this version of the data from the original raw data can be found at <a href="https://github.com/mlr-org/mlr3data/">https://github.com/mlr-org/mlr3data/</a> in the directory <code>data-raw</code>. This original dataset was collected as an alternative to the Boston Housing data and is commonly used to demonstrate feature engineering in ML. Raw and processed versions of the data can be directly loaded from the <a href="https://cran.r-project.org/package=AmesHousing"><code>AmesHousing</code></a> package. The dataset includes 2,930 residential properties (rows) situated in Ames, Iowa, sold between 2006 and 2010. It contains 81 features about various aspects of the property, the size and shape of the lot, and information about its condition and quality. The prediction target is the sale price in USD, hence it is a regression task.</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-2_d958385aabf3e0f0e6cc3d1fd52b1139">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ames</span> <span class="op">=</span> <span class="fu">mlr3data</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/mlr3data/man/ames_housing.html">ames_housing</a></span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="data-cleaning" class="level2" data-number="9.1"><h2 data-number="9.1" class="anchored" data-anchor-id="data-cleaning">
<span class="header-section-number">9.1</span> Data Cleaning</h2>
<p>As a first step, we explore the data and look for simple problems such as constant or duplicated features. This can be done quite efficiently with a package like <a href="https://cran.r-project.org/package=DataExplorer"><code>DataExplorer</code></a> or <a href="https://cran.r-project.org/package=skimr"><code>skimr</code></a> which can be used to create a large number of informative plots.</p>
<p>Below we summarize the most important findings for data cleaning, but we only consider this aspect in a cursory manner:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-003_fd5e9afa42f0700e5bbf1fb6883f8aab">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 1. `Misc_Feature_2` is a factor with only a single level `Othr`.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ames</span><span class="op">$</span><span class="va">Misc_Feature_2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Othr 
2930 </code></pre>
</div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 2. `Condition_2` and `Condition_3` are identical.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">ames</span><span class="op">$</span><span class="va">Condition_2</span>, <span class="va">ames</span><span class="op">$</span><span class="va">Condition_3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 3. `Lot_Area` and `Lot_Area_m2` are same data on different scales</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">ames</span><span class="op">$</span><span class="va">Lot_Area</span>, <span class="va">ames</span><span class="op">$</span><span class="va">Lot_Area_m2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>For all three problems, simply removing the problematic features (or feature in a pair) might be the best course of action.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-006_e6ce4c2c93dcdab78a8cce38b1e0420c">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">to_remove</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Lot_Area_m2"</span>, <span class="st">"Condition_3"</span>, <span class="st">"Misc_Feature_2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Other typical problems that should be checked are:</p>
<ol type="1">
<li>ID columns, i.e., columns that are unique for every observation should be removed or tagged.</li>
<li>
<code>NA</code>s not correctly encoded, e.g.&nbsp;as <code>"NA"</code> or <code>""</code>
</li>
<li>Semantic errors in the data, e.g., negative <code>Lot_Area</code>
</li>
<li>Numeric features encoded as categorical for learners that can not handle such features.</li>
</ol>
<p>Before we continue with feature engineering we will create a task, measure, and resampling strategy to use throughout the chapter.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-007_0f2cf3a1f054edd5db2aeb7eaf11ded3">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_ames</span> <span class="op">=</span> <span class="fu">as_task_regr</span><span class="op">(</span><span class="va">ames</span>, target <span class="op">=</span> <span class="st">"Sale_Price"</span>, id <span class="op">=</span> <span class="st">"ames"</span><span class="op">)</span></span>
<span><span class="co"># remove problematic features</span></span>
<span><span class="va">tsk_ames</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sets.html">setdiff</a></span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">$</span><span class="va">feature_names</span>, <span class="va">to_remove</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">msr_mae</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"regr.mae"</span><span class="op">)</span></span>
<span><span class="va">rsmp_cv3</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">rsmp_cv3</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lastly, we run a very simple experiment to verify our setup works as expected with a simple featureless baseline, note below we set <code>robust = TRUE</code> to always predict the <em>median</em> sale price as opposed to the <em>mean</em>.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-008_72010a163621d8e4bfa9eea9fa04e538">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_baseline</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.featureless"</span>, robust <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">lrn_baseline</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"Baseline"</span></span>
<span><span class="va">rr_baseline</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">tsk_ames</span>, <span class="va">lrn_baseline</span>, <span class="va">rsmp_cv3</span><span class="op">)</span></span>
<span><span class="va">rr_baseline</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="va">msr_mae</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>regr.mae 
   56056 </code></pre>
</div>
</div>
</section><section id="factor-encoding" class="level2 page-columns page-full" data-number="9.2"><h2 data-number="9.2" class="anchored" data-anchor-id="factor-encoding">
<span class="header-section-number">9.2</span> Factor Encoding</h2>
<p>Many machine learning algorithm implementations, such as XGBoost <span class="citation" data-cites="chen2016xgboost">(<a href="../references.html#ref-chen2016xgboost" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>, cannot handle categorical data and so categorical features must be encoded into numerical variables.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-010_3cdf8f025cc57eb6a6b0e6e6baab9bee">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.xgboost"</span>, nrounds <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error: &lt;TaskRegr:ames&gt; has the following unsupported feature types: factor</code></pre>
</div>
</div>
<p>Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. For now, we will consider high-cardinality to be features with more than 10 levels:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-3_2705160a3ab36037c43fa125d86a2a57">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lengths.html">lengths</a></span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">$</span><span class="fu">levels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Exterior_1st" "Exterior_2nd" "MS_SubClass"  "Neighborhood"</code></pre>
</div>
</div>
<p>Binary features can be trivially encoded by setting one of the feature levels to <code>1</code> and the other to <code>0</code>.</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-4_b920d6a81cae0fc9ac100dd647b47d44">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lengths.html">lengths</a></span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">$</span><span class="fu">levels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Alley"       "Central_Air" "Street"     </code></pre>
</div>
</div>
<div class="page-columns page-full"><p>Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM).</p><div class="no-row-height column-margin column-container"><span class="">One-hot Encoding</span></div></div>
<div class="page-columns page-full"><p>Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding <span class="citation" data-cites="MicciBarreca2001">(<a href="../references.html#ref-MicciBarreca2001" role="doc-biblioref">Micci-Barreca 2001</a>)</span> is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:</p><div class="no-row-height column-margin column-container"><span class="">Impact Encoding</span></div></div>
<ol type="1">
<li>Group the target variable by the categorical feature.</li>
<li>Compute the mean of the target variable for each group.</li>
<li>Compute the global mean of the target variable.</li>
<li>Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.</li>
<li>Replace the categorical feature with the impact scores.</li>
</ol>
<p>Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data (<a href="../chapter3/evaluation_and_benchmarking.html"><span>Chapter&nbsp;3</span></a>).</p>
<p>As well as encoding features, other basic preprocessing steps for categorical features include removing constant features (which only have one level and may have been removed as part of data cleaning), and collapsing levels that occur very rarely. These types of problems can occur as artifacts of resampling as the dataset size is further reduced. Stratification on such features would be an alternative way to mitigate this (<a href="../chapter3/evaluation_and_benchmarking.html#sec-strat-group"><span>Section&nbsp;3.2.5</span></a>).</p>
<p>In the code below we use <code>po("removeconstants")</code> to remove features with only one level, <code>po("collapsefactors")</code> to collapse levels that occur less than 1% of the time in the data, <code>po("encodeimpact")</code> to impact-encode high-cardinality features, <code>po("encode", method = "one-hot")</code> to one-hot encode low-cardinality features, and finally <code>po("encode", method = "treatment")</code> to treatment encode binary features.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-011_0dd7428430b4887afdb75d716ee945a7">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">factor_pipeline</span> <span class="op">=</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"removeconstants"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"collapsefactors"</span>, no_collapse_above_prevalence <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"encodeimpact"</span>,</span>
<span>        affect_columns <span class="op">=</span> <span class="fu">selector_cardinality_greater_than</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>        id <span class="op">=</span> <span class="st">"high_card_enc"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"encode"</span>, method <span class="op">=</span> <span class="st">"one-hot"</span>,</span>
<span>        affect_columns <span class="op">=</span> <span class="fu">selector_cardinality_greater_than</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>        id <span class="op">=</span> <span class="st">"low_card_enc"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"encode"</span>, method <span class="op">=</span> <span class="st">"treatment"</span>,</span>
<span>        affect_columns <span class="op">=</span> <span class="fu">selector_type</span><span class="op">(</span><span class="st">"factor"</span><span class="op">)</span>, id <span class="op">=</span> <span class="st">"binary_enc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can apply this pipeline to our xgboost model to use it in a benchmark experiment; we also compare a simpler pipeline that only uses one-hot encoding to demonstrate performance differences resulting from different strategies.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-013_2d0d496216f193869524cd2ce6b73d6a">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn_xgb_impact</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="va">factor_pipeline</span> <span class="op">%&gt;&gt;%</span> <span class="va">lrn_xgb</span><span class="op">)</span></span>
<span><span class="va">glrn_xgb_impact</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"XGB_enc_impact"</span></span>
<span></span>
<span><span class="va">glrn_xgb_one_hot</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">po</span><span class="op">(</span><span class="st">"encode"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="va">lrn_xgb</span><span class="op">)</span></span>
<span><span class="va">glrn_xgb_one_hot</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"XGB_enc_onehot"</span></span>
<span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">tsk_ames</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">lrn_baseline</span>, <span class="va">glrn_xgb_impact</span>, <span class="va">glrn_xgb_one_hot</span><span class="op">)</span>, <span class="va">rsmp_cv3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>measure <span class="op">=</span> <span class="va">msr_mae</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">regr.mae</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098</code></pre>
</div>
</div>
<p>In this small experiment, we see that the difference between the extended factor encoding pipeline and the simpler one-hot encoding strategy pipeline is only very small. If you are interested in learning more about different encoding strategies, including a benchmark study comparing them, we recommend <span class="citation" data-cites="pargent2022regularized">Pargent et al. (<a href="../references.html#ref-pargent2022regularized" role="doc-biblioref">2022</a>)</span>.</p>
</section><section id="sec-preprocessing-missing" class="level2 page-columns page-full" data-number="9.3"><h2 data-number="9.3" class="anchored" data-anchor-id="sec-preprocessing-missing">
<span class="header-section-number">9.3</span> Missing Values</h2>
<p>A common problem in real-world data is missing values in features. In the Ames dataset, several variables have at least one missing data point:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-5_cf199ee3411f5695f43cd8b1d6af7588">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># print first five with missing data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">tsk_ames</span><span class="op">$</span><span class="fu">missings</span><span class="op">(</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Alley"          "BsmtFin_SF_1"   "BsmtFin_SF_2"   "BsmtFin_Type_1"
[5] "BsmtFin_Type_2"</code></pre>
</div>
</div>
<p>Many learners cannot handle missing values automatically (e.g., <code>lrn("regr.ranger")</code> and <code>lrn("regr.lm")</code>) and others may be able to handle missing values but may use simple methods that are not ideal (e.g., just omitting rows with missing data).</p>
<div class="page-columns page-full"><p>The simplest data imputation method is to replace missing values by the feature’s mean (<code>po("imputemean")</code>) (<a href="#fig-imputation">Figure&nbsp;<span>9.1</span></a>), median (<code>po("imputemedian")</code>), or mode (<code>po("imputemode")</code>). Alternatively, one can impute by sampling from the empirical distribution of the feature, for example a histogram (<code>po("imputehist")</code>). Instead of guessing at what a missing feature might be, missing values could instead be replaced by a new level, for example, called <code>.MISSING</code> (<code>po("imputeoor")</code>). For numeric features, <span class="citation" data-cites="ding2010investigation">Ding and Simonoff (<a href="../references.html#ref-ding2010investigation" role="doc-biblioref">2010</a>)</span> show that for binary classification and tree-based models, encoding missing values out-of-range (OOR), e.g.&nbsp;a constant value above the largest observed value, is a reasonable approach.</p><div class="no-row-height column-margin column-container"><span class="">Data Imputation</span></div></div>
<div class="cell" data-hash="preprocessing_cache/html/fig-imputation_9ea2bd07458f0dd7d97d1e71d1d8f62f">
<div class="cell-output-display">
<div id="fig-imputation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-13.svg" class="img-fluid figure-img" style="width:60.0%" alt="On the left is a vector of numbers in a column, (1.3, NA, 1.5, NA). The non-NA numbers have arrows pointing to (1.3+1.5)/2, which then has an arrow pointing to a vector of numbers in a column on the right but now (1.3, 1.4, 1.5, 1.4) with '1.4' in red to highlight they were imputed with the mean."></p>
<figcaption class="figure-caption">Figure&nbsp;9.1: Mean imputation of missing values using observed values.</figcaption></figure>
</div>
</div>
</div>
<p>It is often important for predictive tasks that you keep track of missing data as it is common for missing data to be informative in itself. To preserve the information about which data was missing, imputation should be tracked by adding binary indicator features (one for each imputed feature) that are <code>1</code> if the feature was missing for an observation and <code>0</code> if it was present (<code>po("missind")</code>). It is important to note that recording this information will not prevent problems in model interpretation on its own. As a real-world example, medical data are typically collected more extensively for White communities than for racially minoritized communities. Imputing data from minoritized communities would at best mask this data bias, and at worst would make the data bias even worse by making vastly inaccurate assumptions (see <a href="../chapter14/algorithmic_fairness.html"><span>Chapter&nbsp;14</span></a> for data bias and algorithmic fairness).</p>
<p>In the code below we create a pipeline from the <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html"><code>PipeOp</code></a>s listed above as well as making use of <code>po("featureunion")</code> to combine multiple <code>PipeOp</code>s acting on the <code>"integer"</code> columns.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-014-evalF_a3ee06a125d4ad16074bb4996485cf92">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">impute_hist</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>      <span class="fu">po</span><span class="op">(</span><span class="st">"missind"</span>, type <span class="op">=</span> <span class="st">"integer"</span>,</span>
<span>          affect_columns <span class="op">=</span> <span class="fu">selector_type</span><span class="op">(</span><span class="st">"integer"</span><span class="op">)</span></span>
<span>      <span class="op">)</span>,</span>
<span>      <span class="fu">po</span><span class="op">(</span><span class="st">"imputehist"</span>, affect_columns <span class="op">=</span> <span class="fu">selector_type</span><span class="op">(</span><span class="st">"integer"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"featureunion"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>    <span class="fu">po</span><span class="op">(</span><span class="st">"imputeoor"</span>, affect_columns <span class="op">=</span> <span class="fu">selector_type</span><span class="op">(</span><span class="st">"factor"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">impute_hist</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span>horizontal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="preprocessing_cache/html/fig-impute_5accb49acd1a24602979de9a7b4286aa">
<div class="cell-output-display">
<div id="fig-impute" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="preprocessing_files/figure-html/fig-impute-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Flow diagram shows '<INPUT>' with arrows to 'missind' and 'imputehist', which both have arrows to 'featureunion', which has an arrow to 'imputeoor' that has an arrow to '<OUTPUT'>."></p>
<figcaption class="figure-caption">Figure&nbsp;9.2: Pipeline to impute missing values of numeric features by histogram with binary indicators and missings in categoricals out-of-range with a new level.</figcaption></figure>
</div>
</div>
</div>
<p>Using this pipeline we can now run experiments with <code>lrn("regr.ranger")</code>, which cannot handle missing data; we also compare a simpler pipeline that only uses OOR imputation to demonstrate performance differences resulting from different strategies.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-016_c5240aab9094ed5f93b7d6bed4d82a1b">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn_rf_impute_hist</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="va">impute_hist</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">glrn_rf_impute_hist</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"RF_imp_Hist"</span></span>
<span></span>
<span><span class="va">glrn_rf_impute_oor</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">po</span><span class="op">(</span><span class="st">"imputeoor"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">glrn_rf_impute_oor</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"RF_imp_OOR"</span></span>
<span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">tsk_ames</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">glrn_rf_impute_hist</span>, <span class="va">glrn_rf_impute_oor</span><span class="op">)</span>, <span class="va">rsmp_cv3</span><span class="op">)</span></span>
<span><span class="va">bmr_new</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">combine</span><span class="op">(</span><span class="va">bmr_new</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>measure <span class="op">=</span> <span class="va">msr_mae</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">regr.mae</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098
4:    RF_imp_Hist    16400
5:     RF_imp_OOR    16395</code></pre>
</div>
</div>
<p>Similarly to encoding, we see limited differences in performance between the different imputation strategies. This is expected here and confirms the findings of <span class="citation" data-cites="ding2010investigation">Ding and Simonoff (<a href="../references.html#ref-ding2010investigation" role="doc-biblioref">2010</a>)</span> – out-of-range imputation is a simple yet effective imputation for tree-based methods.</p>
<p>Many more advanced imputation strategies exist, including model-based imputation where machine learning models are used to predict missing values, and multiple imputation where data is repeatedly resampled and imputed in each sample (e.g., by mean imputation) to attain more robust estimates. However, these more advanced techniques rarely improve the models predictive performance substantially and the simple imputation techniques introduced above are usually sufficient <span class="citation" data-cites="Poulos2018">(<a href="../references.html#ref-Poulos2018" role="doc-biblioref">Poulos and Valle 2018</a>)</span>. Nevertheless, these methods are still important, as finding imputations that fit well to the distribution of the observed values allows a model to be fitted that can be interpreted and analyzed in a second step.</p>
</section><section id="sec-prepro-robustify" class="level2 page-columns page-full" data-number="9.4"><h2 data-number="9.4" class="anchored" data-anchor-id="sec-prepro-robustify">
<span class="header-section-number">9.4</span> Pipeline Robustify</h2>
<div class="page-columns page-full"><p><code>mlr3pipelines</code> offers a simple and reusable pipeline for (among other things) imputation and factor encoding called <code>ppl("robustify")</code>, which includes sensible defaults that can be used most of the time when encoding or imputing data. The pipeline includes the following <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html"><code>PipeOp</code></a>s (some are applied multiple times and most use selectors):</p><div class="no-row-height column-margin column-container"><span class="">ppl(“robustify”)</span></div></div>
<ol type="1">
<li>
<code>po("removeconstants")</code> – Constant features are removed.</li>
<li>
<code>po("colapply")</code> – Character and ordinal features are encoded as categorical, and date/time features are encoded as numeric.</li>
<li>
<code>po("imputehist")</code> – Numeric features are imputed by histogram sampling.</li>
<li>
<code>po("imputesample")</code> – Logical features are imputed by sampling from the empirical distribution – this only affects the <code>$predict()</code>-step.</li>
<li>
<code>po("missind")</code> – Missing data indicators are added for imputed numeric and logical variables.</li>
<li>
<code>po("imputeoor")</code> – Missing values of categorical features are encoded with a new level.</li>
<li>
<code>po("fixfactors")</code> – Fixes levels of categorical features such that the same levels are present during prediction and training (which may involve dropping empty factor levels).</li>
<li>
<code>po("imputesample")</code> – Missing values in categorical features introduced from dropping levels in the previous step are imputed by sampling from the empirical distributions.</li>
<li>
<code>po("collapsefactors")</code> – Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are less than a certan number of levels, controlled by the <code>max_cardinality</code> argument (with a conservative default of <code>1000</code>).</li>
<li>
<code>po("encode")</code> – Categorical features are one-hot encoded.</li>
<li>
<code>po("removeconstants")</code> – Constant features that might have been created in the previous steps are removed.</li>
</ol>
<p><code>ppl("robustify")</code> has optional arguments <code>task</code> and <code>learner</code>. If these are provided, then the resulting pipeline will be set up to handle the given task and learner specifically, for example, it will not impute missing values if the learner has the <code>"missings"</code> property, or if there are no missing values in the task to begin with. By default, when <code>task</code> and <code>learner</code> are not provided, the graph is set up to be defensive: it imputes all missing values and converts all feature types to numerics.</p>
<p>Linear regression is a simple model that cannot handle most problems that we may face when processing data, but with the <code>ppl("robustify")</code> we can now include it in our experiment:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-019_4b07204de8089701799fdc0ef5cb8c54">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn_lm_robust</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">ppl</span><span class="op">(</span><span class="st">"robustify"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.lm"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">glrn_lm_robust</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"lm_robust"</span></span>
<span></span>
<span><span class="va">bmr_new</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">tsk_ames</span>, <span class="va">glrn_lm_robust</span>,  <span class="va">rsmp_cv3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">combine</span><span class="op">(</span><span class="va">bmr_new</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>measure <span class="op">=</span> <span class="va">msr_mae</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">regr.mae</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098
4:    RF_imp_Hist    16400
5:     RF_imp_OOR    16395
6:      lm_robust    16298</code></pre>
</div>
</div>
<p>Robustifying the linear regression results in a model that vastly outperforms the featureless baseline and is competitive when compared to more complex machine learning models.</p>
</section><section id="sec-prepro-scale" class="level2" data-number="9.5"><h2 data-number="9.5" class="anchored" data-anchor-id="sec-prepro-scale">
<span class="header-section-number">9.5</span> Transforming Features and Targets</h2>
<p>Simple transformations of features and the target can be beneficial (and sometimes essential) for certain learners. In particular, log transformation of the target can help in making the distribution more symmetrical and can help reduce the impact of outliers. Similarly, log transformation of skewed features can help to reduce the influence of outliers. In <a href="#fig-sale">Figure&nbsp;<span>9.3</span></a> we plot the distribution of the target in the <code>ames</code> dataset and then the log-transformed target, we can see how simply taking the log of the variable results in a distribution that is much more symmetrical and with fewer outliers.</p>
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># copy ames data</span></span>
<span><span class="va">log_ames</span> <span class="op">=</span> <span class="fu">copy</span><span class="op">(</span><span class="va">ames</span><span class="op">)</span></span>
<span><span class="co"># log transform target</span></span>
<span><span class="va">log_ames</span><span class="op">[</span>, <span class="va">logSalePrice</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Sale_Price</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co"># plot</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="fu">as_task_regr</span><span class="op">(</span><span class="va">log_ames</span>, target <span class="op">=</span> <span class="st">"Sale_Price"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">autoplot</span><span class="op">(</span><span class="fu">as_task_regr</span><span class="op">(</span><span class="va">log_ames</span>, target <span class="op">=</span> <span class="st">"logSalePrice"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="preprocessing_cache/html/fig-sale_cf86dcde0e4f4324c4c84a4174e8ad59">
<div class="cell-output-display">
<div id="fig-sale" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="preprocessing_files/figure-html/fig-sale-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Two boxplots. Left plot shows house prices up to $600,000, the majority of prices are between roughly $100,000-$200,000. Right plot shows log house prices primarily around 12 with an even range between 11 and 13 and a few outliers on both sides."></p>
<figcaption class="figure-caption">Figure&nbsp;9.3: Distribution of house sales prices (in USD) in the ames dataset before (left) and after (right) log transformation. Before transformation there is a skewed distribution of prices towards cheaper properties with a few outliers of very expensive properties. After transformation the distribution is much more symmetrical with the majority of points evenly spread around the same range.</figcaption></figure>
</div>
</div>
</div>
<p>Normalization of features may also be necessary to ensure features with a larger scale do not have a higher impact, which is especially important for distance-based methods such as k-nearest neighbors models or regularized parametric models such as Lasso or Elastic net. Many models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing, though if this is required then <code>po("scale")</code> can be used to center and scale numeric features.</p>
<p>Any transformations applied to the target during training must be inverted during model prediction to ensure predictions are made on the correct scale. By example, say we are interested in log transforming the target, then we would take the following steps:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-8_e3959eab49afcfe6bc85cf6355879400">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">df</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">df</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         x     y
1: 0.48004 10.25
2: 0.14466 10.75
3: 0.05795 18.30
4: 0.65004 17.34
5: 0.37355 10.48</code></pre>
</div>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 1. log transform the target</span></span>
<span><span class="va">df</span><span class="op">[</span>, <span class="va">y</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">df</span><span class="op">$</span><span class="va">y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.327 2.375 2.907 2.853 2.350</code></pre>
</div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 2. make linear regression predictions</span></span>
<span><span class="co">#    predictions on the log-transformed scale</span></span>
<span><span class="va">yhat</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">df</span><span class="op">)</span>, <span class="va">df</span><span class="op">)</span></span>
<span><span class="va">yhat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    1     2     3     4     5 
2.556 2.571 2.575 2.548 2.561 </code></pre>
</div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 3. transform to correct scale with inverse of log function</span></span>
<span><span class="co">#    predictions on the original scale</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">yhat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    1     2     3     4     5 
12.88 13.08 13.13 12.79 12.95 </code></pre>
</div>
</div>
<p>In this simple experiment, we could manually transform and invert the target, however, this is much more complex when dealing with resampling and benchmarking experiments and so the pipeline <code>ppl("targettrafo")</code> will do this heavy lifting for you. The pipeline includes a parameter <code>targetmutate.trafo</code> for the transformation to be applied during training to the target, as well as <code>targetmutate.inverter</code> for the transformation to be applied to invert the original transformation during prediction. So now let us consider the log transformation by adding this pipeline to our robust linear regression model:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-020_77223d74679abbbbfebedeae8976aa19">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn_log_lm_robust</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">ppl</span><span class="op">(</span><span class="st">"targettrafo"</span>,</span>
<span>  graph <span class="op">=</span> <span class="va">glrn_lm_robust</span>,</span>
<span>  targetmutate.trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,</span>
<span>  targetmutate.inverter <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>response <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">response</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">glrn_log_lm_robust</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"lm_robust_logtrafo"</span></span>
<span></span>
<span><span class="va">bmr_new</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">tsk_ames</span>, <span class="va">glrn_log_lm_robust</span>,</span>
<span>  <span class="va">rsmp_cv3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">combine</span><span class="op">(</span><span class="va">bmr_new</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>measure <span class="op">=</span> <span class="va">msr_mae</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">regr.mae</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           learner_id regr.mae
1:           Baseline    56056
2:     XGB_enc_impact    16068
3:     XGB_enc_onehot    16098
4:        RF_imp_Hist    16400
5:         RF_imp_OOR    16395
6:          lm_robust    16298
7: lm_robust_logtrafo    15557</code></pre>
</div>
</div>
<p>With the target transformation and the <code>ppl("robustify")</code>, the simple linear regression now appears to be the best-performing model.</p>
</section><section id="functional-feature-extraction" class="level2" data-number="9.6"><h2 data-number="9.6" class="anchored" data-anchor-id="functional-feature-extraction">
<span class="header-section-number">9.6</span> Functional Feature Extraction</h2>
<p>As a final step of data preprocessing, we will look at feature extraction from functional features. In <a href="../chapter6/feature_selection.html"><span>Chapter&nbsp;6</span></a> we look at automated feature selection and how automated approaches with filters and wrappers can be used to reduce a dataset to an optimized set of features. Functional feature extraction differs from this process as we are now interested in features that are dependent on one another and together may provide useful information but not individually. <a href="#fig-functional-features">Figure&nbsp;<span>9.4</span></a> visualizes the difference between regular and functional features.</p>
<div class="cell" data-hash="preprocessing_cache/html/fig-functional-features_4f891dec1b6a78a580debafcb5ab8fd6">
<div class="cell-output-display">
<div id="fig-functional-features" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-14.svg" class="img-fluid figure-img" style="width:100.0%" alt="On the left is a table with columns 'x1,x2,x3,xt1,xt2,...,xt365'. Below the first three columns is the label 'Regular Features', below the others is the label 'Functional Features, e.g. days in year'. The table has a bidirectional arrow to a line graph that indicates plotting of one row of functional features."></p>
<figcaption class="figure-caption">Figure&nbsp;9.4: Variables x1,x2,x3 are regular features, variables xt1,…,xt365 are functional features that could be plotted to identify important properties.</figcaption></figure>
</div>
</div>
</div>
<p>As a concrete example, consider the power consumption of kitchen appliances in houses in the Ames dataset.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-023_2e285e73b34153d36da649586e8333eb">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">energy_data</span> <span class="op">=</span> <span class="fu">mlr3data</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/mlr3data/man/energy_usage.html">energy_usage</a></span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this dataset, each row represents one house and each feature is the total power consumption from kitchen appliances at a given time <span class="citation" data-cites="bagnall2017great">(<a href="../references.html#ref-bagnall2017great" role="doc-biblioref">Bagnall et al. 2017</a>)</span>. The consumption is measured in two-minute intervals, resulting in 720 features.</p>
<div class="cell" data-hash="preprocessing_cache/html/fig-energy_37f4b38812322ad1f164bf1dc1bcd42d">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">energy_data</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">y</span>, x <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">720</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"2-Minute Interval"</span>, y <span class="op">=</span> <span class="st">"Power Consumption"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-energy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="preprocessing_files/figure-html/fig-energy-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Line plot with '2-Minute Interval' on axis ranging from 1 to 720 and 'Power Consumption' on y-axis ranging from 0 to 20. There are spikes at around (200, 20), (300, 20), and then some consistently raised usage between (500-700, 3)."></p>
<figcaption class="figure-caption">Figure&nbsp;9.5: Energy consumption of one example house in a day, recorded in two-minute intervals.</figcaption></figure>
</div>
</div>
</div>
<p>Adding these 720 features to our full dataset is a bad idea as each individual feature does not provide meaningful information, similarly, we cannot automate selection of the best feature subset for the same reason. Instead, we can <em>extract</em> information about the curves to gain insights into the kitchen’s overall energy usage. For example, we could extract the maximum used wattage, overall used wattage, number of peaks, and other similar features.</p>
<p>To extract features we will write our own <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html"><code>PipeOp</code></a> that inherits from <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html"><code>PipeOpTaskPreprocSimple</code></a>. To do this we add a private method called <code>.transform_dt</code> that hardcodes the operations in our task. In this example, we select the functional features (which all start with “att”), extract the mean, minimum, maximum, and variance of the power consumption, and then remove the functional features. To read more about building custom <code>PipeOp</code>s, open the corresponding vignette by running <code><a href="https://mlr3pipelines.mlr-org.com/articles/extending.html">vignette("extending", package = "mlr3pipelines")</a></code> in R.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-025_64c05a90b24c05c55ce3f96392896b07">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">PipeOpFuncExtract</span> <span class="op">=</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html">R6Class</a></span><span class="op">(</span><span class="st">"PipeOpFuncExtract"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="fu">mlr3pipelines</span><span class="fu">::</span><span class="va"><a href="https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html">PipeOpTaskPreprocSimple</a></span>,</span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    .transform_dt <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">dt</span>, <span class="va">levels</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">ffeat_names</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"att"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">720</span><span class="op">)</span></span>
<span>        <span class="va">ffeats</span> <span class="op">=</span> <span class="va">dt</span><span class="op">[</span>, <span class="va">..ffeat_names</span><span class="op">]</span></span>
<span>        <span class="va">dt</span><span class="op">[</span>, <span class="va">energy_means</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">ffeats</span>, <span class="fl">1</span>, <span class="va">mean</span><span class="op">)</span><span class="op">]</span></span>
<span>        <span class="va">dt</span><span class="op">[</span>, <span class="va">energy_mins</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">ffeats</span>, <span class="fl">1</span>, <span class="va">min</span><span class="op">)</span><span class="op">]</span></span>
<span>        <span class="va">dt</span><span class="op">[</span>, <span class="va">energy_maxs</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">ffeats</span>, <span class="fl">1</span>, <span class="va">max</span><span class="op">)</span><span class="op">]</span></span>
<span>        <span class="va">dt</span><span class="op">[</span>, <span class="va">energy_vars</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">ffeats</span>, <span class="fl">1</span>, <span class="va">var</span><span class="op">)</span><span class="op">]</span></span>
<span>        <span class="va">dt</span><span class="op">[</span>, <span class="op">(</span><span class="va">ffeat_names</span><span class="op">)</span> <span class="op">:=</span> <span class="cn">NULL</span><span class="op">]</span></span>
<span>        <span class="va">dt</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before using this in an experiment we first test that the <code>PipeOp</code> works as expected.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-026_99fdd4a64dc5128fed37cb98c1817a2a">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_ames_ext</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ames</span>, <span class="va">energy_data</span><span class="op">)</span></span>
<span><span class="va">tsk_ames_ext</span> <span class="op">=</span> <span class="fu">as_task_regr</span><span class="op">(</span><span class="va">tsk_ames_ext</span>, <span class="st">"Sale_Price"</span>, <span class="st">"ames_ext"</span><span class="op">)</span></span>
<span><span class="co"># remove the redundant variables identified at the start of this chapter</span></span>
<span><span class="va">tsk_ames_ext</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sets.html">setdiff</a></span><span class="op">(</span><span class="va">tsk_ames_ext</span><span class="op">$</span><span class="va">feature_names</span>, <span class="va">to_remove</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">func_extractor</span> <span class="op">=</span> <span class="va">PipeOpFuncExtract</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="st">"energy_extract"</span><span class="op">)</span></span>
<span><span class="va">tsk_ames_ext</span> <span class="op">=</span> <span class="va">func_extractor</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">tsk_ames_ext</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">tsk_ames_ext</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span><span class="fl">1</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"energy_means"</span>, <span class="st">"energy_mins"</span>, <span class="st">"energy_maxs"</span>, <span class="st">"energy_vars"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   energy_means energy_mins energy_maxs energy_vars
1:        1.062     0.01427       21.98       3.708</code></pre>
</div>
</div>
<p>These outputs look sensible compared to <a href="#fig-energy">Figure&nbsp;<span>9.5</span></a> so we can now run our final benchmark experiment using feature extraction. We do not need to add the <code>PipeOp</code> to each learner as we can apply it once (as above) before any model training by applying it to all available data.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-027_09dfde2403f6f010a7922293d3a37d03">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learners</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">lrn_baseline</span>, <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.rpart"</span><span class="op">)</span>, <span class="va">glrn_xgb_impact</span>,</span>
<span>    <span class="va">glrn_rf_impute_oor</span>, <span class="va">glrn_lm_robust</span>, <span class="va">glrn_log_lm_robust</span><span class="op">)</span></span>
<span></span>
<span><span class="va">bmr_final</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="fu">benchmark_grid</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">tsk_ames_ext</span>, <span class="va">tsk_ames</span><span class="op">)</span>, <span class="va">learners</span>,</span>
<span>  <span class="va">rsmp_cv3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">perf</span> <span class="op">=</span> <span class="va">bmr_final</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>measure <span class="op">=</span> <span class="va">msr_mae</span><span class="op">)</span></span>
<span><span class="va">perf</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">task_id</span><span class="op">)</span>, <span class="fu">.</span><span class="op">(</span><span class="va">task_id</span>, <span class="va">learner_id</span>, <span class="va">regr.mae</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     task_id         learner_id regr.mae
 1:     ames           Baseline    56056
 2: ames_ext           Baseline    56056
 3:     ames         RF_imp_OOR    16354
 4: ames_ext         RF_imp_OOR    14320
 5:     ames     XGB_enc_impact    16068
 6: ames_ext     XGB_enc_impact    14400
 7:     ames          lm_robust    16291
 8: ames_ext          lm_robust    15093
 9:     ames lm_robust_logtrafo    15555
10: ames_ext lm_robust_logtrafo    13905
11:     ames         regr.rpart    27371
12: ames_ext         regr.rpart    27111</code></pre>
</div>
</div>
<p>The final results indicate that adding these extracted features improved the performance of all models (except the featureless baseline).</p>
<p>In this example, we could have just applied the transformations to the dataset directly and not used a <code>PipeOp</code>. However, the advantage of using the <code>PipeOp</code> is that we could have chained it to a subset of learners to prevent a blow-up of experiments in the benchmark experiment.</p>
</section><section id="conclusion" class="level2" data-number="9.7"><h2 data-number="9.7" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">9.7</span> Conclusion</h2>
<p>In this chapter, we built on everything learned in <a href="../chapter7/sequential_pipelines.html"><span>Chapter&nbsp;7</span></a> and <a href="../chapter8/non-sequential_pipelines_and_tuning.html"><span>Chapter&nbsp;8</span></a> to look at concrete usage of pipelines for data preprocessing. We focused primarily on feature engineering, which can make use of <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a> to automate preprocessing as much as possible while still ensuring user control. We looked at factor encoding for categorical variables, imputing missing data, transforming variables, and feature extraction. Preprocessing is almost always required in machine learning experiments, and applying the <code>ppl("robustify")</code> will help in many cases to simplify this process by applying the most common preprocessing steps, we will see this in use in <a href="../chapter11/large-scale_benchmarking.html"><span>Chapter&nbsp;11</span></a>.</p>
<p>We have not introduced any new classes in this chapter, so instead <a href="#tbl-prepro-api">Table&nbsp;<span>9.1</span></a> lists the <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html"><code>PipeOp</code></a>s and <a href="https://mlr3pipelines.mlr-org.com/reference/Graph.html"><code>Graph</code></a>s we discussed.</p>
<div id="tbl-prepro-api" class="anchored">
<table class="table">
<caption>Table&nbsp;9.1: <code>PipeOp</code>s and <code>Graph</code>s discussed in this chapter.</caption>
<thead><tr class="header">
<th>PipeOp/Graph</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_removeconstants.html"><code>PipeOpRemoveConstants</code></a></td>
<td>Remove variables consisting of one value</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_collapsefactors.html"><code>PipeOpCollapseFactors</code></a></td>
<td>Combine rare factor levels</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encodeimpact.html"><code>PipeOpEncodeImpact</code></a></td>
<td>Impact encoding</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encode.html"><code>PipeOpEncode</code></a></td>
<td>Other factor encoding methods</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html"><code>PipeOpMissInd</code></a></td>
<td>Add an indicator column to track missing data</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html"><code>PipeOpImputeHist</code></a></td>
<td>Impute missing data by sampling from a histogram</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html"><code>PipeOpImputeOOR</code></a></td>
<td>Impute missing data with out-of-range values</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_robustify.html"><code>pipeline_robustify</code></a></td>
<td>Graph with common imputation and encoding methods</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_targettrafo.html"><code>pipeline_targettrafo</code></a></td>
<td>Graph to transform target during training and invert transformation during prediction</td>
</tr>
</tbody>
</table>
</div>
</section><section id="exercises" class="level2" data-number="9.8"><h2 data-number="9.8" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">9.8</span> Exercises</h2>
<p>We will consider a prediction problem similar to the one from this chapter, but using the King County Housing regression data instead (available with <code>tsk("kc_housing")</code>). To evaluate the models, we again use 10-fold CV, mean absolute error and <code>lrn("regr.glmnet")</code>. For now we will ignore the <code>date</code> column and simply remove it:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-9_d47f93850f16727ad1c2785b04abc55b">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/mlr-org/mlr3data">"mlr3data"</a></span><span class="op">)</span></span>
<span><span class="va">kc_housing</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"kc_housing"</span><span class="op">)</span></span>
<span><span class="va">kc_housing</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sets.html">setdiff</a></span><span class="op">(</span><span class="va">kc_housing</span><span class="op">$</span><span class="va">feature_names</span>, <span class="st">"date"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>Have a look at the features, are there any features which might be problematic? If so, change or remove them. Check the dataset and learner properties to understand which preprocessing steps you need to do.</li>
<li>Build a suitable pipeline that allows <code>glmnet</code> to be trained on the dataset. Construct a new <code>glmnet</code> model with <code>ppl("robustify")</code>. Compare the two pipelines in a benchmark experiment.</li>
<li>Now consider the <code>date</code> feature: How can you extract information from this feature in a way that <code>glmnet</code> can use? Does this improve the performance of your pipeline? Finally, consider the spatial nature of the dataset. Can you extract an additional feature from the lat / long coordinates? (Hint: Downtown Seattle has lat/long coordinates <code>47.605</code>/<code>122.334</code>).</li>
</ol></section><section id="citation" class="level2" data-number="9.9"><h2 data-number="9.9" class="anchored" data-anchor-id="citation">
<span class="header-section-number">9.9</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Thomas J. (2024). Preprocessing. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/preprocessing.html.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-bagnall2017great" class="csl-entry" role="listitem">
Bagnall, Anthony, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. 2017. <span>“The Great Time Series Classification Bake Off: A Review and Experimental Evaluation of Recent Algorithmic Advances.”</span> <em>Data Mining and Knowledge Discovery</em> 31: 606–60. <a href="https://doi.org/10.1007/s10618-016-0483-9">https://doi.org/10.1007/s10618-016-0483-9</a>.
</div>
<div id="ref-chen2016xgboost" class="csl-entry" role="listitem">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“<span>XGB</span>oost: A Scalable Tree Boosting System.”</span> In <em>Proceedings of the 22nd <span>ACM SIGKDD</span> International Conference on Knowledge Discovery and Data Mining</em>, 785–94. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>.
</div>
<div id="ref-de2011ames" class="csl-entry" role="listitem">
De Cock, Dean. 2011. <span>“Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.”</span> <em>Journal of Statistics Education</em> 19 (3). <a href="https://doi.org/10.1080/10691898.2011.11889627">https://doi.org/10.1080/10691898.2011.11889627</a>.
</div>
<div id="ref-ding2010investigation" class="csl-entry" role="listitem">
Ding, Yufeng, and Jeffrey S Simonoff. 2010. <span>“An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data.”</span> <em>Journal of Machine Learning Research</em> 11 (6): 131–70. <a href="https://www.jmlr.org/papers/v11/ding10a.html">https://www.jmlr.org/papers/v11/ding10a.html</a>.
</div>
<div id="ref-MicciBarreca2001" class="csl-entry" role="listitem">
Micci-Barreca, Daniele. 2001. <span>“A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.”</span> <em><span>ACM</span> <span>SIGKDD</span> Explorations Newsletter</em> 3 (1): 27–32. <a href="https://doi.org/10.1145/507533.507538">https://doi.org/10.1145/507533.507538</a>.
</div>
<div id="ref-pargent2022regularized" class="csl-entry" role="listitem">
Pargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl. 2022. <span>“Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.”</span> <em>Computational Statistics</em> 37 (5): 2671–92. <a href="https://doi.org/10.1007/s00180-022-01207-6">https://doi.org/10.1007/s00180-022-01207-6</a>.
</div>
<div id="ref-Poulos2018" class="csl-entry" role="listitem">
Poulos, Jason, and Rafael Valle. 2018. <span>“Missing Data Imputation for Supervised Learning.”</span> <em>Applied Artificial Intelligence</em> 32 (2): 186–96. <a href="https://doi.org/10.1080/08839514.2018.1448143">https://doi.org/10.1080/08839514.2018.1448143</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb47" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Preprocessing {#sec-preprocessing}</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Preprocessing"`</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>@sec-pipelines and @sec-pipelines-nonseq provided a technical introduction to <span class="in">`r mlr3pipelines`</span>, this chapter will now demonstrate how to use those pipelines to tackle common problems when <span class="in">`r index('preprocessing')`</span> data for ML, including <span class="in">`r index('factor encoding')`</span>, <span class="in">`r index('imputation')`</span> of missing values, feature and target transformations, and functional <span class="in">`r index('feature extraction')`</span>.</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>Feature selection, an important preprocessing method, is covered in @sec-feature-selection.</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>In this book, preprocessing refers to everything that happens with *data* before it is used to fit a model, while `r index('postprocessing')` encompasses everything that occurs with *predictions* after the model is fitted.</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Data cleaning', aside = TRUE)`</span>\index{exploratory data analysis|see{data cleaning}} is an important part of preprocessing that involves the removal of errors, noise, and redundancy in the data; we only consider data cleaning very briefly as it is usually performed outside of <span class="in">`mlr3`</span> on the raw dataset.</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>Another aspect of preprocessing is <span class="in">`r index('feature engineering', aside = TRUE)`</span>, which covers all other transformations of data before it is fed to the machine learning model, including the creation of features from possibly unstructured data, such as written text, sequences or images.</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>The goal of feature engineering is to enable the data to be handled by a given learner, and/or to further improve predictive performance.</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>It is important to note that feature engineering helps mostly for simpler algorithms, while highly complex models usually gain less from it and require little data preparation to be trained.</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>Common difficulties in data that can be solved with feature engineering include features with skewed distributions, high cardinality categorical features, missing observations, high dimensionality and imbalanced classes in classification tasks.</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>Deep learning has shown promising results in automating feature engineering, however, its effectiveness depends on the complexity and nature of the data being processed, as well as the specific problem being addressed.</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>Typically it can work well with natural language processing and computer vision problems, while for standard tabular data, tree-based ensembles such as a random forest or gradient boosting are often still superior (and easier to handle). However, tabular deep learning approaches are currently catching up quickly.</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>Hence, manual feature engineering is still often required but with <span class="in">`mlr3pipelines`</span>, which can simplify the process as much as possible.</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>As we work through this chapter we will use an adapted version of the Ames housing data <span class="co">[</span><span class="ot">@de2011ames</span><span class="co">]</span>.</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>The modified version is shipped with <span class="in">`r ref_pkg("mlr3data")`</span> and the code to recreate this version of the data from the original raw data can be found at <span class="in">`r link("https://github.com/mlr-org/mlr3data/")`</span> in the directory <span class="in">`data-raw`</span>.</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>This original dataset was collected as an alternative to the Boston Housing data and is commonly used to demonstrate feature engineering in ML.</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>Raw and processed versions of the data can be directly loaded from the <span class="in">`r ref_pkg("AmesHousing")`</span> package.</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>The dataset includes 2,930 residential properties (rows) situated in Ames, Iowa, sold between 2006 and 2010.</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>It contains 81 features about various aspects of the property, the size and shape of the lot, and information about its condition and quality.</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>The prediction target is the sale price in USD, hence it is a regression task.</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=FALSE}</span></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">=</span> mlr3data<span class="sc">::</span>ames_housing</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Cleaning</span></span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a>As a first step, we explore the data and look for simple problems such as constant or duplicated features.</span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>This can be done quite efficiently with a package like <span class="in">`r ref_pkg("DataExplorer")`</span> or <span class="in">`r ref_pkg("skimr")`</span> which can be used to create a large number of informative plots.</span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>Below we summarize the most important findings for data cleaning, but we only consider this aspect in a cursory manner:</span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-003, message=FALSE}</span></span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. `Misc_Feature_2` is a factor with only a single level `Othr`.</span></span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames<span class="sc">$</span>Misc_Feature_2)</span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. `Condition_2` and `Condition_3` are identical.</span></span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(ames<span class="sc">$</span>Condition_2, ames<span class="sc">$</span>Condition_3)</span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. `Lot_Area` and `Lot_Area_m2` are same data on different scales</span></span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(ames<span class="sc">$</span>Lot_Area, ames<span class="sc">$</span>Lot_Area_m2)</span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a>For all three problems, simply removing the problematic features (or feature in a pair) might be the best course of action.</span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-006, message=FALSE}</span></span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a>to_remove <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"Lot_Area_m2"</span>, <span class="st">"Condition_3"</span>, <span class="st">"Misc_Feature_2"</span>)</span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a>Other typical problems that should be checked are:</span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>ID columns, i.e., columns that are unique for every observation should be removed or tagged.</span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`NA`</span>s not correctly encoded, e.g. as <span class="in">`"NA"`</span> or <span class="in">`""`</span></span>
<span id="cb47-62"><a href="#cb47-62" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Semantic errors in the data, e.g., negative <span class="in">`Lot_Area`</span></span>
<span id="cb47-63"><a href="#cb47-63" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Numeric features encoded as categorical for learners that can not handle such features.</span>
<span id="cb47-64"><a href="#cb47-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-65"><a href="#cb47-65" aria-hidden="true" tabindex="-1"></a>Before we continue with feature engineering we will create a task, measure, and resampling strategy to use throughout the chapter.</span>
<span id="cb47-66"><a href="#cb47-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-67"><a href="#cb47-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-007, message=FALSE}</span></span>
<span id="cb47-68"><a href="#cb47-68" aria-hidden="true" tabindex="-1"></a>tsk_ames <span class="ot">=</span> <span class="fu">as_task_regr</span>(ames, <span class="at">target =</span> <span class="st">"Sale_Price"</span>, <span class="at">id =</span> <span class="st">"ames"</span>)</span>
<span id="cb47-69"><a href="#cb47-69" aria-hidden="true" tabindex="-1"></a><span class="co"># remove problematic features</span></span>
<span id="cb47-70"><a href="#cb47-70" aria-hidden="true" tabindex="-1"></a>tsk_ames<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb47-71"><a href="#cb47-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-72"><a href="#cb47-72" aria-hidden="true" tabindex="-1"></a>msr_mae <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"regr.mae"</span>)</span>
<span id="cb47-73"><a href="#cb47-73" aria-hidden="true" tabindex="-1"></a>rsmp_cv3 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb47-74"><a href="#cb47-74" aria-hidden="true" tabindex="-1"></a>rsmp_cv3<span class="sc">$</span><span class="fu">instantiate</span>(tsk_ames)</span>
<span id="cb47-75"><a href="#cb47-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-76"><a href="#cb47-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-77"><a href="#cb47-77" aria-hidden="true" tabindex="-1"></a>Lastly, we run a very simple experiment to verify our setup works as expected with a simple featureless baseline, note below we set <span class="in">`robust = TRUE`</span> to always predict the *median* sale price as opposed to the *mean*.</span>
<span id="cb47-78"><a href="#cb47-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-79"><a href="#cb47-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-008, message=FALSE}</span></span>
<span id="cb47-80"><a href="#cb47-80" aria-hidden="true" tabindex="-1"></a>lrn_baseline <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"regr.featureless"</span>, <span class="at">robust =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-81"><a href="#cb47-81" aria-hidden="true" tabindex="-1"></a>lrn_baseline<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"Baseline"</span></span>
<span id="cb47-82"><a href="#cb47-82" aria-hidden="true" tabindex="-1"></a>rr_baseline <span class="ot">=</span> <span class="fu">resample</span>(tsk_ames, lrn_baseline, rsmp_cv3)</span>
<span id="cb47-83"><a href="#cb47-83" aria-hidden="true" tabindex="-1"></a>rr_baseline<span class="sc">$</span><span class="fu">aggregate</span>(msr_mae)</span>
<span id="cb47-84"><a href="#cb47-84" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-85"><a href="#cb47-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-86"><a href="#cb47-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## Factor Encoding</span></span>
<span id="cb47-87"><a href="#cb47-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-88"><a href="#cb47-88" aria-hidden="true" tabindex="-1"></a>Many machine learning algorithm implementations, such as XGBoost <span class="co">[</span><span class="ot">@chen2016xgboost</span><span class="co">]</span>, cannot handle categorical data and so categorical features must be encoded\index{encoding} into numerical variables.</span>
<span id="cb47-89"><a href="#cb47-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-90"><a href="#cb47-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-010, message=FALSE, error=TRUE}</span></span>
<span id="cb47-91"><a href="#cb47-91" aria-hidden="true" tabindex="-1"></a>lrn_xgb <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"regr.xgboost"</span>, <span class="at">nrounds =</span> <span class="dv">100</span>)</span>
<span id="cb47-92"><a href="#cb47-92" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_ames)</span>
<span id="cb47-93"><a href="#cb47-93" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-94"><a href="#cb47-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-95"><a href="#cb47-95" aria-hidden="true" tabindex="-1"></a>Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned.</span>
<span id="cb47-96"><a href="#cb47-96" aria-hidden="true" tabindex="-1"></a>For now, we will consider high-cardinality to be features with more than 10 levels:</span>
<span id="cb47-97"><a href="#cb47-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-100"><a href="#cb47-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-101"><a href="#cb47-101" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">&gt;</span> <span class="dv">10</span>))</span>
<span id="cb47-102"><a href="#cb47-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-103"><a href="#cb47-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-104"><a href="#cb47-104" aria-hidden="true" tabindex="-1"></a>Binary features can be trivially encoded by setting one of the feature levels to <span class="in">`1`</span> and the other to <span class="in">`0`</span>.</span>
<span id="cb47-105"><a href="#cb47-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-108"><a href="#cb47-108" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-109"><a href="#cb47-109" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">==</span> <span class="dv">2</span>))</span>
<span id="cb47-110"><a href="#cb47-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-111"><a href="#cb47-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-112"><a href="#cb47-112" aria-hidden="true" tabindex="-1"></a>Low-cardinality features can be handled by <span class="in">`r index('one-hot encoding', 'one-hot', parent = 'encoding', aside = TRUE)`</span>.</span>
<span id="cb47-113"><a href="#cb47-113" aria-hidden="true" tabindex="-1"></a>One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature.</span>
<span id="cb47-114"><a href="#cb47-114" aria-hidden="true" tabindex="-1"></a>Theoretically, it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation.</span>
<span id="cb47-115"><a href="#cb47-115" aria-hidden="true" tabindex="-1"></a>This is typically called dummy\index{dummy encoding|see{encoding, treatment}} or treatment encoding\index{encoding!treatment} and is required if the learner is a generalized linear model (GLM) or additive model (GAM)\index{generalized linear model}.</span>
<span id="cb47-116"><a href="#cb47-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-117"><a href="#cb47-117" aria-hidden="true" tabindex="-1"></a>Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.</span>
<span id="cb47-118"><a href="#cb47-118" aria-hidden="true" tabindex="-1"></a>Impact encoding <span class="co">[</span><span class="ot">@MicciBarreca2001</span><span class="co">]</span> is a good approach for handling high-cardinality features.</span>
<span id="cb47-119"><a href="#cb47-119" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Impact encoding', 'impact', parent = 'encoding', aside = TRUE)`</span> converts categorical features into numeric values.</span>
<span id="cb47-120"><a href="#cb47-120" aria-hidden="true" tabindex="-1"></a>The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature.</span>
<span id="cb47-121"><a href="#cb47-121" aria-hidden="true" tabindex="-1"></a>Impact encoding involves the following steps:</span>
<span id="cb47-122"><a href="#cb47-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-123"><a href="#cb47-123" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Group the target variable by the categorical feature.</span>
<span id="cb47-124"><a href="#cb47-124" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the mean of the target variable for each group.</span>
<span id="cb47-125"><a href="#cb47-125" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Compute the global mean of the target variable.</span>
<span id="cb47-126"><a href="#cb47-126" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.</span>
<span id="cb47-127"><a href="#cb47-127" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Replace the categorical feature with the impact scores.</span>
<span id="cb47-128"><a href="#cb47-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-129"><a href="#cb47-129" aria-hidden="true" tabindex="-1"></a>Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target.</span>
<span id="cb47-130"><a href="#cb47-130" aria-hidden="true" tabindex="-1"></a>Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features.</span>
<span id="cb47-131"><a href="#cb47-131" aria-hidden="true" tabindex="-1"></a>As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data (@sec-performance).</span>
<span id="cb47-132"><a href="#cb47-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-133"><a href="#cb47-133" aria-hidden="true" tabindex="-1"></a>As well as encoding features, other basic preprocessing steps for categorical features include removing constant features (which only have one level and may have been removed as part of data cleaning), and collapsing levels that occur very rarely.</span>
<span id="cb47-134"><a href="#cb47-134" aria-hidden="true" tabindex="-1"></a>These types of problems can occur as artifacts of resampling as the dataset size is further reduced.</span>
<span id="cb47-135"><a href="#cb47-135" aria-hidden="true" tabindex="-1"></a>Stratification on such features would be an alternative way to mitigate this (@sec-strat-group).</span>
<span id="cb47-136"><a href="#cb47-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-137"><a href="#cb47-137" aria-hidden="true" tabindex="-1"></a>In the code below we use <span class="in">`po("removeconstants")`</span> to remove features with only one level, <span class="in">`po("collapsefactors")`</span> to collapse levels that occur less than 1% of the time in the data, <span class="in">`po("encodeimpact")`</span> to impact-encode high-cardinality features, <span class="in">`po("encode", method = "one-hot")`</span> to one-hot encode low-cardinality features, and finally <span class="in">`po("encode", method = "treatment")`</span> to treatment encode binary features.</span>
<span id="cb47-138"><a href="#cb47-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-139"><a href="#cb47-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-011, message=FALSE}</span></span>
<span id="cb47-140"><a href="#cb47-140" aria-hidden="true" tabindex="-1"></a>factor_pipeline <span class="ot">=</span></span>
<span id="cb47-141"><a href="#cb47-141" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"removeconstants"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-142"><a href="#cb47-142" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"collapsefactors"</span>, <span class="at">no_collapse_above_prevalence =</span> <span class="fl">0.01</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-143"><a href="#cb47-143" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"encodeimpact"</span>,</span>
<span id="cb47-144"><a href="#cb47-144" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">10</span>),</span>
<span id="cb47-145"><a href="#cb47-145" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">"high_card_enc"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-146"><a href="#cb47-146" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"encode"</span>, <span class="at">method =</span> <span class="st">"one-hot"</span>,</span>
<span id="cb47-147"><a href="#cb47-147" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">2</span>),</span>
<span id="cb47-148"><a href="#cb47-148" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">"low_card_enc"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-149"><a href="#cb47-149" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"encode"</span>, <span class="at">method =</span> <span class="st">"treatment"</span>,</span>
<span id="cb47-150"><a href="#cb47-150" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"factor"</span>), <span class="at">id =</span> <span class="st">"binary_enc"</span>)</span>
<span id="cb47-151"><a href="#cb47-151" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-152"><a href="#cb47-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-153"><a href="#cb47-153" aria-hidden="true" tabindex="-1"></a>Now we can apply this pipeline to our xgboost model to use it in a benchmark experiment; we also compare a simpler pipeline that only uses one-hot encoding to demonstrate performance differences resulting from different strategies.</span>
<span id="cb47-154"><a href="#cb47-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-155"><a href="#cb47-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-013, message=FALSE}</span></span>
<span id="cb47-156"><a href="#cb47-156" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact <span class="ot">=</span> <span class="fu">as_learner</span>(factor_pipeline <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb47-157"><a href="#cb47-157" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"XGB_enc_impact"</span></span>
<span id="cb47-158"><a href="#cb47-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-159"><a href="#cb47-159" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">"encode"</span>) <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb47-160"><a href="#cb47-160" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"XGB_enc_onehot"</span></span>
<span id="cb47-161"><a href="#cb47-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-162"><a href="#cb47-162" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames,</span>
<span id="cb47-163"><a href="#cb47-163" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(lrn_baseline, glrn_xgb_impact, glrn_xgb_one_hot), rsmp_cv3))</span>
<span id="cb47-164"><a href="#cb47-164" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> msr_mae)[, .(learner_id, regr.mae)]</span>
<span id="cb47-165"><a href="#cb47-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-166"><a href="#cb47-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-167"><a href="#cb47-167" aria-hidden="true" tabindex="-1"></a>In this small experiment, we see that the difference between the extended factor encoding pipeline and the simpler one-hot encoding strategy pipeline is only very small.</span>
<span id="cb47-168"><a href="#cb47-168" aria-hidden="true" tabindex="-1"></a>If you are interested in learning more about different encoding strategies, including a benchmark study comparing them, we recommend @pargent2022regularized.</span>
<span id="cb47-169"><a href="#cb47-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-170"><a href="#cb47-170" aria-hidden="true" tabindex="-1"></a><span class="fu">## Missing Values {#sec-preprocessing-missing}</span></span>
<span id="cb47-171"><a href="#cb47-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-172"><a href="#cb47-172" aria-hidden="true" tabindex="-1"></a>A common problem in real-world data is <span class="in">`r index('missing values', 'missing data')`</span> in features.</span>
<span id="cb47-173"><a href="#cb47-173" aria-hidden="true" tabindex="-1"></a>In the Ames dataset, several variables have at least one missing data point:</span>
<span id="cb47-174"><a href="#cb47-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-177"><a href="#cb47-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-178"><a href="#cb47-178" aria-hidden="true" tabindex="-1"></a><span class="co"># print first five with missing data</span></span>
<span id="cb47-179"><a href="#cb47-179" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(tsk_ames<span class="sc">$</span><span class="fu">missings</span>() <span class="sc">&gt;</span> <span class="dv">0</span>))[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb47-180"><a href="#cb47-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-181"><a href="#cb47-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-182"><a href="#cb47-182" aria-hidden="true" tabindex="-1"></a>Many learners cannot handle missing values automatically (e.g., <span class="in">`lrn("regr.ranger")`</span> and <span class="in">`lrn("regr.lm")`</span>) and others may be able to handle missing values but may use simple methods that are not ideal (e.g., just omitting rows with missing data).</span>
<span id="cb47-183"><a href="#cb47-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-184"><a href="#cb47-184" aria-hidden="true" tabindex="-1"></a>The simplest <span class="in">`r index('data imputation', aside = TRUE)`</span> method is to replace missing values by the feature's mean (<span class="in">`po("imputemean")`</span>) (@fig-imputation), median (<span class="in">`po("imputemedian")`</span>), or mode (<span class="in">`po("imputemode")`</span>).</span>
<span id="cb47-185"><a href="#cb47-185" aria-hidden="true" tabindex="-1"></a>Alternatively, one can impute by sampling from the empirical distribution of the feature, for example a histogram (<span class="in">`po("imputehist")`</span>).</span>
<span id="cb47-186"><a href="#cb47-186" aria-hidden="true" tabindex="-1"></a>Instead of guessing at what a missing feature might be, missing values could instead be replaced by a new level, for example, called <span class="in">`.MISSING`</span> (<span class="in">`po("imputeoor")`</span>).</span>
<span id="cb47-187"><a href="#cb47-187" aria-hidden="true" tabindex="-1"></a>For numeric features, @ding2010investigation show that for binary classification and tree-based models, encoding missing values out-of-range (OOR), e.g. a constant value above the largest observed value, is a reasonable approach.</span>
<span id="cb47-188"><a href="#cb47-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-189"><a href="#cb47-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE, out.width = "60%"}</span></span>
<span id="cb47-190"><a href="#cb47-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-imputation</span></span>
<span id="cb47-191"><a href="#cb47-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Mean imputation of missing values using observed values.</span></span>
<span id="cb47-192"><a href="#cb47-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "On the left is a vector of numbers in a column, (1.3, NA, 1.5, NA). The non-NA numbers have arrows pointing to (1.3+1.5)/2, which then has an arrow pointing to a vector of numbers in a column on the right but now (1.3, 1.4, 1.5, 1.4) with '1.4' in red to highlight they were imputed with the mean."</span></span>
<span id="cb47-193"><a href="#cb47-193" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"mlr3book_figures-13"</span>)</span>
<span id="cb47-194"><a href="#cb47-194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-195"><a href="#cb47-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-196"><a href="#cb47-196" aria-hidden="true" tabindex="-1"></a>It is often important for predictive tasks that you keep track of missing data as it is common for missing data to be informative in itself.</span>
<span id="cb47-197"><a href="#cb47-197" aria-hidden="true" tabindex="-1"></a>To preserve the information about which data was missing, imputation should be tracked by adding binary indicator features (one for each imputed feature) that are <span class="in">`1`</span> if the feature was missing for an observation and <span class="in">`0`</span> if it was present (<span class="in">`po("missind")`</span>).</span>
<span id="cb47-198"><a href="#cb47-198" aria-hidden="true" tabindex="-1"></a>It is important to note that recording this information will not prevent problems in model interpretation on its own.</span>
<span id="cb47-199"><a href="#cb47-199" aria-hidden="true" tabindex="-1"></a>As a real-world example, medical data are typically collected more extensively for White communities than for racially minoritized communities.</span>
<span id="cb47-200"><a href="#cb47-200" aria-hidden="true" tabindex="-1"></a>Imputing data from minoritized communities would at best mask this data bias, and at worst would make the data bias even worse by making vastly inaccurate assumptions (see @sec-fairness for data bias and algorithmic fairness).</span>
<span id="cb47-201"><a href="#cb47-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-202"><a href="#cb47-202" aria-hidden="true" tabindex="-1"></a>In the code below we create a pipeline from the <span class="in">`r ref("PipeOp")`</span>s listed above as well as making use of <span class="in">`po("featureunion")`</span> to combine multiple <span class="in">`PipeOp`</span>s acting on the <span class="in">`"integer"`</span> columns.</span>
<span id="cb47-203"><a href="#cb47-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-204"><a href="#cb47-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-014-evalF, eval = FALSE}</span></span>
<span id="cb47-205"><a href="#cb47-205" aria-hidden="true" tabindex="-1"></a>impute_hist <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb47-206"><a href="#cb47-206" aria-hidden="true" tabindex="-1"></a>      <span class="fu">po</span>(<span class="st">"missind"</span>, <span class="at">type =</span> <span class="st">"integer"</span>,</span>
<span id="cb47-207"><a href="#cb47-207" aria-hidden="true" tabindex="-1"></a>          <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"integer"</span>)</span>
<span id="cb47-208"><a href="#cb47-208" aria-hidden="true" tabindex="-1"></a>      ),</span>
<span id="cb47-209"><a href="#cb47-209" aria-hidden="true" tabindex="-1"></a>      <span class="fu">po</span>(<span class="st">"imputehist"</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"integer"</span>))</span>
<span id="cb47-210"><a href="#cb47-210" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-211"><a href="#cb47-211" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"featureunion"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-212"><a href="#cb47-212" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"imputeoor"</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"factor"</span>))</span>
<span id="cb47-213"><a href="#cb47-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-214"><a href="#cb47-214" aria-hidden="true" tabindex="-1"></a>impute_hist<span class="sc">$</span><span class="fu">plot</span>(<span class="at">horizontal =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-215"><a href="#cb47-215" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-216"><a href="#cb47-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-217"><a href="#cb47-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-014-evalT, fig.width = 8, echo = FALSE}</span></span>
<span id="cb47-218"><a href="#cb47-218" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-impute</span></span>
<span id="cb47-219"><a href="#cb47-219" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pipeline to impute missing values of numeric features by histogram with binary indicators and missings in categoricals out-of-range with a new level.</span></span>
<span id="cb47-220"><a href="#cb47-220" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Flow diagram shows '&lt;INPUT&gt;' with arrows to 'missind' and 'imputehist', which both have arrows to 'featureunion', which has an arrow to 'imputeoor' that has an arrow to '&lt;OUTPUT'&gt;."</span></span>
<span id="cb47-221"><a href="#cb47-221" aria-hidden="true" tabindex="-1"></a>impute_hist <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb47-222"><a href="#cb47-222" aria-hidden="true" tabindex="-1"></a>      <span class="fu">po</span>(<span class="st">"missind"</span>, <span class="at">type =</span> <span class="st">"integer"</span>,</span>
<span id="cb47-223"><a href="#cb47-223" aria-hidden="true" tabindex="-1"></a>          <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"integer"</span>)</span>
<span id="cb47-224"><a href="#cb47-224" aria-hidden="true" tabindex="-1"></a>      ),</span>
<span id="cb47-225"><a href="#cb47-225" aria-hidden="true" tabindex="-1"></a>      <span class="fu">po</span>(<span class="st">"imputehist"</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"integer"</span>))</span>
<span id="cb47-226"><a href="#cb47-226" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-227"><a href="#cb47-227" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"featureunion"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb47-228"><a href="#cb47-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">"imputeoor"</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"factor"</span>))</span>
<span id="cb47-229"><a href="#cb47-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-230"><a href="#cb47-230" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">=</span> magick<span class="sc">::</span><span class="fu">image_graph</span>(<span class="at">width =</span> <span class="dv">1500</span>, <span class="at">height =</span> <span class="dv">1000</span>, <span class="at">res =</span> <span class="dv">100</span>, <span class="at">pointsize =</span> <span class="dv">24</span>)</span>
<span id="cb47-231"><a href="#cb47-231" aria-hidden="true" tabindex="-1"></a>impute_hist<span class="sc">$</span><span class="fu">plot</span>(<span class="at">horizontal =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-232"><a href="#cb47-232" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(<span class="fu">dev.off</span>())</span>
<span id="cb47-233"><a href="#cb47-233" aria-hidden="true" tabindex="-1"></a>magick<span class="sc">::</span><span class="fu">image_trim</span>(fig)</span>
<span id="cb47-234"><a href="#cb47-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-235"><a href="#cb47-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-236"><a href="#cb47-236" aria-hidden="true" tabindex="-1"></a>Using this pipeline we can now run experiments with <span class="in">`lrn("regr.ranger")`</span>, which cannot handle missing data; we also compare a simpler pipeline that only uses OOR imputation to demonstrate performance differences resulting from different strategies.</span>
<span id="cb47-237"><a href="#cb47-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-238"><a href="#cb47-238" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-016}</span></span>
<span id="cb47-239"><a href="#cb47-239" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist <span class="ot">=</span> <span class="fu">as_learner</span>(impute_hist <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>))</span>
<span id="cb47-240"><a href="#cb47-240" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"RF_imp_Hist"</span></span>
<span id="cb47-241"><a href="#cb47-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-242"><a href="#cb47-242" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">"imputeoor"</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>))</span>
<span id="cb47-243"><a href="#cb47-243" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"RF_imp_OOR"</span></span>
<span id="cb47-244"><a href="#cb47-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-245"><a href="#cb47-245" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tsk_ames,</span>
<span id="cb47-246"><a href="#cb47-246" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(glrn_rf_impute_hist, glrn_rf_impute_oor), rsmp_cv3)</span>
<span id="cb47-247"><a href="#cb47-247" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb47-248"><a href="#cb47-248" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb47-249"><a href="#cb47-249" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> msr_mae)[, .(learner_id, regr.mae)]</span>
<span id="cb47-250"><a href="#cb47-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-251"><a href="#cb47-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-252"><a href="#cb47-252" aria-hidden="true" tabindex="-1"></a>Similarly to encoding, we see limited differences in performance between the different imputation strategies.</span>
<span id="cb47-253"><a href="#cb47-253" aria-hidden="true" tabindex="-1"></a>This is expected here and confirms the findings of @ding2010investigation -- out-of-range imputation is a simple yet effective imputation for tree-based methods.</span>
<span id="cb47-254"><a href="#cb47-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-255"><a href="#cb47-255" aria-hidden="true" tabindex="-1"></a>Many more advanced imputation strategies exist, including model-based imputation where machine learning models are used to predict missing values, and multiple imputation where data is repeatedly resampled and imputed in each sample (e.g., by mean imputation) to attain more robust estimates.</span>
<span id="cb47-256"><a href="#cb47-256" aria-hidden="true" tabindex="-1"></a>However, these more advanced techniques rarely improve the models predictive performance substantially and the simple imputation techniques introduced above are usually sufficient <span class="co">[</span><span class="ot">@Poulos2018</span><span class="co">]</span>.</span>
<span id="cb47-257"><a href="#cb47-257" aria-hidden="true" tabindex="-1"></a>Nevertheless, these methods are still important, as finding imputations that fit well to the distribution of the observed values allows a model to be fitted that can be interpreted and analyzed in a second step.</span>
<span id="cb47-258"><a href="#cb47-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-259"><a href="#cb47-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pipeline Robustify {#sec-prepro-robustify}</span></span>
<span id="cb47-260"><a href="#cb47-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-261"><a href="#cb47-261" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3pipelines`</span> offers a simple and reusable pipeline for (among other things) <span class="in">`r index('imputation')`</span> and factor <span class="in">`r index('encoding')`</span> called <span class="in">`r index('ppl("robustify")', aside = TRUE, code = TRUE)`</span>, which includes sensible defaults that can be used most of the time when encoding or imputing data.</span>
<span id="cb47-262"><a href="#cb47-262" aria-hidden="true" tabindex="-1"></a>The pipeline includes the following <span class="in">`r ref("PipeOp")`</span>s (some are applied multiple times and most use selectors):</span>
<span id="cb47-263"><a href="#cb47-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-264"><a href="#cb47-264" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`po("removeconstants")`</span> -- Constant features are removed.</span>
<span id="cb47-265"><a href="#cb47-265" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`po("colapply")`</span> -- Character and ordinal features are encoded as categorical, and date/time features are encoded as numeric.</span>
<span id="cb47-266"><a href="#cb47-266" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="in">`po("imputehist")`</span> -- Numeric features are imputed by histogram sampling.</span>
<span id="cb47-267"><a href="#cb47-267" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="in">`po("imputesample")`</span> -- Logical features are imputed by sampling from the empirical distribution -- this only affects the <span class="in">`$predict()`</span>-step.</span>
<span id="cb47-268"><a href="#cb47-268" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span><span class="in">`po("missind")`</span> -- Missing data indicators are added for imputed numeric and logical variables.</span>
<span id="cb47-269"><a href="#cb47-269" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span><span class="in">`po("imputeoor")`</span> -- Missing values of categorical features are encoded with a new level.</span>
<span id="cb47-270"><a href="#cb47-270" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span><span class="in">`po("fixfactors")`</span> -- Fixes levels of categorical features such that the same levels are present during prediction and training (which may involve dropping empty factor levels).</span>
<span id="cb47-271"><a href="#cb47-271" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span><span class="in">`po("imputesample")`</span> -- Missing values in categorical features introduced from dropping levels in the previous step are imputed by sampling from the empirical distributions.</span>
<span id="cb47-272"><a href="#cb47-272" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span><span class="in">`po("collapsefactors")`</span> -- Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are less than a certan number of levels, controlled by the <span class="in">`max_cardinality`</span> argument (with a conservative default of <span class="in">`1000`</span>).</span>
<span id="cb47-273"><a href="#cb47-273" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span><span class="in">`po("encode")`</span> -- Categorical features are one-hot encoded.</span>
<span id="cb47-274"><a href="#cb47-274" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span><span class="in">`po("removeconstants")`</span> -- Constant features that might have been created in the previous steps are removed.</span>
<span id="cb47-275"><a href="#cb47-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-276"><a href="#cb47-276" aria-hidden="true" tabindex="-1"></a><span class="in">`ppl("robustify")`</span> has optional arguments <span class="in">`task`</span> and <span class="in">`learner`</span>.</span>
<span id="cb47-277"><a href="#cb47-277" aria-hidden="true" tabindex="-1"></a>If these are provided, then the resulting pipeline will be set up to handle the given task and learner specifically, for example, it will not impute missing values if the learner has the <span class="in">`"missings"`</span> property, or if there are no missing values in the task to begin with.</span>
<span id="cb47-278"><a href="#cb47-278" aria-hidden="true" tabindex="-1"></a>By default, when <span class="in">`task`</span> and <span class="in">`learner`</span> are not provided, the graph is set up to be defensive: it imputes all missing values and converts all feature types to numerics.</span>
<span id="cb47-279"><a href="#cb47-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-280"><a href="#cb47-280" aria-hidden="true" tabindex="-1"></a>Linear regression is a simple model that cannot handle most problems that we may face when processing data, but with the <span class="in">`ppl("robustify")`</span> we can now include it in our experiment:</span>
<span id="cb47-281"><a href="#cb47-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-282"><a href="#cb47-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-019, warning = FALSE}</span></span>
<span id="cb47-283"><a href="#cb47-283" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">"robustify"</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"regr.lm"</span>))</span>
<span id="cb47-284"><a href="#cb47-284" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"lm_robust"</span></span>
<span id="cb47-285"><a href="#cb47-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-286"><a href="#cb47-286" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_lm_robust,  rsmp_cv3))</span>
<span id="cb47-287"><a href="#cb47-287" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb47-288"><a href="#cb47-288" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> msr_mae)[, .(learner_id, regr.mae)]</span>
<span id="cb47-289"><a href="#cb47-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-290"><a href="#cb47-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-291"><a href="#cb47-291" aria-hidden="true" tabindex="-1"></a>Robustifying the linear regression results in a model that vastly outperforms the featureless baseline and is competitive when compared to more complex machine learning models.</span>
<span id="cb47-292"><a href="#cb47-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-293"><a href="#cb47-293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transforming Features and Targets {#sec-prepro-scale}</span></span>
<span id="cb47-294"><a href="#cb47-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-295"><a href="#cb47-295" aria-hidden="true" tabindex="-1"></a>Simple transformations of features and the target can be beneficial (and sometimes essential) for certain learners.</span>
<span id="cb47-296"><a href="#cb47-296" aria-hidden="true" tabindex="-1"></a>In particular, log transformation of the target can help in making the distribution more symmetrical and can help reduce the impact of outliers.</span>
<span id="cb47-297"><a href="#cb47-297" aria-hidden="true" tabindex="-1"></a>Similarly, log transformation of skewed features can help to reduce the influence of outliers.</span>
<span id="cb47-298"><a href="#cb47-298" aria-hidden="true" tabindex="-1"></a>In @fig-sale we plot the distribution of the target in the <span class="in">`ames`</span> dataset and then the log-transformed target, we can see how simply taking the log of the variable results in a distribution that is much more symmetrical and with fewer outliers.</span>
<span id="cb47-299"><a href="#cb47-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-300"><a href="#cb47-300" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-001, message=FALSE}</span></span>
<span id="cb47-301"><a href="#cb47-301" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb47-302"><a href="#cb47-302" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb47-303"><a href="#cb47-303" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb47-304"><a href="#cb47-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-305"><a href="#cb47-305" aria-hidden="true" tabindex="-1"></a><span class="co"># copy ames data</span></span>
<span id="cb47-306"><a href="#cb47-306" aria-hidden="true" tabindex="-1"></a>log_ames <span class="ot">=</span> <span class="fu">copy</span>(ames)</span>
<span id="cb47-307"><a href="#cb47-307" aria-hidden="true" tabindex="-1"></a><span class="co"># log transform target</span></span>
<span id="cb47-308"><a href="#cb47-308" aria-hidden="true" tabindex="-1"></a>log_ames[, logSalePrice <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(Sale_Price)]</span>
<span id="cb47-309"><a href="#cb47-309" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb47-310"><a href="#cb47-310" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">"Sale_Price"</span>)) <span class="sc">+</span></span>
<span id="cb47-311"><a href="#cb47-311" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">"logSalePrice"</span>))</span>
<span id="cb47-312"><a href="#cb47-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-313"><a href="#cb47-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-316"><a href="#cb47-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-317"><a href="#cb47-317" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-sale</span></span>
<span id="cb47-318"><a href="#cb47-318" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Distribution of house sales prices (in USD) in the ames dataset before (left) and after (right) log transformation. Before transformation there is a skewed distribution of prices towards cheaper properties with a few outliers of very expensive properties. After transformation the distribution is much more symmetrical with the majority of points evenly spread around the same range.</span></span>
<span id="cb47-319"><a href="#cb47-319" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Two boxplots. Left plot shows house prices up to $600,000, the majority of prices are between roughly $100,000-$200,000. Right plot shows log house prices primarily around 12 with an even range between 11 and 13 and a few outliers on both sides.</span></span>
<span id="cb47-320"><a href="#cb47-320" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb47-321"><a href="#cb47-321" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb47-322"><a href="#cb47-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb47-323"><a href="#cb47-323" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb47-324"><a href="#cb47-324" aria-hidden="true" tabindex="-1"></a>plt<span class="sc">$</span>layers[[<span class="dv">1</span>]]<span class="sc">$</span>aes_params<span class="sc">$</span>fill <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb47-325"><a href="#cb47-325" aria-hidden="true" tabindex="-1"></a>plt<span class="sc">$</span>patches<span class="sc">$</span>plots[[<span class="dv">1</span>]]<span class="sc">$</span>layers[[<span class="dv">1</span>]]<span class="sc">$</span>aes_params<span class="sc">$</span>fill <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb47-326"><a href="#cb47-326" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(plt)</span>
<span id="cb47-327"><a href="#cb47-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-328"><a href="#cb47-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-329"><a href="#cb47-329" aria-hidden="true" tabindex="-1"></a>Normalization of features may also be necessary to ensure features with a larger scale do not have a higher impact, which is especially important for distance-based methods such as <span class="in">`r index('k-nearest neighbors')`</span> models or regularized parametric models such as Lasso or Elastic net.</span>
<span id="cb47-330"><a href="#cb47-330" aria-hidden="true" tabindex="-1"></a>Many models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing, though if this is required then <span class="in">`po("scale")`</span> can be used to center and scale numeric features.</span>
<span id="cb47-331"><a href="#cb47-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-332"><a href="#cb47-332" aria-hidden="true" tabindex="-1"></a>Any transformations applied to the target during training must be inverted during model prediction to ensure predictions are made on the correct scale.</span>
<span id="cb47-333"><a href="#cb47-333" aria-hidden="true" tabindex="-1"></a>By example, say we are interested in log transforming the target, then we would take the following steps:</span>
<span id="cb47-334"><a href="#cb47-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-337"><a href="#cb47-337" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-338"><a href="#cb47-338" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">5</span>), <span class="at">y =</span> <span class="fu">runif</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>))</span>
<span id="cb47-339"><a href="#cb47-339" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb47-340"><a href="#cb47-340" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. log transform the target</span></span>
<span id="cb47-341"><a href="#cb47-341" aria-hidden="true" tabindex="-1"></a>df[, y <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(y)]</span>
<span id="cb47-342"><a href="#cb47-342" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>y</span>
<span id="cb47-343"><a href="#cb47-343" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. make linear regression predictions</span></span>
<span id="cb47-344"><a href="#cb47-344" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the log-transformed scale</span></span>
<span id="cb47-345"><a href="#cb47-345" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, df), df)</span>
<span id="cb47-346"><a href="#cb47-346" aria-hidden="true" tabindex="-1"></a>yhat</span>
<span id="cb47-347"><a href="#cb47-347" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. transform to correct scale with inverse of log function</span></span>
<span id="cb47-348"><a href="#cb47-348" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the original scale</span></span>
<span id="cb47-349"><a href="#cb47-349" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(yhat)</span>
<span id="cb47-350"><a href="#cb47-350" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-351"><a href="#cb47-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-352"><a href="#cb47-352" aria-hidden="true" tabindex="-1"></a>In this simple experiment, we could manually transform and invert the target, however, this is much more complex when dealing with resampling and benchmarking experiments and so the pipeline <span class="in">`ppl("targettrafo")`</span> will do this heavy lifting for you.</span>
<span id="cb47-353"><a href="#cb47-353" aria-hidden="true" tabindex="-1"></a>The pipeline includes a parameter <span class="in">`targetmutate.trafo`</span> for the transformation to be applied during training to the target, as well as <span class="in">`targetmutate.inverter`</span> for the transformation to be applied to invert the original transformation during prediction.</span>
<span id="cb47-354"><a href="#cb47-354" aria-hidden="true" tabindex="-1"></a>So now let us consider the log transformation by adding this pipeline to our robust linear regression model:</span>
<span id="cb47-355"><a href="#cb47-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-356"><a href="#cb47-356" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-020, warning=FALSE}</span></span>
<span id="cb47-357"><a href="#cb47-357" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">"targettrafo"</span>,</span>
<span id="cb47-358"><a href="#cb47-358" aria-hidden="true" tabindex="-1"></a>  <span class="at">graph =</span> glrn_lm_robust,</span>
<span id="cb47-359"><a href="#cb47-359" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.trafo =</span> <span class="cf">function</span>(x) <span class="fu">log</span>(x),</span>
<span id="cb47-360"><a href="#cb47-360" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.inverter =</span> <span class="cf">function</span>(x) <span class="fu">list</span>(<span class="at">response =</span> <span class="fu">exp</span>(x<span class="sc">$</span>response))))</span>
<span id="cb47-361"><a href="#cb47-361" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"lm_robust_logtrafo"</span></span>
<span id="cb47-362"><a href="#cb47-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-363"><a href="#cb47-363" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_log_lm_robust,</span>
<span id="cb47-364"><a href="#cb47-364" aria-hidden="true" tabindex="-1"></a>  rsmp_cv3))</span>
<span id="cb47-365"><a href="#cb47-365" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb47-366"><a href="#cb47-366" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> msr_mae)[, .(learner_id, regr.mae)]</span>
<span id="cb47-367"><a href="#cb47-367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-368"><a href="#cb47-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-369"><a href="#cb47-369" aria-hidden="true" tabindex="-1"></a>With the target transformation and the <span class="in">`ppl("robustify")`</span>, the simple linear regression now appears to be the best-performing model.</span>
<span id="cb47-370"><a href="#cb47-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-371"><a href="#cb47-371" aria-hidden="true" tabindex="-1"></a><span class="fu">## Functional Feature Extraction</span></span>
<span id="cb47-372"><a href="#cb47-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-373"><a href="#cb47-373" aria-hidden="true" tabindex="-1"></a>As a final step of data preprocessing, we will look at <span class="in">`r index('feature extraction')`</span> from functional features.</span>
<span id="cb47-374"><a href="#cb47-374" aria-hidden="true" tabindex="-1"></a>In @sec-feature-selection we look at automated <span class="in">`r index('feature selection')`</span> and how automated approaches with filters and wrappers can be used to reduce a dataset to an optimized set of features.</span>
<span id="cb47-375"><a href="#cb47-375" aria-hidden="true" tabindex="-1"></a>Functional feature extraction differs from this process as we are now interested in features that are dependent on one another and together may provide useful information but not individually.</span>
<span id="cb47-376"><a href="#cb47-376" aria-hidden="true" tabindex="-1"></a>@fig-functional-features visualizes the difference between regular and functional features.</span>
<span id="cb47-377"><a href="#cb47-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-378"><a href="#cb47-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-003, echo = FALSE}</span></span>
<span id="cb47-379"><a href="#cb47-379" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-functional-features</span></span>
<span id="cb47-380"><a href="#cb47-380" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Variables x1,x2,x3 are regular features, variables xt1,...,xt365 are functional features that could be plotted to identify important properties.</span></span>
<span id="cb47-381"><a href="#cb47-381" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: On the left is a table with columns 'x1,x2,x3,xt1,xt2,...,xt365'. Below the first three columns is the label 'Regular Features', below the others is the label 'Functional Features, e.g. days in year'. The table has a bidirectional arrow to a line graph that indicates plotting of one row of functional features.</span></span>
<span id="cb47-382"><a href="#cb47-382" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"mlr3book_figures-14"</span>)</span>
<span id="cb47-383"><a href="#cb47-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-384"><a href="#cb47-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-385"><a href="#cb47-385" aria-hidden="true" tabindex="-1"></a>As a concrete example, consider the power consumption of kitchen appliances in houses in the Ames dataset.</span>
<span id="cb47-386"><a href="#cb47-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-387"><a href="#cb47-387" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-023, message=FALSE, warning=FALSE}</span></span>
<span id="cb47-388"><a href="#cb47-388" aria-hidden="true" tabindex="-1"></a>energy_data <span class="ot">=</span> mlr3data<span class="sc">::</span>energy_usage</span>
<span id="cb47-389"><a href="#cb47-389" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-390"><a href="#cb47-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-391"><a href="#cb47-391" aria-hidden="true" tabindex="-1"></a>In this dataset, each row represents one house and each feature is the total power consumption from kitchen appliances at a given time <span class="co">[</span><span class="ot">@bagnall2017great</span><span class="co">]</span>.</span>
<span id="cb47-392"><a href="#cb47-392" aria-hidden="true" tabindex="-1"></a>The consumption is measured in two-minute intervals, resulting in 720 features.</span>
<span id="cb47-393"><a href="#cb47-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-394"><a href="#cb47-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-024, message=FALSE, warning=FALSE}</span></span>
<span id="cb47-395"><a href="#cb47-395" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-energy</span></span>
<span id="cb47-396"><a href="#cb47-396" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Energy consumption of one example house in a day, recorded in two-minute intervals.</span></span>
<span id="cb47-397"><a href="#cb47-397" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Line plot with '2-Minute Interval' on axis ranging from 1 to 720 and 'Power Consumption' on y-axis ranging from 0 to 20. There are spikes at around (200, 20), (300, 20), and then some consistently raised usage between (500-700, 3).</span></span>
<span id="cb47-398"><a href="#cb47-398" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb47-399"><a href="#cb47-399" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">as.numeric</span>(energy_data[<span class="dv">1</span>, ])),</span>
<span id="cb47-400"><a href="#cb47-400" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> y, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)) <span class="sc">+</span></span>
<span id="cb47-401"><a href="#cb47-401" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb47-402"><a href="#cb47-402" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"2-Minute Interval"</span>, <span class="at">y =</span> <span class="st">"Power Consumption"</span>)</span>
<span id="cb47-403"><a href="#cb47-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-404"><a href="#cb47-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-405"><a href="#cb47-405" aria-hidden="true" tabindex="-1"></a>Adding these 720 features to our full dataset is a bad idea as each individual feature does not provide meaningful information, similarly, we cannot automate selection of the best feature subset for the same reason.</span>
<span id="cb47-406"><a href="#cb47-406" aria-hidden="true" tabindex="-1"></a>Instead, we can *extract* information about the curves to gain insights into the kitchen's overall energy usage.</span>
<span id="cb47-407"><a href="#cb47-407" aria-hidden="true" tabindex="-1"></a>For example, we could extract the maximum used wattage, overall used wattage, number of peaks, and other similar features.</span>
<span id="cb47-408"><a href="#cb47-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-409"><a href="#cb47-409" aria-hidden="true" tabindex="-1"></a>To extract features we will write our own <span class="in">`r ref("PipeOp")`</span> that inherits from <span class="in">`r ref("PipeOpTaskPreprocSimple")`</span>.</span>
<span id="cb47-410"><a href="#cb47-410" aria-hidden="true" tabindex="-1"></a>To do this we add a private method called <span class="in">`.transform_dt`</span> that hardcodes the operations in our task.</span>
<span id="cb47-411"><a href="#cb47-411" aria-hidden="true" tabindex="-1"></a>In this example, we select the functional features (which all start with "att"), extract the mean, minimum, maximum, and variance of the power consumption, and then remove the functional features.</span>
<span id="cb47-412"><a href="#cb47-412" aria-hidden="true" tabindex="-1"></a>To read more about building custom <span class="in">`PipeOp`</span>s, open the corresponding vignette by running <span class="in">`vignette("extending", package = "mlr3pipelines")`</span> in R.</span>
<span id="cb47-413"><a href="#cb47-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-414"><a href="#cb47-414" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-025}</span></span>
<span id="cb47-415"><a href="#cb47-415" aria-hidden="true" tabindex="-1"></a>PipeOpFuncExtract <span class="ot">=</span> R6<span class="sc">::</span><span class="fu">R6Class</span>(<span class="st">"PipeOpFuncExtract"</span>,</span>
<span id="cb47-416"><a href="#cb47-416" aria-hidden="true" tabindex="-1"></a>  <span class="at">inherit =</span> mlr3pipelines<span class="sc">::</span>PipeOpTaskPreprocSimple,</span>
<span id="cb47-417"><a href="#cb47-417" aria-hidden="true" tabindex="-1"></a>  <span class="at">private =</span> <span class="fu">list</span>(</span>
<span id="cb47-418"><a href="#cb47-418" aria-hidden="true" tabindex="-1"></a>    <span class="at">.transform_dt =</span> <span class="cf">function</span>(dt, levels) {</span>
<span id="cb47-419"><a href="#cb47-419" aria-hidden="true" tabindex="-1"></a>        ffeat_names <span class="ot">=</span> <span class="fu">paste0</span>(<span class="st">"att"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)</span>
<span id="cb47-420"><a href="#cb47-420" aria-hidden="true" tabindex="-1"></a>        ffeats <span class="ot">=</span> dt[, ..ffeat_names]</span>
<span id="cb47-421"><a href="#cb47-421" aria-hidden="true" tabindex="-1"></a>        dt[, energy_means <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, mean)]</span>
<span id="cb47-422"><a href="#cb47-422" aria-hidden="true" tabindex="-1"></a>        dt[, energy_mins <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, min)]</span>
<span id="cb47-423"><a href="#cb47-423" aria-hidden="true" tabindex="-1"></a>        dt[, energy_maxs <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, max)]</span>
<span id="cb47-424"><a href="#cb47-424" aria-hidden="true" tabindex="-1"></a>        dt[, energy_vars <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, var)]</span>
<span id="cb47-425"><a href="#cb47-425" aria-hidden="true" tabindex="-1"></a>        dt[, (ffeat_names) <span class="sc">:</span><span class="er">=</span> <span class="cn">NULL</span>]</span>
<span id="cb47-426"><a href="#cb47-426" aria-hidden="true" tabindex="-1"></a>        dt</span>
<span id="cb47-427"><a href="#cb47-427" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb47-428"><a href="#cb47-428" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb47-429"><a href="#cb47-429" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-430"><a href="#cb47-430" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-431"><a href="#cb47-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-432"><a href="#cb47-432" aria-hidden="true" tabindex="-1"></a>Before using this in an experiment we first test that the <span class="in">`PipeOp`</span> works as expected.</span>
<span id="cb47-433"><a href="#cb47-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-434"><a href="#cb47-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-026}</span></span>
<span id="cb47-435"><a href="#cb47-435" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">cbind</span>(ames, energy_data)</span>
<span id="cb47-436"><a href="#cb47-436" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">as_task_regr</span>(tsk_ames_ext, <span class="st">"Sale_Price"</span>, <span class="st">"ames_ext"</span>)</span>
<span id="cb47-437"><a href="#cb47-437" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the redundant variables identified at the start of this chapter</span></span>
<span id="cb47-438"><a href="#cb47-438" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames_ext<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb47-439"><a href="#cb47-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-440"><a href="#cb47-440" aria-hidden="true" tabindex="-1"></a>func_extractor <span class="ot">=</span> PipeOpFuncExtract<span class="sc">$</span><span class="fu">new</span>(<span class="st">"energy_extract"</span>)</span>
<span id="cb47-441"><a href="#cb47-441" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> func_extractor<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_ames_ext))[[<span class="dv">1</span>]]</span>
<span id="cb47-442"><a href="#cb47-442" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">data</span>(<span class="dv">1</span>,</span>
<span id="cb47-443"><a href="#cb47-443" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">"energy_means"</span>, <span class="st">"energy_mins"</span>, <span class="st">"energy_maxs"</span>, <span class="st">"energy_vars"</span>))</span>
<span id="cb47-444"><a href="#cb47-444" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-445"><a href="#cb47-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-446"><a href="#cb47-446" aria-hidden="true" tabindex="-1"></a>These outputs look sensible compared to @fig-energy so we can now run our final benchmark experiment using feature extraction.</span>
<span id="cb47-447"><a href="#cb47-447" aria-hidden="true" tabindex="-1"></a>We do not need to add the <span class="in">`PipeOp`</span> to each learner as we can apply it once (as above) before any model training by applying it to all available data.</span>
<span id="cb47-448"><a href="#cb47-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-449"><a href="#cb47-449" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-027, warning=FALSE, R.options = list(datatable.print.nrows = 13, datatable.print.class = FALSE, datatable.print.keys = FALSE, datatable.print.trunc.cols = TRUE)}</span></span>
<span id="cb47-450"><a href="#cb47-450" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">list</span>(lrn_baseline, <span class="fu">lrn</span>(<span class="st">"regr.rpart"</span>), glrn_xgb_impact,</span>
<span id="cb47-451"><a href="#cb47-451" aria-hidden="true" tabindex="-1"></a>    glrn_rf_impute_oor, glrn_lm_robust, glrn_log_lm_robust)</span>
<span id="cb47-452"><a href="#cb47-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-453"><a href="#cb47-453" aria-hidden="true" tabindex="-1"></a>bmr_final <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(<span class="fu">c</span>(tsk_ames_ext, tsk_ames), learners,</span>
<span id="cb47-454"><a href="#cb47-454" aria-hidden="true" tabindex="-1"></a>  rsmp_cv3))</span>
<span id="cb47-455"><a href="#cb47-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-456"><a href="#cb47-456" aria-hidden="true" tabindex="-1"></a>perf <span class="ot">=</span> bmr_final<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> msr_mae)</span>
<span id="cb47-457"><a href="#cb47-457" aria-hidden="true" tabindex="-1"></a>perf[<span class="fu">order</span>(learner_id, task_id), .(task_id, learner_id, regr.mae)]</span>
<span id="cb47-458"><a href="#cb47-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-459"><a href="#cb47-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-460"><a href="#cb47-460" aria-hidden="true" tabindex="-1"></a>The final results indicate that adding these extracted features improved the performance of all models (except the featureless baseline).</span>
<span id="cb47-461"><a href="#cb47-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-462"><a href="#cb47-462" aria-hidden="true" tabindex="-1"></a>In this example, we could have just applied the transformations to the dataset directly and not used a <span class="in">`PipeOp`</span>.</span>
<span id="cb47-463"><a href="#cb47-463" aria-hidden="true" tabindex="-1"></a>However, the advantage of using the <span class="in">`PipeOp`</span> is that we could have chained it to a subset of learners to prevent a blow-up of experiments in the benchmark experiment.</span>
<span id="cb47-464"><a href="#cb47-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-465"><a href="#cb47-465" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb47-466"><a href="#cb47-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-467"><a href="#cb47-467" aria-hidden="true" tabindex="-1"></a>In this chapter, we built on everything learned in @sec-pipelines and @sec-pipelines-nonseq to look at concrete usage of pipelines for data preprocessing.</span>
<span id="cb47-468"><a href="#cb47-468" aria-hidden="true" tabindex="-1"></a>We focused primarily on feature engineering, which can make use of <span class="in">`r mlr3pipelines`</span> to automate preprocessing as much as possible while still ensuring user control.</span>
<span id="cb47-469"><a href="#cb47-469" aria-hidden="true" tabindex="-1"></a>We looked at factor encoding for categorical variables, imputing missing data, transforming variables, and feature extraction.</span>
<span id="cb47-470"><a href="#cb47-470" aria-hidden="true" tabindex="-1"></a>Preprocessing is almost always required in machine learning experiments, and applying the <span class="in">`ppl("robustify")`</span> will help in many cases to simplify this process by applying the most common preprocessing steps, we will see this in use in @sec-large-benchmarking.</span>
<span id="cb47-471"><a href="#cb47-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-472"><a href="#cb47-472" aria-hidden="true" tabindex="-1"></a>We have not introduced any new classes in this chapter, so instead @tbl-prepro-api lists the <span class="in">`r ref("PipeOp")`</span>s and <span class="in">`r ref("Graph")`</span>s we discussed.</span>
<span id="cb47-473"><a href="#cb47-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-474"><a href="#cb47-474" aria-hidden="true" tabindex="-1"></a>| PipeOp/Graph | Description |</span>
<span id="cb47-475"><a href="#cb47-475" aria-hidden="true" tabindex="-1"></a>| -- | --- |</span>
<span id="cb47-476"><a href="#cb47-476" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpRemoveConstants")`</span> | Remove variables consisting of one value |</span>
<span id="cb47-477"><a href="#cb47-477" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpCollapseFactors")`</span> | Combine rare factor levels |</span>
<span id="cb47-478"><a href="#cb47-478" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpEncodeImpact")`</span> | Impact encoding |</span>
<span id="cb47-479"><a href="#cb47-479" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpEncode")`</span> | Other factor encoding methods |</span>
<span id="cb47-480"><a href="#cb47-480" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpMissInd")`</span> | Add an indicator column to track missing data |</span>
<span id="cb47-481"><a href="#cb47-481" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpImputeHist")`</span> | Impute missing data by sampling from a histogram |</span>
<span id="cb47-482"><a href="#cb47-482" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("PipeOpImputeOOR")`</span> | Impute missing data with out-of-range values |</span>
<span id="cb47-483"><a href="#cb47-483" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("pipeline_robustify")`</span> | Graph with common imputation and encoding methods |</span>
<span id="cb47-484"><a href="#cb47-484" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("pipeline_targettrafo")`</span> | Graph to transform target during training and invert transformation during prediction |</span>
<span id="cb47-485"><a href="#cb47-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-486"><a href="#cb47-486" aria-hidden="true" tabindex="-1"></a>: <span class="in">`PipeOp`</span>s and <span class="in">`Graph`</span>s discussed in this chapter. {#tbl-prepro-api}</span>
<span id="cb47-487"><a href="#cb47-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-488"><a href="#cb47-488" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb47-489"><a href="#cb47-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-490"><a href="#cb47-490" aria-hidden="true" tabindex="-1"></a>We will consider a prediction problem similar to the one from this chapter, but using the King County Housing regression data instead (available with <span class="in">`tsk("kc_housing")`</span>).</span>
<span id="cb47-491"><a href="#cb47-491" aria-hidden="true" tabindex="-1"></a>To evaluate the models, we again use 10-fold CV, mean absolute error and <span class="in">`lrn("regr.glmnet")`</span>.</span>
<span id="cb47-492"><a href="#cb47-492" aria-hidden="true" tabindex="-1"></a>For now we will ignore the <span class="in">`date`</span> column and simply remove it:</span>
<span id="cb47-493"><a href="#cb47-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-496"><a href="#cb47-496" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb47-497"><a href="#cb47-497" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3data"</span>)</span>
<span id="cb47-498"><a href="#cb47-498" aria-hidden="true" tabindex="-1"></a>kc_housing <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"kc_housing"</span>)</span>
<span id="cb47-499"><a href="#cb47-499" aria-hidden="true" tabindex="-1"></a>kc_housing<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(kc_housing<span class="sc">$</span>feature_names, <span class="st">"date"</span>))</span>
<span id="cb47-500"><a href="#cb47-500" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-501"><a href="#cb47-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-502"><a href="#cb47-502" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Have a look at the features, are there any features which might be problematic? If so, change or remove them.</span>
<span id="cb47-503"><a href="#cb47-503" aria-hidden="true" tabindex="-1"></a>  Check the dataset and learner properties to understand which preprocessing steps you need to do.</span>
<span id="cb47-504"><a href="#cb47-504" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Build a suitable pipeline that allows <span class="in">`glmnet`</span> to be trained on the dataset.</span>
<span id="cb47-505"><a href="#cb47-505" aria-hidden="true" tabindex="-1"></a>  Construct a new <span class="in">`glmnet`</span> model with <span class="in">`ppl("robustify")`</span>.</span>
<span id="cb47-506"><a href="#cb47-506" aria-hidden="true" tabindex="-1"></a>  Compare the two pipelines in a benchmark experiment.</span>
<span id="cb47-507"><a href="#cb47-507" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Now consider the <span class="in">`date`</span> feature:</span>
<span id="cb47-508"><a href="#cb47-508" aria-hidden="true" tabindex="-1"></a>  How can you extract information from this feature in a way that <span class="in">`glmnet`</span> can use?</span>
<span id="cb47-509"><a href="#cb47-509" aria-hidden="true" tabindex="-1"></a>  Does this improve the performance of your pipeline?</span>
<span id="cb47-510"><a href="#cb47-510" aria-hidden="true" tabindex="-1"></a>  Finally, consider the spatial nature of the dataset.</span>
<span id="cb47-511"><a href="#cb47-511" aria-hidden="true" tabindex="-1"></a>  Can you extract an additional feature from the lat / long coordinates?</span>
<span id="cb47-512"><a href="#cb47-512" aria-hidden="true" tabindex="-1"></a>  (Hint: Downtown Seattle has lat/long coordinates <span class="in">`47.605`</span>/<span class="in">`122.334`</span>).</span>
<span id="cb47-513"><a href="#cb47-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-514"><a href="#cb47-514" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb47-515"><a href="#cb47-515" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb47-516"><a href="#cb47-516" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</div>   
    <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>