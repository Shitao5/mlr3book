<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 12&nbsp; Model Interpretation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter13/beyond_regression_and_classification.html" rel="next">
<link href="../../chapters/chapter11/large-scale_benchmarking.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter12/model_interpretation.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-iml" id="toc-sec-iml" class="nav-link active" data-scroll-target="#sec-iml"><span class="header-section-number">12.1</span> The iml Package</a>
  <ul class="collapse">
<li><a href="#sec-feat-importance" id="toc-sec-feat-importance" class="nav-link" data-scroll-target="#sec-feat-importance"><span class="header-section-number">12.1.1</span> Feature Importance</a></li>
  <li><a href="#sec-feature-effects" id="toc-sec-feature-effects" class="nav-link" data-scroll-target="#sec-feature-effects"><span class="header-section-number">12.1.2</span> Feature Effects</a></li>
  <li><a href="#surrogate-models" id="toc-surrogate-models" class="nav-link" data-scroll-target="#surrogate-models"><span class="header-section-number">12.1.3</span> Surrogate Models</a></li>
  <li><a href="#sec-shapley" id="toc-sec-shapley" class="nav-link" data-scroll-target="#sec-shapley"><span class="header-section-number">12.1.4</span> Shapley Values</a></li>
  </ul>
</li>
  <li>
<a href="#sec-counterfactuals" id="toc-sec-counterfactuals" class="nav-link" data-scroll-target="#sec-counterfactuals"><span class="header-section-number">12.2</span> The counterfactuals Package</a>
  <ul class="collapse">
<li><a href="#what-if-method" id="toc-what-if-method" class="nav-link" data-scroll-target="#what-if-method"><span class="header-section-number">12.2.1</span> What-If Method</a></li>
  <li><a href="#moc-method" id="toc-moc-method" class="nav-link" data-scroll-target="#moc-method"><span class="header-section-number">12.2.2</span> MOC Method</a></li>
  </ul>
</li>
  <li>
<a href="#sec-dalex" id="toc-sec-dalex" class="nav-link" data-scroll-target="#sec-dalex"><span class="header-section-number">12.3</span> The <code>DALEX</code> Package</a>
  <ul class="collapse">
<li><a href="#sec-interpretability-dataset-level" id="toc-sec-interpretability-dataset-level" class="nav-link" data-scroll-target="#sec-interpretability-dataset-level"><span class="header-section-number">12.3.1</span> Global EMA</a></li>
  <li><a href="#sec-interpretability-instance-level" id="toc-sec-interpretability-instance-level" class="nav-link" data-scroll-target="#sec-interpretability-instance-level"><span class="header-section-number">12.3.2</span> Local EMA</a></li>
  </ul>
</li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">12.4</span> Conclusions</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">12.5</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">12.6</span> Citation</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter12/model_interpretation.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter12/model_interpretation.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-interpretation" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p><strong>Susanne Dandl</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)</em></p>
<p><strong>Przemysław Biecek</strong> <br><em>MI2.AI, Warsaw University of Technology, and University of Warsaw</em></p>
<p><strong>Giuseppe Casalicchio</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH</em></p>
<p><strong>Marvin N. Wright</strong> <br><em>Leibniz Institute for Prevention Research and Epidemiology – BIPS, and University of Bremen, and University of Copenhagen</em> <br><br></p>
<p>The increasing availability of data and software frameworks to create predictive models has allowed the widespread adoption of ML in many applications. However, high predictive performance of such models often comes at the cost of interpretability. Many models are called a ‘black box’ as the decision-making process behind their predictions is often not immediately interpretable. This lack of explanation can decrease trust in ML and may create barriers to the adoption of predictive models, especially in critical applications such as medicine, engineering, and finance <span class="citation" data-cites="lipton2018mythos">(<a href="../references.html#ref-lipton2018mythos" role="doc-biblioref">Lipton 2018</a>)</span>.</p>
<p>In recent years, many interpretation methods have been developed that allow developers to ‘peek’ inside these models and produce explanations to, for example, understand how features are used by the model to make predictions <span class="citation" data-cites="guidotti2018survey">(<a href="../references.html#ref-guidotti2018survey" role="doc-biblioref">Guidotti et al. 2018</a>)</span>. Interpretation methods can be valuable from multiple perspectives:</p>
<ol type="1">
<li>To gain global insights into a model, for example, to identify which features were the most important overall or how the features act on the predictions.</li>
<li>To improve the model if flaws are identified (in the data or model), for example, if the model depends on one feature unexpectedly.</li>
<li>To understand and control individual predictions, for example, to identify how a given prediction may change if a feature is altered.</li>
<li>To assess algorithmic fairness, for example, to inspect whether the model adversely affects certain subpopulations or individuals (see <a href="../chapter14/algorithmic_fairness.html"><span>Chapter&nbsp;14</span></a>).</li>
</ol>
<div class="page-columns page-full"><p>In this chapter, we will look at model-agnostic (i.e., can be applied to any model) interpretable machine learning (IML) methods that can be used to understand models post hoc (after they have been trained). We will focus on methods implemented in three R packages that nicely interface with <code>mlr3</code>: <a href="https://cran.r-project.org/package=iml"><code>iml</code></a> (<a href="#sec-iml"><span>Section&nbsp;12.1</span></a>), <a href="https://cran.r-project.org/package=counterfactuals"><code>counterfactuals</code></a> (<a href="#sec-counterfactuals"><span>Section&nbsp;12.2</span></a>), and <a href="https://cran.r-project.org/package=DALEX"><code>DALEX</code></a> (<a href="#sec-dalex"><span>Section&nbsp;12.3</span></a>).</p><div class="no-row-height column-margin column-container"><span class="">Interpretable Machine Learning</span></div></div>
<p><code>iml</code> and <code>DALEX</code> offer similar functionality but differ in design choices in that <code>iml</code> makes use of the <code>R6</code> class system whereas <code>DALEX</code> is based on the S3 class system. <code>counterfactuals</code> also uses the <code>R6</code> class system. In contrast to <code>iml</code> and <code>counterfactuals</code>, <code>DALEX</code> focuses on comparing multiple predictive models, usually of different types. We will only provide a brief overview of the methodology discussed below, we recommend <span class="citation" data-cites="Molnar2022">Molnar (<a href="../references.html#ref-Molnar2022" role="doc-biblioref">2022</a>)</span> as a comprehensive introductory book about IML.</p>
<p>As a running example throughout this chapter, we will consider a gradient boosting machine (GBM) fit on half the features in the <code>"german_credit"</code> task. In practice, we would tune the hyperparameters of GBM as discussed in <a href="../chapter4/hyperparameter_optimization.html"><span>Chapter&nbsp;4</span></a> and perform feature selection as discussed in <a href="../chapter6/feature_selection.html"><span>Chapter&nbsp;6</span></a> to select the most relevant features. However, for the sake of simplicity, we utilize an untuned GBM in these examples as it exhibited satisfactory performance even without fine-tuning.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-003_deb63a23c9b8af27da34dbebb5da9ea8">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3verse.mlr-org.com">mlr3verse</a></span><span class="op">)</span></span>
<span><span class="va">tsk_german</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"german_credit"</span><span class="op">)</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span></span>
<span>  cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"duration"</span>, <span class="st">"amount"</span>, <span class="st">"age"</span>, <span class="st">"status"</span>, <span class="st">"savings"</span>, <span class="st">"purpose"</span>,</span>
<span>  <span class="st">"credit_history"</span>, <span class="st">"property"</span>, <span class="st">"employment_duration"</span>, <span class="st">"other_debtors"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">split</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/partition.html">partition</a></span><span class="op">(</span><span class="va">tsk_german</span><span class="op">)</span></span>
<span><span class="va">lrn_gbm</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.gbm"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span><span class="va">lrn_gbm</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_german</span>, row_ids <span class="op">=</span> <span class="va">split</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Performance-based Interpretation Methods Require Test Data
</div>
</div>
<div class="callout-body-container callout-body">
<p>Performance-based interpretation methods such as permutation feature importance (<a href="#sec-feat-importance"><span>Section&nbsp;12.1.1</span></a>) rely on measuring the generalization performance. Hence, they should be computed on an independent test set to decrease bias in estimation (see <a href="../chapter3/evaluation_and_benchmarking.html"><span>Chapter&nbsp;3</span></a>).</p>
<p>However, the differences in interpretation between training and test data are less pronounced <span class="citation" data-cites="Molnar2022pitfalls">(<a href="../references.html#ref-Molnar2022pitfalls" role="doc-biblioref">Molnar et al. 2022</a>)</span> in prediction-based methods that do not require performance estimation such as ICE/PD (<a href="#sec-feature-effects"><span>Section&nbsp;12.1.2</span></a>) or Shapley values (<a href="#sec-shapley"><span>Section&nbsp;12.1.4</span></a>).</p>
</div>
</div>
<section id="sec-iml" class="level2 page-columns page-full" data-number="12.1"><h2 data-number="12.1" class="anchored" data-anchor-id="sec-iml">
<span class="header-section-number">12.1</span> The iml Package</h2>
<p><a href="https://cran.r-project.org/package=iml"><code>iml</code></a> <span class="citation" data-cites="Molnar2018">(<a href="../references.html#ref-Molnar2018" role="doc-biblioref">Molnar, Bischl, and Casalicchio 2018</a>)</span> implements a unified interface for a variety of model-agnostic interpretation methods that facilitate the analysis and interpretation of machine learning models. <code>iml</code> supports machine learning models (for classification or regression) fitted by <em>any</em> R package, and in particular all <code>mlr3</code> models are supported by wrapping learners in an <a href="https://www.rdocumentation.org/packages/iml/topics/Predictor"><code>Predictor</code></a> object, which unifies the input-output behavior of the trained models. This object contains the prediction model as well as the data used for analyzing the model and producing the desired explanation. We construct the <code>Predictor</code> object using our trained learner and heldout test data:</p>
<div class="cell" data-hash="model_interpretation_cache/html/iml-Predictor_c0f33990643cb555a7fc875061b6b0b4">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://christophm.github.io/iml/">iml</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># features in test data</span></span>
<span><span class="va">credit_x</span> <span class="op">=</span> <span class="va">tsk_german</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">split</span><span class="op">$</span><span class="va">test</span>,</span>
<span>  cols <span class="op">=</span> <span class="va">tsk_german</span><span class="op">$</span><span class="va">feature_names</span><span class="op">)</span></span>
<span><span class="co"># target in test data</span></span>
<span><span class="va">credit_y</span> <span class="op">=</span> <span class="va">tsk_german</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">split</span><span class="op">$</span><span class="va">test</span>,</span>
<span>  cols <span class="op">=</span> <span class="va">tsk_german</span><span class="op">$</span><span class="va">target_names</span><span class="op">)</span></span>
<span></span>
<span><span class="va">predictor</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/Predictor.html">Predictor</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lrn_gbm</span>, data <span class="op">=</span> <span class="va">credit_x</span>, y <span class="op">=</span> <span class="va">credit_y</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With our <code>Predictor</code> setup we can now consider different model interpretation methods.</p>
<section id="sec-feat-importance" class="level3 page-columns page-full" data-number="12.1.1"><h3 data-number="12.1.1" class="anchored" data-anchor-id="sec-feat-importance">
<span class="header-section-number">12.1.1</span> Feature Importance</h3>
<p>When deploying a model in practice, it is often of interest to know which features contribute the most to the <em>predictive performance</em> of the model. This can be useful to better understand the problem at hand and the relationship between features and target. In model development, this can be used to filter features (<a href="../chapter6/feature_selection.html#sec-fs-filter"><span>Section&nbsp;6.1</span></a>) that do not contribute a lot to the model’s predictive ability. In this book, we use the term ‘feature importance’ to describe global methods that calculate a single score per feature that reflect the importance regarding a given quantity of interest, e.g., model performance, thus allowing features to be ranked.</p>
<div class="page-columns page-full"><p>One of the most popular feature importance methods is the permutation feature importance (PFI), originally introduced by <span class="citation" data-cites="breiman2001random">Breiman (<a href="../references.html#ref-breiman2001random" role="doc-biblioref">2001a</a>)</span> for random forests and adapted by <span class="citation" data-cites="Fisher2019pfi">Fisher, Rudin, and Dominici (<a href="../references.html#ref-Fisher2019pfi" role="doc-biblioref">2019</a>)</span> as a model-agnostic feature importance measure (originally termed, ‘model reliance’). Feature permutation is the process of randomly shuffling observed values for a single feature in a dataset. This removes the original dependency structure of the feature with the target variable and with all other features while maintaining the marginal distribution of the feature. The PFI measures the change in the model performance before (original model performance) and after (permuted model performance) permuting a feature. If a feature is not important, then there will be little change in model performance after permuting that feature. Conversely, we would expect a clear decrease in model performance if the feature is more important. It is generally recommended to repeat the permutation process and aggregate performance changes over multiple repetitions to decrease randomness in results.</p><div class="no-row-height column-margin column-container"><span class="">Permutation Feature Importance</span></div></div>
<p>PFI is run in <code>iml</code> by constructing an object of class <a href="https://www.rdocumentation.org/packages/iml/topics/FeatureImp"><code>FeatureImp</code></a> and specifying the performance measure, below we use classification error. By default, the permutation is repeated five times to keep computation time low (this can be changed with <code>n.repetitions</code> when calling the constructor <code>$new()</code>, below we set <code>n.repetitions = 100</code>) and in each repetition, the importance value corresponding to the change in the classification error is calculated. The <code>$plot()</code> method shows the median of the five resulting importance values (as a point) and the boundaries of the error bars in the plot refer to the 5% and 95% quantiles of the importance values (<a href="#fig-iml-pfi">Figure&nbsp;<span>12.1</span></a>).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Increase the Number of Repetitions to Obtain Useful Error Bars
</div>
</div>
<div class="callout-body-container callout-body">
<p>The default number of repetitions when constructing a <code>FeatureImp</code> object is <code>5</code>. However, the number of repetitions should be increased if you want to obtain useful error bars from the resulting plot.</p>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">importance</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/FeatureImp.html">FeatureImp</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, loss <span class="op">=</span> <span class="st">"ce"</span>, n.repetitions <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">importance</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/fig-iml-pfi_2d71aa39b9e0b0e84dd4b0dccef71206">
<div class="cell-output-display">
<div id="fig-iml-pfi" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-iml-pfi-1.png" class="img-fluid figure-img" style="width:100.0%" alt="x-axis says 'Feature Importance (loss: ce)' and y-axis lists the features in the data. Plot shows 10 error bars, one for each feature, with solid black circles in the middle (the median importance value across the repetitions) and horizontal black lines on each row (from the 5% to 95% quantile of the feature importance values). Top three most important features are `status`, `duration`, and `savings`."></p>
<figcaption class="figure-caption">Figure&nbsp;12.1: Permutation feature importance (PFI). Points indicate the median and bars the 5% and 95% quantiles of the PFI over five repetitions of the permutation process.</figcaption></figure>
</div>
</div>
</div>
<p>The plot automatically ranks features from most (largest median performance change) to least (smallest median performance change) important. In <a href="#fig-iml-pfi">Figure&nbsp;<span>12.1</span></a>, the feature <code>status</code> is most important, if we permute the <code>status</code> column in the data the classification error of our model increases by a factor of around 1.08. By default, <code>FeatureImp</code> calculates the <em>ratio</em> of the model performance before and after permutation as an importance value; the <em>difference</em> of the performance measures can be returned by passing <code>compare = "difference"</code> when calling <code>$new()</code>.</p>
</section><section id="sec-feature-effects" class="level3 page-columns page-full" data-number="12.1.2"><h3 data-number="12.1.2" class="anchored" data-anchor-id="sec-feature-effects">
<span class="header-section-number">12.1.2</span> Feature Effects</h3>
<p>Feature effect methods describe how or to what extent a feature contributes towards the <em>model predictions</em> by analyzing how the predictions change when changing a feature. These methods can be distinguished between local and global feature effect methods. Global feature effect methods refer to how a prediction changes <em>on average</em> when a feature is changed. In contrast, local feature effect methods address the question of how a <em>single</em> prediction of a given observation changes when a feature value is changed. To a certain extent, local feature effect methods can reveal interactions in the model that become visible when the local effects are heterogeneous, i.e., if changes in the local effect are different across the observations.</p>
<div class="page-columns page-full"><p>Partial dependence (PD) plots <span class="citation" data-cites="Friedman2001pdp">(<a href="../references.html#ref-Friedman2001pdp" role="doc-biblioref">Friedman 2001</a>)</span> can be used to visualize global feature effects by visualizing how model predictions change on average when varying the values of a given feature of interest.</p><div class="no-row-height column-margin column-container"><span class="">Partial Dependence</span></div></div>
<div class="page-columns page-full"><p>Individual conditional expectation (ICE) curves <span class="citation" data-cites="Goldstein2015ice">(<a href="../references.html#ref-Goldstein2015ice" role="doc-biblioref">Goldstein et al. 2015</a>)</span> (a.k.a. Ceteris Paribus Effects) are a local feature effects method that display how the prediction of a <em>single</em> observation changes when varying a feature of interest, while all other features stay constant. <span class="citation" data-cites="Goldstein2015ice">Goldstein et al. (<a href="../references.html#ref-Goldstein2015ice" role="doc-biblioref">2015</a>)</span> demonstrated that the PD plot is the average of ICE curves. ICE curves are constructed by taking a single observation and feature of interest, and then replacing the feature’s value with another value and plotting the new prediction, this is then repeated for many feature values (e.g., across an equidistant grid of the feature’s value range). The x-axis of an ICE curve visualizes the set of replacement feature values and the y-axis is the model prediction. Each ICE curve is a local explanation that assesses the feature effect of a single observation on the model prediction. An ICE plot contains one ICE curve (line) per observation. If the ICE curves are heterogeneous, i.e., not parallel, then the model may have estimated an interaction involving the considered feature.</p><div class="no-row-height column-margin column-container"><span class="">Individual Conditional Expectation</span></div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feature Effects Can Be Non-Linear
</div>
</div>
<div class="callout-body-container callout-body">
<p>Feature effects are very similar to regression coefficients, <span class="math inline">\(\beta\)</span>, in linear models which offer interpretations such as “if you increase this feature by one unit, your prediction increases on average by <span class="math inline">\(\beta\)</span> if all other features stay constant”. However, feature effects are not limited to linear effects and can be applied to any type of predictive model.</p>
</div>
</div>
<p>Let us put this into practice by considering how the feature <code>amount</code> influences the predictions in our subsetted credit classification task. Below we initialize an object of class <a href="https://www.rdocumentation.org/packages/iml/topics/FeatureEffect"><code>FeatureEffect</code></a> by passing the feature name of interest and the feature effect method, we use <code>"pdp+ice"</code> to indicate that we want to visualize ICE curves with a PD plot (average of the ICE curves). We recommend always plotting PD and ICE curves together as PD plots on their own could mask heterogeneous effects. We use <code>$plot()</code> to visualize the results (<a href="#fig-iml-pdice">Figure&nbsp;<span>12.2</span></a>).</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-iml-pdice_45e38ec776a4feca05b04e957bd85e94">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">effect</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/FeatureEffect.html">FeatureEffect</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, feature <span class="op">=</span> <span class="st">"amount"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"pdp+ice"</span><span class="op">)</span></span>
<span><span class="va">effect</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-iml-pdice" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-iml-pdice-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Two plots are visualized side-by-side. The x-axis for both says 'amount' and ranges from 0 to around 16000. The y-axis for both says 'Predicted credit_risk' and ranges from 0 to 1. The left plot is captioned 'good' and shows many thin black curves that are roughly parallel and slowly decrease from 0-10000 and then are roughly flat until the end of the plot. The right plot is captioned 'bad' and shows many thin black curves that are roughly parallel and slowly increase from 0-10000 and then are roughly flat until the end of the plot."></p>
<figcaption class="figure-caption">Figure&nbsp;12.2: Partial dependence (PD) plot (yellow) and individual conditional expectation (ICE) curves (black) that show how the credit amount affects the predicted credit risk.</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-iml-pdice">Figure&nbsp;<span>12.2</span></a> shows that if the <code>amount</code> is smaller than roughly 10,000 then on average there is a high chance that the predicted creditworthiness will be <code>good</code>. Furthermore, the ICE curves are roughly parallel, meaning that there do not seem to be strong interactions present where <code>amount</code> is involved.</p>
</section><section id="surrogate-models" class="level3" data-number="12.1.3"><h3 data-number="12.1.3" class="anchored" data-anchor-id="surrogate-models">
<span class="header-section-number">12.1.3</span> Surrogate Models</h3>
<p>Interpretable models such as decision trees or linear models can be used as surrogate models to approximate or mimic an, often very complex, black box model. Inspecting the surrogate model can provide insights into the behavior of a black box model, for example by looking at the model coefficients in a linear regression or splits in a decision tree. We differentiate between local surrogate models, which approximate a model locally around a specific data point of interest, and global surrogate models which approximate the model across the entire input space <span class="citation" data-cites="Ribeiro2016lime Molnar2022">(<a href="../references.html#ref-Ribeiro2016lime" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>; <a href="../references.html#ref-Molnar2022" role="doc-biblioref">Molnar 2022</a>)</span>.</p>
<p>The features used to train a surrogate model are usually the same features used to train the black box model or at least data with the same distribution to ensure a representative input space. However, the target used to train the surrogate model is the predictions obtained from the black box model, not the real outcome of the underlying data. Hence, conclusions drawn from the surrogate model are only valid if the surrogate model approximates the black box model very well (i.e., if the model fidelity is high). It is therefore also important to measure and report the approximation error of the surrogate model.</p>
<p>The data used to train the black box model may be very complex or limited, making it challenging to directly train a well-performing interpretable model on that data. Instead, we can use the black box model to generate new labeled data in specific regions of the input space with which we can augment the original data. The augmented data can then be used to train an interpretable model that captures and explains the relationships learned by the black box model (in specific regions) or to identify flaws or unexpected behavior.</p>
<section id="global-surrogate-model" class="level4" data-number="12.1.3.1"><h4 data-number="12.1.3.1" class="anchored" data-anchor-id="global-surrogate-model">
<span class="header-section-number">12.1.3.1</span> Global Surrogate Model</h4>
<p>Initializing the <a href="https://www.rdocumentation.org/packages/iml/topics/TreeSurrogate"><code>TreeSurrogate</code></a> class fits a conditional inference tree (<a href="https://www.rdocumentation.org/packages/partykit/topics/ctree"><code>ctree()</code></a>) surrogate model to the predictions from our trained model. This class extracts the decision rules created by the tree surrogate and the <code>$plot()</code> method visualizes the distribution of the predicted outcomes from each terminal node. Below, we pass <code>maxdepth = 2</code> to the constructor to build a tree with two binary splits, yielding four terminal nodes.</p>
<div class="cell" data-hash="model_interpretation_cache/html/iml-globalsurrogate_b1dc28e1988abd84e8bada72d7a3c1c4">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tree_surrogate</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/TreeSurrogate.html">TreeSurrogate</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, maxdepth <span class="op">=</span> <span class="fl">2L</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before inspecting this model, we need to first check if the surrogate model approximates the prediction model accurately, which we can assess by comparing the predictions of the tree surrogate and the predictions of the black box model. For example, we could quantify the number of matching predictions and measure the accuracy of the surrogate in predicting the predictions of the black box GBM model:</p>
<div class="cell" data-hash="model_interpretation_cache/html/iml-crosstable_d9fbc63e9921e0186980a411e18c57ac">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred_surrogate</span> <span class="op">=</span> <span class="va">tree_surrogate</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">credit_x</span>, type <span class="op">=</span> <span class="st">"class"</span><span class="op">)</span><span class="op">$</span><span class="va">.class</span></span>
<span><span class="va">pred_surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">pred_surrogate</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"good"</span>, <span class="st">"bad"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pred_gbm</span> <span class="op">=</span> <span class="va">lrn_gbm</span><span class="op">$</span><span class="fu">predict_newdata</span><span class="op">(</span><span class="va">credit_x</span><span class="op">)</span><span class="op">$</span><span class="va">response</span></span>
<span><span class="va">confusion</span> <span class="op">=</span> <span class="fu">mlr3measures</span><span class="fu">::</span><span class="fu"><a href="https://mlr3measures.mlr-org.com/reference/confusion_matrix.html">confusion_matrix</a></span><span class="op">(</span><span class="va">pred_surrogate</span>, <span class="va">pred_gbm</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"good"</span><span class="op">)</span></span>
<span><span class="va">confusion</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  269   4
    bad    38  19
acc :  0.8727; ce  :  0.1273; dor :  33.6250; f1  :  0.9276 
fdr :  0.0147; fnr :  0.1238; fomr:  0.6667; fpr :  0.1739 
mcc :  0.4731; npv :  0.3333; ppv :  0.9853; tnr :  0.8261 
tpr :  0.8762 </code></pre>
</div>
</div>
<p>This shows an accuracy of around 87% in predictions from the surrogate compared to the black box model, which is good enough for us to use our surrogate for further interpretation, for example by plotting the splits in the terminal node:</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-iml-surro_4b9bbdc69ff84affaaf91b87f04a45c7">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tree_surrogate</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-iml-surro" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-iml-surro-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Four barplots with 'count' on the y-axis and '.class' on the x-axis. Top left shows 150 'good' credit predictions and around 1 'bad' prediction. Top right shows around 10 'good' predictions and 1 'bad' one. Bottom left shows around 120 'good' predictions and 40 'bad' ones. Bottom right shows about 23 'bad' predictions and around 5 'good' ones."></p>
<figcaption class="figure-caption">Figure&nbsp;12.3: Distribution of the predicted outcomes for each terminal node identified by the tree surrogate. The top two nodes consist of applications with a positive balance in the account (<code>status</code>is either <code>"0 &lt;= ... &lt; 200 DM"</code>, <code>"... &gt;= 200 DM"</code> or <code>"salary for at least 1 year"</code>) and either a duration of less or equal than 42 months (top left), or more than 42 months (top right). The bottom nodes contain applicants that either have no checking account or a negative balance (<code>status</code>) and either a duration of less than or equal to 36 months (bottom left) or more than 36 months (bottom right).</figcaption></figure>
</div>
</div>
</div>
<p>Or we could access the trained tree surrogate via the <code>$tree</code> field of the <code>TreeSurrogate</code> object and then have access to all methods in <a href="https://cran.r-project.org/package=partykit"><code>partykit</code></a>:</p>
<div class="cell" data-hash="model_interpretation_cache/html/iml-globalsurrogate-tree_f2596f7ace68c6ceae2ddba0de850c9c">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">partykit</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/partykit/man/party-methods.html">print.party</a></span><span class="op">(</span><span class="va">tree_surrogate</span><span class="op">$</span><span class="va">tree</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] root
|   [2] status in no checking account, ... &lt; 0 DM
|   |   [3] duration &lt;= 36: *
|   |   [4] duration &gt; 36: *
|   [5] status in 0&lt;= ... &lt; 200 DM, ... &gt;= 200 DM / salary for at least 1 year
|   |   [6] duration &lt;= 42: *
|   |   [7] duration &gt; 42: *</code></pre>
</div>
</div>
<!-- Since the surrogate model only uses the predictions of the black box model (here, the GBM model) and not the real outcomes of the underlying data, the conclusions drawn from the surrogate model do not apply generally, but only to the black box model (if the approximation of the surrogate model is accurate enough). -->
</section><section id="local-surrogate-model" class="level4" data-number="12.1.3.2"><h4 data-number="12.1.3.2" class="anchored" data-anchor-id="local-surrogate-model">
<span class="header-section-number">12.1.3.2</span> Local Surrogate Model</h4>
<p>In general, it can be very difficult to accurately approximate the black box model with an interpretable surrogate in the entire feature space. Therefore, local surrogate models focus on a small area in the feature space surrounding a point of interest. Local surrogate models are constructed as follows:</p>
<ol type="1">
<li>Obtain predictions from the black box model for a given dataset.</li>
<li>Weight the observations in this dataset by their proximity to our point of interest.</li>
<li>Fit an interpretable, surrogate model on the weighted dataset using the predictions of the black box model as the target.</li>
<li>Explain the prediction of our point of interest with the surrogate model.</li>
</ol>
<p>To illustrate this, we will select a random data point to explain. As we are dealing with people, we will name our observation “Charlie” and first look at the black box predictions:</p>
<div class="cell" data-asis="results" data-hash="model_interpretation_cache/html/Charlie_4e7d80501525591dcbb7193db8bba586">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Charlie</span> <span class="op">=</span> <span class="va">credit_x</span><span class="op">[</span><span class="fl">35</span>, <span class="op">]</span></span>
<span><span class="va">gbm_predict</span> <span class="op">=</span> <span class="va">predictor</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">Charlie</span><span class="op">)</span></span>
<span><span class="va">gbm_predict</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    good    bad
1 0.6346 0.3654</code></pre>
</div>
</div>
<p>We can see that the model predicts the class ‘good’ with 63.5% probability, so now we can use <a href="https://www.rdocumentation.org/packages/iml/topics/LocalModel"><code>LocalModel</code></a> to find out why this prediction was made. The underlying surrogate model is a locally weighted L1-penalized linear regression model such that only a pre-defined number of features per class, <code>k</code> (default is <code>3</code>), will have a non-zero coefficient and as such are the <code>k</code> most influential features, below we set <code>k = 2</code>. We can also set the parameter <code>gower.power</code> which specifies the size of the neighborhood for the local model (default is <code>gower.power = 1</code>), the smaller the value, the more the model will focus on points closer to the point of interest, below we set <code>gower.power = 0.1</code>. This implementation is very closely related to Local Interpretable Model-agnostic Explanations (LIME) <span class="citation" data-cites="Ribeiro2016lime">(<a href="../references.html#ref-Ribeiro2016lime" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>, the differences are outlined in the documentation of <code><a href="https://christophm.github.io/iml/reference/LocalModel.html">iml::LocalModel</a></code>.</p>
<div class="cell" data-hash="model_interpretation_cache/html/iml-local_surrogate_7973798255b9da8835cda3b59cecab02">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">predictor</span><span class="op">$</span><span class="va">class</span> <span class="op">=</span> <span class="st">"good"</span> <span class="co"># explain the 'good' class</span></span>
<span><span class="va">local_surrogate</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/LocalModel.html">LocalModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, <span class="va">Charlie</span>, gower.power <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If the prediction of the local model and the prediction of the black box GBM model greatly differ, then you might want to experiment with changing the <code>k</code> and <code>gower.power</code> parameters. These parameters can be considered as hyperparameters of the local surrogate model, which should be tuned to obtain an accurate local surrogate. First, we check if the predictions for Charlie match:</p>
<div class="cell" data-hash="model_interpretation_cache/html/unnamed-chunk-3_e829fab86b912f20cd490cf38764cbe6">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>gbm <span class="op">=</span> <span class="va">gbm_predict</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, local <span class="op">=</span> <span class="va">local_surrogate</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   gbm  local 
0.6346 0.6539 </code></pre>
</div>
</div>
<p>Ideally, we should assess the fidelity of the surrogate model in the local neighborhood of Charlie, i.e., how well the local surrogate model approximates the predictions of the black box GBM model for multiple data points in the vicinity of Charlie. A practical approach to assess this local model fidelity involves generating artificial data points within Charlie’s local neighborhood (and potentially applying distance-based weighting) or selecting the <span class="math inline">\(k\)</span> nearest neighbors from the original data. For illustration purposes, we now quantify the approximation error using the mean absolute error calculated from the 10 nearest neighbors (including Charlie) according to the Gower distance <span class="citation" data-cites="gower1971general">(<a href="../references.html#ref-gower1971general" role="doc-biblioref">Gower 1971</a>)</span>:</p>
<div class="cell" data-hash="model_interpretation_cache/html/unnamed-chunk-4_c05925e76648413ae5d794df9f6e7daa">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ind_10nn</span> <span class="op">=</span> <span class="fu">gower</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gower/man/gower_topn.html">gower_topn</a></span><span class="op">(</span><span class="va">Charlie</span>, <span class="va">credit_x</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">$</span><span class="va">index</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">Charlie_10nn</span> <span class="op">=</span> <span class="va">credit_x</span><span class="op">[</span><span class="va">ind_10nn</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">gbm_pred_10nn</span> <span class="op">=</span> <span class="va">predictor</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">Charlie_10nn</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">local_pred_10nn</span> <span class="op">=</span> <span class="va">local_surrogate</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">Charlie_10nn</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">gbm_pred_10nn</span> <span class="op">-</span> <span class="va">local_pred_10nn</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05475</code></pre>
</div>
</div>
<p>As we see good agreement between the local and black box model (on average, the predictions of both the local surrogate and the black box model for Charlie’s 10 nearest neighbors differ only by 0.055), we can move on to look at the most influential features for Charlie’s predictions:</p>
<div class="cell" data-hash="model_interpretation_cache/html/unnamed-chunk-5_17e761bcb0d4f0dc0a93b8cfa5c3414b">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">local_surrogate</span><span class="op">$</span><span class="va">results</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"feature.value"</span>, <span class="st">"effect"</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/unnamed-chunk-6_6f9b74dfca9889d7c96d26b4fbe5a9dc">
<div class="cell-output cell-output-stdout">
<pre><code>               feature.value   effect
1                duration=12 -0.02000
2 status=no checking account -0.08544</code></pre>
</div>
</div>
<p>In this case, ‘duration’ and ‘status’ were most important and both have a negative effect on the prediction of Charlie.</p>
</section></section><section id="sec-shapley" class="level3" data-number="12.1.4"><h3 data-number="12.1.4" class="anchored" data-anchor-id="sec-shapley">
<span class="header-section-number">12.1.4</span> Shapley Values</h3>
<p>Shapley values were originally developed in the context of cooperative game theory to study how the payout of a game can be fairly distributed among the players that form a team. This concept has been adapted for use in ML as a local interpretation method to explain the contributions of each input feature to the final model prediction of a single observation <span class="citation" data-cites="Trumbelj2013Shapley">(<a href="../references.html#ref-Trumbelj2013Shapley" role="doc-biblioref">Štrumbelj and Kononenko 2013</a>)</span>. Hence, the ‘players’ are the features, and the ‘payout’, which should be fairly distributed among features, refers to the difference between the individual observation’s prediction and the mean prediction.</p>
<p>Shapley values estimate how much each input feature contributed to the final prediction for a single observation (after subtracting the mean prediction). By assigning a value to each feature, we can gain insights into which features were the most important ones for the considered observation. Compared to the penalized linear model as a local surrogate model, Shapley values guarantee that the prediction is fairly distributed among the features as they also inherently consider interactions between features when calculating the contribution of each feature.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Correctly Interpreting Shapley Values
</div>
</div>
<div class="callout-body-container callout-body">
<p>Shapley values are frequently <strong>misinterpreted</strong> as the difference between the predicted value after removing the feature from model training. The Shapley value of a feature is calculated by considering all possible subsets of features and computing the difference in the model prediction with and without the feature of interest included. Hence, it refers to the average marginal contribution of a feature to the difference between the actual prediction and the mean prediction, given the current set of features.</p>
</div>
</div>
<p>Shapley values can be calculated by passing the <code>Predictor</code> and the observation of interest to the constructor of <a href="https://www.rdocumentation.org/packages/iml/topics/Shapley"><code>Shapley</code></a>. The exact computation of Shapley values is time consuming, as it involves taking into account all possible combinations of features to calculate the marginal contribution of a feature. Therefore, the estimation of Shapley values is often approximated. The <code>sample.size</code> argument (default is <code>sample.size = 100</code>) can be increased to obtain a more accurate approximation of exact Shapley values.</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-iml-shapley_cba9f8a23c7c0d7a0888f496da278447">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">shapley</span> <span class="op">=</span> <span class="va"><a href="https://christophm.github.io/iml/reference/Shapley.html">Shapley</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, x.interest <span class="op">=</span> <span class="va">Charlie</span>,</span>
<span>  sample.size <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">shapley</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-iml-shapley" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-iml-shapley-1.png" class="img-fluid figure-img" style="width:100.0%" alt="10 bar plots of Shapley values, one for each feature. x-axis says 'phi' and ranges from -0.1 to 0.05. The strongest positive contributions are from the `duration`, `purpose` and `property` variables. The strongest negative contributions are `status`, `amount`, and `savings`."></p>
<figcaption class="figure-caption">Figure&nbsp;12.4: Shapley values for Charlie. The actual prediction (0.63) displays the prediction of the model for the observation we are interested in, the average prediction (0.71) displays the average prediction over the given test dataset. Each horizontal bar is the Shapley value (phi) for the given feature.</figcaption></figure>
</div>
</div>
</div>
<p>In <a href="#fig-iml-shapley">Figure&nbsp;<span>12.4</span></a>, the Shapley values (<code>phi</code>) of the features show us how to fairly distribute the difference of Charlie’s probability of being creditworthy to the dataset’s average probability among the given features. The approximation is sufficiently good if all Shapley values (<code>phi</code>) sum up to the difference of the actual prediction and the average prediction. Here, we used <code>sample.size = 1000</code> leading to sufficiently good prediction difference of -0.079 between the actual prediction of Charlie (0.635) and the average prediction (0.706). The ‘purpose’ variable has the most positive effect on the probability of being creditworthy, with an increase in the predicted probability of around 5%. In contrast, the ‘status’ variable leads to a decrease in the predicted probability of over 10%.</p>
</section></section><section id="sec-counterfactuals" class="level2 page-columns page-full" data-number="12.2"><h2 data-number="12.2" class="anchored" data-anchor-id="sec-counterfactuals">
<span class="header-section-number">12.2</span> The counterfactuals Package</h2>
<p>Counterfactual explanations try to identify the smallest possible changes to the input features of a given observation that would lead to a different prediction <span class="citation" data-cites="Wachter2017">(<a href="../references.html#ref-Wachter2017" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>. In other words, a counterfactual explanation provides an answer to the question: “What changes in the current feature values are necessary to achieve a different prediction?”.</p>
<p>Counterfactual explanations can have many applications in different areas such as healthcare, finance, and criminal justice, where it may be important to understand how small changes in input features could affect the model’s prediction. For example, a counterfactual explanation could be used to suggest lifestyle changes to a patient to reduce their risk of developing a particular disease, or to suggest actions that would increase the chance of a credit being approved. For our <code>tsk("german_credit")</code> example, we might consider what changes in features would turn a ‘bad’ credit prediction into a ‘good’ one (<a href="#fig-counterfactuals-ill">Figure&nbsp;<span>12.5</span></a>).</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-counterfactuals-ill_71b34c52bae343222892e0bcbce16f65">
<div class="cell-output-display">
<div id="fig-counterfactuals-ill" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/counterfactuals.png" class="img-fluid figure-img" style="width:50.0%" alt="Figure shows a rectangle where bottom left triangle is light blue and labeled 'good' and top right triangle is brown and labeled 'bad'. There is a dot in the 'bad' area and a dot in the 'good' area and an arrow pointing from the 'bad dot' to the 'good dot'. The x-axis is labeled 'duration' and the y-axis is labeled 'amount'."></p>
<figcaption class="figure-caption">Figure&nbsp;12.5: Illustration of a counterfactual explanation. The real observation (blue, right dot) is predicted to have ‘bad’ credit. The brown (left) dot is one possible counterfactual that would result in a ‘good’ credit prediction.</figcaption></figure>
</div>
</div>
</div>
<div class="page-columns page-full"><p>A simple counterfactual method is the What-If approach <span class="citation" data-cites="Wexler2019">(<a href="../references.html#ref-Wexler2019" role="doc-biblioref">Wexler et al. 2019</a>)</span> where, for a given prediction to explain, the counterfactual is the closest data point in the dataset with the desired prediction. Usually, many possible counterfactual data points can exist. However, the approach by <span class="citation" data-cites="Wexler2019">Wexler et al. (<a href="../references.html#ref-Wexler2019" role="doc-biblioref">2019</a>)</span>, and several other early counterfactual methods (see <span class="citation" data-cites="guidotti2022counterfactual">Guidotti (<a href="../references.html#ref-guidotti2022counterfactual" role="doc-biblioref">2022</a>)</span> for a comprehensive overview), only produce a single, somewhat arbitrary counterfactual explanation, which can be regarded as problematic when counterfactuals are used for insights or actions against the model.</p><div class="no-row-height column-margin column-container"><span class="">What-If</span></div></div>
<div class="page-columns page-full"><p>In contrast, the multi-objective counterfactuals method (MOC) <span class="citation" data-cites="Dandl2020">(<a href="../references.html#ref-Dandl2020" role="doc-biblioref">Dandl et al. 2020</a>)</span> generates multiple artificially-generated counterfactuals that may not be equal to observations in a given dataset. The generation of counterfactuals is based on an optimization problem that aims for counterfactuals that:</p><div class="no-row-height column-margin column-container"><span class="">Multi-objective Counterfactuals</span></div></div>
<ol type="1">
<li>Have the desired prediction;</li>
<li>Are close to the observation of interest;</li>
<li>Only require changes in a few features; and</li>
<li>Originate from the same distribution as the observations in the given dataset.</li>
</ol>
<p>In MOC, all four objectives are optimized simultaneously via a multi-objective optimization method. Several other counterfactual methods rely on single-objective optimization methods, where multiple objectives are combined into a single objective, e.g., using a weighted sum. However, a single-objective approach raises concerns about the appropriate weighting of objectives and is unable to account for inherent trade-offs among individual objectives. Moreover, it may restrict the solution set of the counterfactural search to a single candidate. MOC returns a set of non-dominated and, therefore equally good, counterfactuals with respect to the four objectives (similarly to the Pareto front we saw in <a href="../chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning"><span>Section&nbsp;5.2</span></a>).</p>
<p>Counterfactual explanations are available in the <code>counterfactuals</code> package, which depends on <a href="https://www.rdocumentation.org/packages/iml/topics/Predictor"><code>Predictor</code></a> objects as inputs.</p>
<section id="what-if-method" class="level3" data-number="12.2.1"><h3 data-number="12.2.1" class="anchored" data-anchor-id="what-if-method">
<span class="header-section-number">12.2.1</span> What-If Method</h3>
<p>Continuing our previous example, we saw that the GBM model classifies Charlie as having good credit with a predicted probability of 63.5%. We can use the What-If method to understand how the features need to change for this predicted probability to increase to 75%. We initialize a <a href="https://www.rdocumentation.org/packages/counterfactuals/topics/WhatIfClassif"><code>WhatIfClassif</code></a> object with our <code>Predictor</code> and state that we only want to find one counterfactual (<code>n_counterfactuals = 1L</code>), increasing <code>n_counterfactuals</code> would return the specified number of counterfactuals closest to the point of interest. The <code>$find_counterfactuals()</code> method generates a counterfactual of class <a href="https://www.rdocumentation.org/packages/counterfactuals/topics/Counterfactuals"><code>Counterfactuals</code></a>, below we set our desired predicted probability to be between <code>0.75</code> and <code>1</code> (<code>desired_prob = c(0.75, 1)</code>). The <code>$evaluate(show_diff = TRUE)</code> method tells us how features need to be changed to generate our desired class.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-whatif_32eab66c4a85e09c36b511e0fa83c3c3">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dandls/counterfactuals">counterfactuals</a></span><span class="op">)</span></span>
<span><span class="va">whatif</span> <span class="op">=</span> <span class="va"><a href="https://rdrr.io/pkg/counterfactuals/man/WhatIfClassif.html">WhatIfClassif</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, n_counterfactuals <span class="op">=</span> <span class="fl">1L</span><span class="op">)</span></span>
<span><span class="va">cfe</span> <span class="op">=</span> <span class="va">whatif</span><span class="op">$</span><span class="fu">find_counterfactuals</span><span class="op">(</span><span class="va">Charlie</span>,</span>
<span>  desired_class <span class="op">=</span> <span class="st">"good"</span>, desired_prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.75</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">cfe</span><span class="op">$</span><span class="fu">evaluate</span><span class="op">(</span>show_diff <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  age amount credit_history duration employment_duration other_debtors
1  -3   1417           &lt;NA&gt;       -3                &lt;NA&gt;          &lt;NA&gt;
  property purpose savings     status dist_x_interest no_changed
1     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt; ... &lt; 0 DM          0.1176          4
  dist_train dist_target minimality
1          0           0          1</code></pre>
</div>
</div>
<p>Here we can see that, to achieve a predicted probability of at least 75% for good credit, Charlie would have to be three years younger, the duration of credit would have to be reduced by three months, the amount would have to be increased by 1417 DM and the status would have to be ‘… &lt; 0 DM’ (instead of ‘no checking account’) .</p>
</section><section id="moc-method" class="level3" data-number="12.2.2"><h3 data-number="12.2.2" class="anchored" data-anchor-id="moc-method">
<span class="header-section-number">12.2.2</span> MOC Method</h3>
<p>Calling the MOC method is similar to the What-If method but with a <a href="https://www.rdocumentation.org/packages/counterfactuals/topics/MOCClassif"><code>MOCClassif()</code></a> object. We set the <code>epsilon</code> parameter to 0 to penalize counterfactuals in the optimization process with predictions outside the desired range. With MOC, we can also prohibit changes in specific features via the <code>fixed_features</code> argument, below we restrict changes in the ‘age’ variable. For illustrative purposes, we only run the multi-objective optimizer for 30 generations.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-mocmulti_f2e3fd5374e5245f8c8a14f9067f6268">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">moc</span> <span class="op">=</span> <span class="va"><a href="https://rdrr.io/pkg/counterfactuals/man/MOCClassif.html">MOCClassif</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, epsilon <span class="op">=</span> <span class="fl">0</span>, n_generations <span class="op">=</span> <span class="fl">30L</span>,</span>
<span>  fixed_features <span class="op">=</span> <span class="st">"age"</span><span class="op">)</span></span>
<span><span class="va">cfe_multi</span> <span class="op">=</span> <span class="va">moc</span><span class="op">$</span><span class="fu">find_counterfactuals</span><span class="op">(</span><span class="va">Charlie</span>,</span>
<span>  desired_class <span class="op">=</span> <span class="st">"good"</span>, desired_prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.75</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The multi-objective approach does not guarantee that all counterfactuals have the desired prediction so we use <code>$subset_to_valid()</code> to restrict counterfactuals to those we are interested in:</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-mocmulti-subset_59af39f1b73822d41fd13d6821e8a853">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cfe_multi</span><span class="op">$</span><span class="fu">subset_to_valid</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">cfe_multi</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6 Counterfactual(s) 
 
Desired class: good 
Desired predicted probability range: [0.75, 1] 
 
Head: 
   age amount                              credit_history duration
1:  40    701 no credits taken/all credits paid back duly       12
2:  40    701 no credits taken/all credits paid back duly       12
3:  40    701 no credits taken/all credits paid back duly       12
6 variables not shown: [employment_duration, other_debtors, property, purpose, savings, status]</code></pre>
</div>
</div>
<p>This method generated 6 counterfactuals but as these are artificially generated they are not necessarily equal to actual observations in the underlying dataset. For a concise overview of the required feature changes, we can use the <code>plot_freq_of_feature_changes()</code> method, which visualizes the frequency of feature changes across all returned counterfactuals.</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-cf-mocfreq_623d357cb94419b54184ec910de83408">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cfe_multi</span><span class="op">$</span><span class="fu">plot_freq_of_feature_changes</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cf-mocfreq" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-cf-mocfreq-1.png" class="img-fluid figure-img" style="width:100.0%" alt="x-axis says 'relative frequency' and ranges from 0 to just over 0.3. Changed features were 'status' (in 35% of the counterfactuals), 'savings' (35%), 'purpose' (10%), 'employment_duration' (10%), 'duration' (10%), and 'amount' (10%)."></p>
<figcaption class="figure-caption">Figure&nbsp;12.6: Barplots of the relative frequency of feature changes of the counterfactuals found by MOC.</figcaption></figure>
</div>
</div>
</div>
<p>We can see that ‘status’ and ‘savings’ were changed most frequently in the counterfactuals. To see <em>how</em> the features were changed, we can visualize the counterfactuals for two features on a two-dimensional ICE plot.</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-cf-mocsurface_351b87962f4482508537d017b9b81c24">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cfe_multi</span><span class="op">$</span><span class="fu">plot_surface</span><span class="op">(</span>feature_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"status"</span>, <span class="st">"savings"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">15</span>, hjust <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-cf-mocsurface" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-cf-mocsurface-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Surface plot that is primarily light blue when status is positive and dark blue when status is negative. y-axis is the 'savings' variable and x-axis is the 'status' variable. There is a white dot in the bottom left corner at (status = 'no checking account', savings = unknown/no savings account'). Two black dots are in a straight line above the white dot and two black dots are in a roughly straight line to the right of the white dot."></p>
<figcaption class="figure-caption">Figure&nbsp;12.7: Two-dimensional surface plot for the ‘status’ and ‘savings’ variables, higher predictions are lighter. The colors and contour lines indicate the predicted value of the model when ‘status’ and ‘savings’ differ while all other features are set to the true (Charlie’s) values. The white point displays the true prediction (Charlie), and the black points are the counterfactuals that only propose changes in the two features.</figcaption></figure>
</div>
</div>
</div>
</section></section><section id="sec-dalex" class="level2 page-columns page-full" data-number="12.3"><h2 data-number="12.3" class="anchored" data-anchor-id="sec-dalex">
<span class="header-section-number">12.3</span> The <code>DALEX</code> Package</h2>
<p><a href="https://cran.r-project.org/package=DALEX"><code>DALEX</code></a> <span class="citation" data-cites="Biecek2018">(<a href="../references.html#ref-Biecek2018" role="doc-biblioref">Biecek 2018</a>)</span> implements a similar set of methods as <code>iml</code>, but the architecture of <code>DALEX</code> is oriented towards model comparison. The logic behind working with this package assumes that the process of exploring models is iterative, and in successive iterations, we want to compare different perspectives, including perspectives presented/learned by different models. This logic is commonly referred to as the Rashomon perspective, first described in <span class="citation" data-cites="Breiman2001">Breiman (<a href="../references.html#ref-Breiman2001" role="doc-biblioref">2001b</a>)</span> and more extensively developed and formalized as interactive explanatory model analysis <span class="citation" data-cites="Baniecki2023">(<a href="../references.html#ref-Baniecki2023" role="doc-biblioref">Baniecki, Parzych, and Biecek 2023</a>)</span>.</p>
<p>You can use the <code>DALEX</code> package with any classification and regression model built with <code>mlr3</code> as well as with other frameworks in R. As we have already explored the methodology behind most of the methods discussed in this section, we will just focus on the implementations of these methods in <code>DALEX</code> using the <code>tsk("german_credit")</code> running example.</p>
<p>Once you become familiar with the philosophy of working with the <code>DALEX</code> package, you can use other packages from this family such as <a href="https://cran.r-project.org/package=fairmodels"><code>fairmodels</code></a> <span class="citation" data-cites="Wisniewski2022">(<a href="../references.html#ref-Wisniewski2022" role="doc-biblioref">Wiśniewski and Biecek 2022</a>)</span> for detection and mitigation of biases, <a href="https://cran.r-project.org/package=modelStudio"><code>modelStudio</code></a> <span class="citation" data-cites="Baniecki2019">(<a href="../references.html#ref-Baniecki2019" role="doc-biblioref">Baniecki and Biecek 2019</a>)</span> for interactive model exploration, <a href="https://cran.r-project.org/package=modelDown"><code>modelDown</code></a> <span class="citation" data-cites="Romaszko2019">(<a href="../references.html#ref-Romaszko2019" role="doc-biblioref">Romaszko et al. 2019</a>)</span> for the automatic generation of IML model documentation, <a href="https://cran.r-project.org/package=survex"><code>survex</code></a> <span class="citation" data-cites="Krzyzinski2023">(<a href="../references.html#ref-Krzyzinski2023" role="doc-biblioref">Krzyziński et al. 2023</a>)</span> for the explanation of survival models, or <a href="https://cran.r-project.org/package=treeshap"><code>treeshap</code></a> for the analysis of tree-based models.</p>
<p>The analysis of a model is usually an interactive process starting with evaluating a model based on one or more performance metrics, known as a ‘shallow analysis’. In a series of subsequent steps, one can systematically deepen understanding of the model by exploring the importance of single variables or pairs of variables to an in-depth analysis of the relationship between selected variables to the model outcome. See <span class="citation" data-cites="Bucker2022">Bücker et al. (<a href="../references.html#ref-Bucker2022" role="doc-biblioref">2022</a>)</span> for a broader discussion of what the model exploration process looks like.</p>
<div class="page-columns page-full"><p>This explanatory model analysis (EMA) process can focus on a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we refer to global model analysis. <a href="#fig-dalex-fig-plot-01">Figure&nbsp;<span>12.8</span></a> visualizes an overview of the key functions in these two scenarios that we will discuss in this section. An in-depth description of this methodology can be found in <span class="citation" data-cites="biecek_burzykowski_2021">Biecek and Burzykowski (<a href="../references.html#ref-biecek_burzykowski_2021" role="doc-biblioref">2021</a>)</span>.</p><div class="no-row-height column-margin column-container"><span class="">Explanatory Model Analysis</span></div></div>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-fig-plot-01_526244790cbec0bf3861f4bc4b9e197f">
<div class="cell-output-display">
<div id="fig-dalex-fig-plot-01" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/DALEX_ema_process.png" class="img-fluid figure-img" style="width:92.0%" alt="Title says 'Explanatory Model Analysis', just below that in code font says 'DALEX::explain()'. Far left side is an arrow pointing upwards labeled 'Shallow' and one pointing down labeled 'Deep'. To the right of these arrows is the text 'Global Analysis' with an arrow pointing down to 'Model Performance, AUC, RMSE; DALEX::model_performance()', which has an arrow pointing down to 'Feature Importance, VIP; DALEX::model_parts()', which has an arrow pointing down to 'Feature Profiles, PD, ALE; DALEX::model_profile()'. To the right of 'Global Analysis' is the text 'Local Analysis', which has an arrow pointing to 'Model Predict; DALEX::predict()', which has an arrow pointing down to 'Feature Attributions, SHAP, BD; DALEX::predict_parts()', which has an arrow pointing down to 'Feature Profiles, Ceteris Paribus; DALEX::predict_profile()'."></p>
<figcaption class="figure-caption">Figure&nbsp;12.8: Taxonomy of methods for model exploration presented in this section. The left side shows global analysis methods and the right shows local analysis methods. Methods increase in analysis complexity from top to bottom.</figcaption></figure>
</div>
</div>
</div>
<p>As with <code>iml</code>, <code>DALEX</code> also implements a wrapper that enables a unified interface to its functionality. For models created with the <code>mlr3</code> package, we would use <a href="https://www.rdocumentation.org/packages/DALEXtra/topics/explain_mlr3"><code>explain_mlr3()</code></a>, which creates an S3 <code>explainer</code> object, which is a list containing at least: the model object, the dataset that will be used for calculation of explanations, the predict function, the function that calculates residuals, name/label of the model name and other additional information about the model.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-019_7e576121e89a47b2d4f404c1433ba60d">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://modeloriented.github.io/DALEX/">DALEX</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ModelOriented.github.io/DALEXtra/">DALEXtra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">gbm_exp</span> <span class="op">=</span> <span class="fu">DALEXtra</span><span class="fu">::</span><span class="fu"><a href="https://ModelOriented.github.io/DALEXtra/reference/explain_mlr3.html">explain_mlr3</a></span><span class="op">(</span><span class="va">lrn_gbm</span>,</span>
<span>  data <span class="op">=</span> <span class="va">credit_x</span>,</span>
<span>  y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">credit_y</span><span class="op">$</span><span class="va">credit_risk</span> <span class="op">==</span> <span class="st">"bad"</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="st">"GBM Credit"</span>,</span>
<span>  colorize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gbm_exp</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/unnamed-chunk-8_11074fa3a34d5b403b80431cfb57a2d1">
<div class="cell-output cell-output-stdout">
<pre><code>Model label:  GBM Credit 
Model class:  LearnerClassifGBM,LearnerClassif,Learner,R6 
Data head  :
  age amount                          credit_history duration
1  67   1169 all credits at this bank paid back duly        6
2  49   2096 all credits at this bank paid back duly       12
  employment_duration other_debtors              property
1            &gt;= 7 yrs          none unknown / no property
2    4 &lt;= ... &lt; 7 yrs          none unknown / no property
              purpose                    savings
1 furniture/equipment             ... &gt;= 1000 DM
2             repairs unknown/no savings account
                                      status
1                        no checking account
2 ... &gt;= 200 DM / salary for at least 1 year</code></pre>
</div>
</div>
<section id="sec-interpretability-dataset-level" class="level3" data-number="12.3.1"><h3 data-number="12.3.1" class="anchored" data-anchor-id="sec-interpretability-dataset-level">
<span class="header-section-number">12.3.1</span> Global EMA</h3>
<p>Global EMA aims to understand how a model behaves on average for a set of observations. In <code>DALEX</code>, functions for global level analysis are prefixed with <code>model_</code>.</p>
<p>The model exploration process starts (<a href="#fig-dalex-fig-plot-01">Figure&nbsp;<span>12.8</span></a>) by evaluating the performance of a model. <a href="https://www.rdocumentation.org/packages/DALEX/topics/model_performance"><code>model_performance()</code></a> detects the task type and selects the most appropriate measure, as we are using binary classification the function automatically suggests recall, precision, F1-score, accuracy, and AUC; similarly the default plotting method is selected based on the task type, below ROC is selected.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-020a_92dc9a1285599cadd31869146cab9985">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">perf_credit</span> <span class="op">=</span> <span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/model_performance.html">model_performance</a></span><span class="op">(</span><span class="va">gbm_exp</span><span class="op">)</span></span>
<span><span class="va">perf_credit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Measures for:  classification
recall     : 0.3535 
precision  : 0.614 
f1         : 0.4487 
accuracy   : 0.7394 
auc        : 0.7689

Residuals:
      0%      10%      20%      30%      40%      50%      60%      70% 
-0.88117 -0.44188 -0.31691 -0.20743 -0.14601 -0.10782 -0.07089  0.03232 
     80%      90%     100% 
 0.49779  0.65661  0.94458 </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">old_theme</span> <span class="op">=</span> <span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/theme_dalex.html">set_theme_dalex</a></span><span class="op">(</span><span class="st">"ema"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">perf_credit</span>, geom <span class="op">=</span> <span class="st">"roc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-roc_aaf1938d69b596a2202d3157af347b7e">
<div class="cell-output-display">
<div id="fig-dalex-roc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-roc-1.png" class="img-fluid figure-img" style="width:60.0%" alt="ROC curve with 'True positive rate' on the y-axis and 'False positive rate' on the x-axis, curve shows reasonably good model fit as it sits comfortably in the upper left diagonal."></p>
<figcaption class="figure-caption">Figure&nbsp;12.9: Graphical summary of model performance using the Receiver Operator Curve (<a href="../chapter3/evaluation_and_benchmarking.html#sec-roc"><span>Section&nbsp;3.4</span></a>).</figcaption></figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visual Summaries
</div>
</div>
<div class="callout-body-container callout-body">
<p>Various visual summaries may be selected with the <code>geom</code> parameter. For the credit risk task, the LIFT curve is a popular graphical summary.</p>
</div>
</div>
<p>Feature importance methods can be calculated with <a href="https://www.rdocumentation.org/packages/DALEX/topics/model_parts"><code>model_parts()</code></a> and then plotted.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-021_ecd614c383a78c8b7e34f6d395865983">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gbm_effect</span> <span class="op">=</span> <span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/model_parts.html">model_parts</a></span><span class="op">(</span><span class="va">gbm_exp</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">gbm_effect</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             variable mean_dropout_loss      label
1        _full_model_            0.2311 GBM Credit
2       other_debtors            0.2351 GBM Credit
3              amount            0.2351 GBM Credit
4            property            0.2353 GBM Credit
5                 age            0.2355 GBM Credit
6 employment_duration            0.2403 GBM Credit</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gbm_effect</span>, show_boxplots <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-featimp_73db69a9956a36efe43c3d43f917d517">
<div class="cell-output-display">
<div id="fig-dalex-featimp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-featimp-1.png" class="img-fluid figure-img" style="width:90.0%" alt="Feature importance plot. x-axis label is 'One minus AUC loss after permutations', y-axis labels are features. Horizontal bars range from 0.24 to 0.35."></p>
<figcaption class="figure-caption">Figure&nbsp;12.10: Graphical summary of permutation importance of features. The longer the bar, the larger the change in the loss function after permutation of the particular feature and therefore the more important the feature. This plot shows that ‘status’ is the most important feature and ‘other_debtors’ is the least important.</figcaption></figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Calculating Importance
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>type</code> argument in the <code>model_parts</code> function allows you to specify how the importance of the features is to be calculated, by the difference of the loss functions (<code>type = "difference"</code>), by the quotient (<code>type = "ratio"</code>), or without any transformation (<code>type = "raw"</code>).</p>
</div>
</div>
<p>Feature effects can be calculated with <a href="https://www.rdocumentation.org/packages/DALEX/topics/model_profile"><code>model_profile()</code></a> and by default are plotted as PD plots.</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-024_6128bd283eb07f9340af35c6422111d2">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">gbm_profiles</span> <span class="op">=</span> <span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/model_profile.html">model_profile</a></span><span class="op">(</span><span class="va">gbm_exp</span><span class="op">)</span></span>
<span><span class="va">gbm_profiles</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top profiles    : 
   _vname_    _label_ _x_ _yhat_ _ids_
1 duration GBM Credit   4 0.2052     0
2 duration GBM Credit   6 0.2052     0
3 duration GBM Credit   7 0.2052     0
4 duration GBM Credit   8 0.2052     0
5 duration GBM Credit   9 0.2246     0
6 duration GBM Credit  10 0.2246     0</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gbm_profiles</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"top"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggtitle</span><span class="op">(</span><span class="st">"Partial Dependence for GBM Credit model"</span>,<span class="st">""</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-pdp_d2f46b2202c4de49610b123fda7450ee">
<div class="cell-output-display">
<div id="fig-dalex-pdp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-pdp-1.png" class="img-fluid figure-img" style="width:90.0%" alt="Left plot is PD plot of 'age' against 'average prediction', between ages 20-40 the prediction dips from 0.35 to 0.3 then is flat. Middle plot is PD plot of 'amount', between amounts 0-5000 the prediction starts at 0.3 then spikes briefly then returns to 0.3, then between 5000-15000 the plot slowly increases to 0.5. Right plot is PD plot of 'duration', between duration 0-40 the prediction linearly increases from 0.2 to 0.45 then stays flat."></p>
<figcaption class="figure-caption">Figure&nbsp;12.11: Graphical summary of the model’s partial dependence profile for three selected variables (age, amount, duration).</figcaption></figure>
</div>
</div>
</div>
<p>From <a href="#fig-dalex-pdp">Figure&nbsp;<span>12.11</span></a>, we can see that the GBM model has learned a non-monotonic relationship for the feature <code>amount</code>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Marginal and Accumulated Local Profiles
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>type</code> argument of the <a href="https://www.rdocumentation.org/packages/DALEX/topics/model_profile"><code>model_profile()</code></a> function also allows <em>marginal profiles</em> (with <code>type = "conditional"</code>) and <em>accumulated local profiles</em> (with <code>type = "accumulated"</code>) to be calculated.</p>
</div>
</div>
</section><section id="sec-interpretability-instance-level" class="level3" data-number="12.3.2"><h3 data-number="12.3.2" class="anchored" data-anchor-id="sec-interpretability-instance-level">
<span class="header-section-number">12.3.2</span> Local EMA</h3>
<p>Local EMA aims to understand how a model behaves for a single observation. In <code>DALEX</code>, functions for local analysis are prefixed with <code>predict_</code>. We will carry out the following examples using Charlie again.</p>
<p>Local analysis starts with the calculation of a model prediction (<a href="#fig-dalex-fig-plot-01">Figure&nbsp;<span>12.8</span></a>).</p>
<div class="cell" data-hash="model_interpretation_cache/html/interpretation-025_d6486740975f43c6aaeac85f41b183bc">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">gbm_exp</span>, <span class="va">Charlie</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bad 
0.3654 </code></pre>
</div>
</div>
<p>As a next step, we might consider break-down plots, which decompose the model’s prediction into contributions that can be attributed to different explanatory variables (see the <em>Break-down Plots for Additive Attributions</em> chapter in <span class="citation" data-cites="biecek_burzykowski_2021">Biecek and Burzykowski (<a href="../references.html#ref-biecek_burzykowski_2021" role="doc-biblioref">2021</a>)</span> for more on this method). These are calculated with <a href="https://www.rdocumentation.org/packages/DALEX/topics/predict_parts"><code>predict_parts()</code></a>:</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-breakdown_0fea18423c24b27177cfe18dade429ee">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/predict_parts.html">predict_parts</a></span><span class="op">(</span><span class="va">gbm_exp</span>, new_observation <span class="op">=</span> <span class="va">Charlie</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dalex-breakdown" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-breakdown-1.png" class="img-fluid figure-img" style="width:90.0%" alt="On the x-axis are numbers from 0.2 to 0.5, and y-axis is variables from the dataset. There are four bars in red with negative number labels and five bars in green with positive number labels. A dashed vertical lines runs through x=0.3 and there is a violet bar with text '0.365'."></p>
<figcaption class="figure-caption">Figure&nbsp;12.12: Graphical summary of local attributions of features calculated by the break-down method. Positive attributions are shown in green and negative attributions in red. The violet bar corresponds to the model prediction for the explained observation and the dashed line corresponds to the average model prediction.</figcaption></figure>
</div>
</div>
</div>
<p>Looking at <a href="#fig-dalex-breakdown">Figure&nbsp;<span>12.12</span></a>, we can read that the biggest contributors to the final prediction for Charlie were the features <code>status</code> and <code>savings</code>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Selected Order of Features
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>order</code> argument allows you to indicate the selected order of the features. This is a useful option when the features have some relative conditional importance (e.g.&nbsp;pregnancy and sex).</p>
</div>
</div>
<p>The <code><a href="https://modeloriented.github.io/DALEX/reference/predict_parts.html">predict_parts()</a></code> function can also be used to plot Shapley values with the SHAP algorithm <span class="citation" data-cites="Lundberg2019">(<a href="../references.html#ref-Lundberg2019" role="doc-biblioref">Lundberg, Erion, and Lee 2019</a>)</span> by setting <code>type = "shap"</code>:</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-shaps_3e1653da9d9eac2a1235f255834f0b4e">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/predict_parts.html">predict_parts</a></span><span class="op">(</span><span class="va">gbm_exp</span>, new_observation <span class="op">=</span> <span class="va">Charlie</span>, type <span class="op">=</span> <span class="st">"shap"</span><span class="op">)</span>,</span>
<span>  show_boxplots <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dalex-shaps" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-shaps-1.png" class="img-fluid figure-img" style="width:90.0%" alt="x-axis says 'contribution' and ranges from -0.05 to 0.1, y-axis is feature names. Plots show four red bars with negative contributions and five green bars making positive contributions. Longest bar is for 'status' and shortest for 'other_debtors'."></p>
<figcaption class="figure-caption">Figure&nbsp;12.13: Graphical summary of local attributions of features calculated by the Shap method. Positive attributions are shown in green and negative attributions in red. The most important feature here is the ‘status’ variable and least is ‘other_debtors’.</figcaption></figure>
</div>
</div>
</div>
<p>The results for Break Down and SHAP methods are generally similar. Differences will emerge if there are many complex interactions in the model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Speeding Up Shapley Computation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Shapley values can take a long time to compute. This process can be sped up at the expense of accuracy. The parameters <code>B</code> and <code>N</code> can be used to tune this trade-off, where <code>N</code> is the number of observations on which conditional expectation values are estimated (500 by default) and <code>B</code> is the number of random paths used to calculate Shapley values (25 by default).</p>
</div>
</div>
<p>Finally, we can plot ICE curves using <a href="https://www.rdocumentation.org/packages/DALEX/topics/predict_profile"><code>predict_profile()</code></a>:</p>
<div class="cell" data-hash="model_interpretation_cache/html/fig-dalex-ice_ee66adcf68e58483e162e3405ca19906">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/predict_profile.html">predict_profile</a></span><span class="op">(</span><span class="va">gbm_exp</span>,  <span class="va">credit_x</span><span class="op">[</span><span class="fl">30</span><span class="op">:</span><span class="fl">40</span>, <span class="op">]</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-dalex-ice" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="model_interpretation_files/figure-html/fig-dalex-ice-1.png" class="img-fluid figure-img" style="width:90.0%" alt="Plots have the same pattern as the previous PD plots but with 10 lines plotted in parallel."></p>
<figcaption class="figure-caption">Figure&nbsp;12.14: Individual conditional explanations (aka Ceteris Paribus) plots for 10 rows in the credit data (including Charlie) for three selected variables (age, amount, duration).</figcaption></figure>
</div>
</div>
</div>
</section></section><section id="conclusions" class="level2" data-number="12.4"><h2 data-number="12.4" class="anchored" data-anchor-id="conclusions">
<span class="header-section-number">12.4</span> Conclusions</h2>
<p>In this chapter, we learned how to gain post hoc insights into a model trained with <code>mlr3</code> by using the most popular approaches from the field of interpretable machine learning. The methods are all model-agnostic and so do not depend on specific model classes. <a href="https://cran.r-project.org/package=iml"><code>iml</code></a> and <a href="https://cran.r-project.org/package=DALEX"><code>DALEX</code></a> offer a wide range of (partly) overlapping methods, while <a href="https://cran.r-project.org/package=counterfactuals"><code>counterfactuals</code></a> focuses solely on counterfactual methods. We demonstrated on <code>tsk("german_credit")</code> how these packages offer an in-depth analysis of a GBM model fitted with <code>mlr3</code>. As we conclude the chapter we will highlight some limitations in the methods discussed above to help guide your own post hoc analyses.</p>
<section id="correlated-features" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="correlated-features">Correlated Features</h4>
<p>If features are correlated, the insights from the interpretation methods should be treated with caution. Changing the feature values of an observation without taking the correlation with other features into account leads to unrealistic combinations of the feature values. Since such feature combinations are also unlikely to be part of the training data, the model will likely extrapolate in these areas <span class="citation" data-cites="Molnar2022pitfalls Hooker2019PleaseSP">(<a href="../references.html#ref-Molnar2022pitfalls" role="doc-biblioref">Molnar et al. 2022</a>; <a href="../references.html#ref-Hooker2019PleaseSP" role="doc-biblioref">Hooker and Mentch 2019</a>)</span>. This distorts the interpretation of methods that are based on changing single feature values such as PFI, PD plots, and Shapley values. Alternative methods can help in these cases: conditional feature importance instead of PFI <span class="citation" data-cites="Strobl2008 Watson2021">(<a href="../references.html#ref-Strobl2008" role="doc-biblioref">Strobl et al. 2008</a>; <a href="../references.html#ref-Watson2021" role="doc-biblioref">Watson and Wright 2021</a>)</span>, accumulated local effect plots instead of PD plots <span class="citation" data-cites="Apley2020">(<a href="../references.html#ref-Apley2020" role="doc-biblioref">Apley and Zhu 2020</a>)</span>, and the KernelSHAP method instead of Shapley values <span class="citation" data-cites="Lundberg2019">(<a href="../references.html#ref-Lundberg2019" role="doc-biblioref">Lundberg, Erion, and Lee 2019</a>)</span>.</p>
</section><section id="rashomon-effect" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="rashomon-effect">Rashomon Effect</h4>
<p>Explanations derived from an interpretation method can be ambiguous. A method can deliver multiple equally plausible but potentially contradicting explanations. This phenomenon is also called the Rashomon effect <span class="citation" data-cites="Breiman2001">(<a href="../references.html#ref-Breiman2001" role="doc-biblioref">Breiman 2001b</a>)</span>. This effect can be due to changes in hyperparameters, the underlying dataset, or even the initial seed <span class="citation" data-cites="Molnar2022pitfalls">(<a href="../references.html#ref-Molnar2022pitfalls" role="doc-biblioref">Molnar et al. 2022</a>)</span>.</p>
</section><section id="high-dimensional-data" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="high-dimensional-data">High-Dimensional Data</h4>
<p><code>tsk("german_credit")</code> is low-dimensional with a limited number of observations. Applying interpretation methods off-the-shelf to higher dimensional datasets is often not feasible due to the enormous computational costs and so recent methods, such as Shapley values that use kernel-based estimators, have been developed to help over come this. Another challenge is that the high-dimensional IML output generated for high-dimensional datasets can overwhelm users. If the features can be meaningfully grouped, grouped versions of methods, e.g.&nbsp;the grouped feature importance proposed by <span class="citation" data-cites="Au2022">Au et al. (<a href="../references.html#ref-Au2022" role="doc-biblioref">2022</a>)</span>, can be applied.</p>
<div id="tbl-interpretation-api" class="anchored">
<table class="table">
<caption>Table&nbsp;12.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).</caption>
<thead><tr class="header">
<th>Class</th>
<th>Constructor/Function</th>
<th>Fields/Methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/iml/topics/Predictor"><code>Predictor</code></a></td>
<td><code>$new()</code></td>
<td>-</td>
</tr>
<tr class="even">
<td><a href="https://www.rdocumentation.org/packages/iml/topics/FeatureImp"><code>FeatureImp</code></a></td>
<td><code>$new(some_predictor)</code></td>
<td><code>$plot()</code></td>
</tr>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/iml/topics/FeatureEffect"><code>FeatureEffect</code></a></td>
<td><code>$new(some_predictor)</code></td>
<td><code>$plot()</code></td>
</tr>
<tr class="even">
<td><a href="https://www.rdocumentation.org/packages/iml/topics/LocalModel"><code>LocalModel</code></a></td>
<td><code>$new(some_predictor, some_x)</code></td>
<td><code>$results()</code></td>
</tr>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/iml/topics/Shapley"><code>Shapley</code></a></td>
<td><code>$new(some_predictor, x.interest)</code></td>
<td><code>$plot()</code></td>
</tr>
<tr class="even">
<td><a href="https://www.rdocumentation.org/packages/counterfactuals/topics/WhatIfClassif"><code>WhatIfClassif</code></a></td>
<td><code>$new(some_predictor)</code></td>
<td><code>$find_counterfactuals()</code></td>
</tr>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/counterfactuals/topics/MOCClassif"><code>MOCClassif</code></a></td>
<td><code>$new(some_predictor)</code></td>
<td><code>$find_counterfactuals()</code></td>
</tr>
<tr class="even">
<td><a href="https://www.rdocumentation.org/packages/DALEX/topics/explainer"><code>explainer</code></a></td>
<td><a href="https://www.rdocumentation.org/packages/DALEXtra/topics/explain_mlr3"><code>explain_mlr3()</code></a></td>
<td>&nbsp;&nbsp;&nbsp; <code><a href="https://modeloriented.github.io/DALEX/reference/model_parts.html">model_parts()</a></code>; <code><a href="https://modeloriented.github.io/DALEX/reference/model_performance.html">model_performance()</a></code>; <code><a href="https://modeloriented.github.io/DALEX/reference/predict_parts.html">predict_parts()</a></code>
</td>
</tr>
</tbody>
</table>
</div>
</section></section><section id="exercises" class="level2" data-number="12.5"><h2 data-number="12.5" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">12.5</span> Exercises</h2>
<p>The following exercises are based on predictions of the value of soccer players based on their characteristics in the FIFA video game series. They use the 2020 <code>fifa</code> data available in DALEX. Solve them with either <code>iml</code> or <code>DALEX</code>.</p>
<ol type="1">
<li>Prepare an <code>mlr3</code> regression task for the <code>fifa</code> data. Select only features describing the age and skills of soccer players. Train a predictive model of your own choice on this task, to predict the value of a soccer player.</li>
<li>Use the permutation importance method to calculate feature importance ranking. Which feature is the most important? Do you find the results surprising?</li>
<li>Use the partial dependence plot/profile to draw the global behavior of the model for this feature. Is it aligned with your expectations?</li>
<li>Choose Manuel Neuer as a specific example and calculate and plot the Shapley values. Which feature is locally the most important and has the strongest influence on his valuation as a soccer player? Calculate the ceteris paribus profiles / individual conditional expectation curves to visualize the local behavior of the model for this feature. Is it different from the global behavior?</li>
</ol></section><section id="citation" class="level2" data-number="12.6"><h2 data-number="12.6" class="anchored" data-anchor-id="citation">
<span class="header-section-number">12.6</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Dandl S, Biecek P, Casalicchio G, Wright MN. (2024). Model Interpretation. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/model_interpretation.html.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Apley2020" class="csl-entry" role="listitem">
Apley, Daniel W., and Jingyu Zhu. 2020. <span>“Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 82 (4): 1059–86. <a href="https://doi.org/10.1111/rssb.12377">https://doi.org/10.1111/rssb.12377</a>.
</div>
<div id="ref-Au2022" class="csl-entry" role="listitem">
Au, Quay, Julia Herbinger, Clemens Stachl, Bernd Bischl, and Giuseppe Casalicchio. 2022. <span>“Grouped Feature Importance and Combined Features Effect Plot.”</span> <em>Data Mining and Knowledge Discovery</em> 36 (4): 1401–50. <a href="https://doi.org/10.1007/s10618-022-00840-5">https://doi.org/10.1007/s10618-022-00840-5</a>.
</div>
<div id="ref-Baniecki2019" class="csl-entry" role="listitem">
Baniecki, Hubert, and Przemyslaw Biecek. 2019. <span>“<span class="nocase">modelStudio</span>: Interactive Studio with Explanations for ML Predictive Models.”</span> <em>Journal of Open Source Software</em> 4 (43): 1798. <a href="https://doi.org/10.21105/joss.01798">https://doi.org/10.21105/joss.01798</a>.
</div>
<div id="ref-Baniecki2023" class="csl-entry" role="listitem">
Baniecki, Hubert, Dariusz Parzych, and Przemyslaw Biecek. 2023. <span>“The Grammar of Interactive Explanatory Model Analysis.”</span> <em>Data Mining and Knowledge Discovery</em>, 1573–756X. <a href="https://doi.org/10.1007/s10618-023-00924-w">https://doi.org/10.1007/s10618-023-00924-w</a>.
</div>
<div id="ref-Biecek2018" class="csl-entry" role="listitem">
Biecek, Przemyslaw. 2018. <span>“<span>DALEX</span>: Explainers for Complex Predictive Models in <span>R</span>.”</span> <em>Journal of Machine Learning Research</em> 19 (84): 1–5. <a href="https://jmlr.org/papers/v19/18-416.html">https://jmlr.org/papers/v19/18-416.html</a>.
</div>
<div id="ref-biecek_burzykowski_2021" class="csl-entry" role="listitem">
Biecek, Przemyslaw, and Tomasz Burzykowski. 2021. <em>Explanatory Model Analysis</em>. Chapman; Hall/CRC, New York. <a href="https://ema.drwhy.ai/">https://ema.drwhy.ai/</a>.
</div>
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
Breiman, Leo. 2001a. <span>“Random Forests.”</span> <em>Machine Learning</em> 45: 5–32. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>.
</div>
<div id="ref-Breiman2001" class="csl-entry" role="listitem">
———. 2001b. <span>“Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).”</span> <em>Statistical Science</em> 16 (3). <a href="https://doi.org/10.1214/ss/1009213726">https://doi.org/10.1214/ss/1009213726</a>.
</div>
<div id="ref-Bucker2022" class="csl-entry" role="listitem">
Bücker, Michael, Gero Szepannek, Alicja Gosiewska, and Przemyslaw Biecek. 2022. <span>“Transparency, Auditability, and Explainability of Machine Learning Models in Credit Scoring.”</span> <em>Journal of the Operational Research Society</em> 73 (1): 70–90. <a href="https://doi.org/10.1080/01605682.2021.1922098">https://doi.org/10.1080/01605682.2021.1922098</a>.
</div>
<div id="ref-Dandl2020" class="csl-entry" role="listitem">
Dandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020. <span>“Multi-Objective Counterfactual Explanations.”</span> In <em>Parallel Problem Solving from Nature <span>PPSN</span> <span>XVI</span></em>, 448–69. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-58112-1_31">https://doi.org/10.1007/978-3-030-58112-1_31</a>.
</div>
<div id="ref-Fisher2019pfi" class="csl-entry" role="listitem">
Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2019. <span>“All Models Are Wrong, but Many Are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously.”</span> <a href="https://doi.org/10.48550/arxiv.1801.01489">https://doi.org/10.48550/arxiv.1801.01489</a>.
</div>
<div id="ref-Friedman2001pdp" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>The Annals of Statistics</em> 29 (5). <a href="https://doi.org/10.1214/aos/1013203451">https://doi.org/10.1214/aos/1013203451</a>.
</div>
<div id="ref-Goldstein2015ice" class="csl-entry" role="listitem">
Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. <span>“Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.”</span> <em>Journal of Computational and Graphical Statistics</em> 24 (1): 44–65. <a href="https://doi.org/10.1080/10618600.2014.907095">https://doi.org/10.1080/10618600.2014.907095</a>.
</div>
<div id="ref-gower1971general" class="csl-entry" role="listitem">
Gower, John C. 1971. <span>“A General Coefficient of Similarity and Some of Its Properties.”</span> <em>Biometrics</em>, 857–71. <a href="https://doi.org/10.2307/2528823">https://doi.org/10.2307/2528823</a>.
</div>
<div id="ref-guidotti2022counterfactual" class="csl-entry" role="listitem">
Guidotti, Riccardo. 2022. <span>“Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking.”</span> <em>Data Mining and Knowledge Discovery</em>, 1–55. <a href="https://doi.org/10.1007/s10618-022-00831-6">https://doi.org/10.1007/s10618-022-00831-6</a>.
</div>
<div id="ref-guidotti2018survey" class="csl-entry" role="listitem">
Guidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. <span>“A Survey of Methods for Explaining Black Box Models.”</span> <em>ACM Computing Surveys (CSUR)</em> 51 (5): 1–42. <a href="https://doi.org/10.1145/3236009">https://doi.org/10.1145/3236009</a>.
</div>
<div id="ref-Hooker2019PleaseSP" class="csl-entry" role="listitem">
Hooker, Giles, and Lucas K. Mentch. 2019. <span>“Please Stop Permuting Features: An Explanation and Alternatives.”</span> <a href="https://doi.org/10.48550/arxiv.1905.03151">https://doi.org/10.48550/arxiv.1905.03151</a>.
</div>
<div id="ref-Krzyzinski2023" class="csl-entry" role="listitem">
Krzyziński, Mateusz, Mikołaj Spytek, Hubert Baniecki, and Przemysław Biecek. 2023. <span>“<span class="nocase">SurvSHAP(t)</span>: Time-Dependent Explanations of Machine Learning Survival Models.”</span> <em>Knowledge-Based Systems</em> 262: 110234. <a href="https://doi.org/10.1016/j.knosys.2022.110234">https://doi.org/10.1016/j.knosys.2022.110234</a>.
</div>
<div id="ref-lipton2018mythos" class="csl-entry" role="listitem">
Lipton, Zachary C. 2018. <span>“The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.”</span> <em>Queue</em> 16 (3): 31–57. <a href="https://doi.org/10.1145/3236386.3241340">https://doi.org/10.1145/3236386.3241340</a>.
</div>
<div id="ref-Lundberg2019" class="csl-entry" role="listitem">
Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. 2019. <span>“Consistent Individualized Feature Attribution for Tree Ensembles.”</span> arXiv. <a href="https://doi.org/10.48550/arxiv.1802.03888">https://doi.org/10.48550/arxiv.1802.03888</a>.
</div>
<div id="ref-Molnar2022" class="csl-entry" role="listitem">
Molnar, Christoph. 2022. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>. 2nd ed. <a href="https://christophm.github.io/interpretable-ml-book">https://christophm.github.io/interpretable-ml-book</a>.
</div>
<div id="ref-Molnar2018" class="csl-entry" role="listitem">
Molnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. <span>“<span class="nocase">iml</span>: An <span>R</span> Package for Interpretable Machine Learning.”</span> <em>JOSS</em> 3 (26): 786. <a href="https://doi.org/10.21105/joss.00786">https://doi.org/10.21105/joss.00786</a>.
</div>
<div id="ref-Molnar2022pitfalls" class="csl-entry" role="listitem">
Molnar, Christoph, Gunnar König, Julia Herbinger, Timo Freiesleben, Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz Grosse-Wentrup, and Bernd Bischl. 2022. <span>“General Pitfalls of&nbsp;Model-Agnostic Interpretation Methods for&nbsp;Machine Learning Models.”</span> In <em>xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers</em>, edited by Andreas Holzinger, Randy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert Müller, and Wojciech Samek, 39–68. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-04083-2_4">https://doi.org/10.1007/978-3-031-04083-2_4</a>.
</div>
<div id="ref-Ribeiro2016lime" class="csl-entry" role="listitem">
Ribeiro, Marco, Sameer Singh, and Carlos Guestrin. 2016. <span>“<span>“</span>Why Should <span>I</span> Trust You?<span>”</span>: Explaining the Predictions of Any Classifier.”</span> In <em>Proceedings of the 2016 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Demonstrations</em>, 97–101. San Diego, California: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N16-3020">https://doi.org/10.18653/v1/N16-3020</a>.
</div>
<div id="ref-Romaszko2019" class="csl-entry" role="listitem">
Romaszko, Kamil, Magda Tatarynowicz, Mateusz Urbański, and Przemysław Biecek. 2019. <span>“modelDown: Automated Website Generator with Interpretable Documentation for Predictive Machine Learning Models.”</span> <em>Journal of Open Source Software</em> 4 (38): 1444. <a href="https://doi.org/10.21105/joss.01444">https://doi.org/10.21105/joss.01444</a>.
</div>
<div id="ref-Strobl2008" class="csl-entry" role="listitem">
Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. <span>“Conditional Variable Importance for Random Forests.”</span> <em><span>BMC</span> Bioinformatics</em> 9 (1). <a href="https://doi.org/10.1186/1471-2105-9-307">https://doi.org/10.1186/1471-2105-9-307</a>.
</div>
<div id="ref-Trumbelj2013Shapley" class="csl-entry" role="listitem">
Štrumbelj, Erik, and Igor Kononenko. 2013. <span>“Explaining Prediction Models and Individual Predictions with Feature Contributions.”</span> <em>Knowledge and Information Systems</em> 41 (3): 647–65. <a href="https://doi.org/10.1007/s10115-013-0679-x">https://doi.org/10.1007/s10115-013-0679-x</a>.
</div>
<div id="ref-Wachter2017" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the <span>GDPR</span>.”</span> <em><span>SSRN</span> Electronic Journal</em>. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div>
<div id="ref-Watson2021" class="csl-entry" role="listitem">
Watson, David S, and Marvin N Wright. 2021. <span>“Testing Conditional Independence in Supervised Learning Algorithms.”</span> <em>Machine Learning</em> 110 (8): 2107–29. <a href="https://doi.org/10.1007/s10994-021-06030-6">https://doi.org/10.1007/s10994-021-06030-6</a>.
</div>
<div id="ref-Wexler2019" class="csl-entry" role="listitem">
Wexler, James, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, and Jimbo Wilson. 2019. <span>“The What-If Tool: Interactive Probing of Machine Learning Models.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 26 (1): 56–65. <a href="https://doi.org/10.1109/TVCG.2019.2934619">https://doi.org/10.1109/TVCG.2019.2934619</a>.
</div>
<div id="ref-Wisniewski2022" class="csl-entry" role="listitem">
Wiśniewski, Jakub, and Przemysław Biecek. 2022. <span>“The <span>R</span> Journal: Fairmodels: A Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models.”</span> <em>The <span>R</span> Journal</em> 14: 227–43. <a href="https://doi.org/10.32614/RJ-2022-019">https://doi.org/10.32614/RJ-2022-019</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb44" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Model Interpretation {#sec-interpretation}</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Model Interpretation"`</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>The increasing availability of data and software frameworks to create predictive models has allowed the widespread adoption of ML in many applications.</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>However, high predictive performance of such models often comes at the cost of <span class="in">`r index("interpretability")`</span>.</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>Many models are called a '<span class="in">`r index('black box')`</span>' as the decision-making process behind their predictions is often not immediately interpretable.</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>This lack of explanation can decrease trust in ML and may create barriers to the adoption of predictive models, especially in critical applications such as medicine, engineering, and finance <span class="co">[</span><span class="ot">@lipton2018mythos</span><span class="co">]</span>.</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>In recent years, many interpretation methods have been developed that allow developers to 'peek' inside these models and produce explanations to, for example, understand how features are used by the model to make predictions <span class="co">[</span><span class="ot">@guidotti2018survey</span><span class="co">]</span>.</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>Interpretation methods can be valuable from multiple perspectives:</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>To gain global insights into a model, for example, to identify which features were the most important overall or how the features act on the predictions.</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>To improve the model if flaws are identified (in the data or model), for example, if the model depends on one feature unexpectedly.</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>To understand and control individual predictions, for example, to identify how a given prediction may change if a feature is altered.</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>To assess <span class="in">`r index('algorithmic fairness')`</span>, for example, to inspect whether the model adversely affects certain subpopulations or individuals (see @sec-fairness).</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>In this chapter, we will look at model-agnostic (i.e., can be applied to any model) <span class="in">`r index('interpretable machine learning', aside = TRUE)`</span> (IML) methods that can be used to understand models <span class="in">`r index("post hoc")`</span> (after they have been trained).</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>We will focus on methods implemented in three R packages that nicely interface with <span class="in">`mlr3`</span>: <span class="in">`r ref_pkg("iml")`</span> (@sec-iml), <span class="in">`r ref_pkg("counterfactuals")`</span> (@sec-counterfactuals), and <span class="in">`r ref_pkg("DALEX")`</span> (@sec-dalex).</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="in">`iml`</span> and <span class="in">`DALEX`</span> offer similar functionality but differ in design choices in that <span class="in">`iml`</span> makes use of the <span class="in">`R6`</span> class system whereas <span class="in">`DALEX`</span> is based on the S3 class system.</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="in">`counterfactuals`</span> also uses the <span class="in">`R6`</span> class system.</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>In contrast to <span class="in">`iml`</span> and <span class="in">`counterfactuals`</span>, <span class="in">`DALEX`</span> focuses on comparing multiple predictive models, usually of different types.</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>We will only provide a brief overview of the methodology discussed below, we recommend @Molnar2022 as a comprehensive introductory book about IML.</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>As a running example throughout this chapter, we will consider a gradient boosting machine (GBM) fit on half the features in the <span class="in">`"german_credit"`</span> task.</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>In practice, we would tune the hyperparameters of GBM as discussed in @sec-optimization and perform feature selection as discussed in @sec-feature-selection to select the most relevant features.</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>However, for the sake of simplicity, we utilize an untuned GBM in these examples as it exhibited satisfactory performance even without fine-tuning.</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-003, results = 'hide'}</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3verse)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>tsk_german <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>)<span class="sc">$</span><span class="fu">select</span>(</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"duration"</span>, <span class="st">"amount"</span>, <span class="st">"age"</span>, <span class="st">"status"</span>, <span class="st">"savings"</span>, <span class="st">"purpose"</span>,</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>  <span class="st">"credit_history"</span>, <span class="st">"property"</span>, <span class="st">"employment_duration"</span>, <span class="st">"other_debtors"</span>))</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>split <span class="ot">=</span> <span class="fu">partition</span>(tsk_german)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>lrn_gbm <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.gbm"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>lrn_gbm<span class="sc">$</span><span class="fu">train</span>(tsk_german, <span class="at">row_ids =</span> split<span class="sc">$</span>train)</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## Performance-based Interpretation Methods Require Test Data</span></span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>Performance-based interpretation methods such as permutation feature importance (@sec-feat-importance) rely on measuring the generalization performance. Hence, they should be computed on an independent test set to decrease bias in estimation (see @sec-performance).</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>However, the differences in interpretation between training and test data are less pronounced <span class="co">[</span><span class="ot">@Molnar2022pitfalls</span><span class="co">]</span> in prediction-based methods that do not require performance estimation such as ICE/PD (@sec-feature-effects) or Shapley values (@sec-shapley).</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## The iml Package {#sec-iml}</span></span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>\index{\texttt{iml}}<span class="in">`r ref_pkg("iml")`</span> <span class="co">[</span><span class="ot">@Molnar2018</span><span class="co">]</span> implements a unified interface for a variety of model-agnostic interpretation methods that facilitate the analysis and interpretation of machine learning models.</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a><span class="in">`iml`</span> supports machine learning models (for classification or regression) fitted by *any* R package, and in particular all <span class="in">`mlr3`</span> models are supported by wrapping learners in an <span class="in">`r ref("iml::Predictor")`</span> object, which unifies the input-output behavior of the trained models.</span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>This object contains the prediction model as well as the data used for analyzing the model and producing the desired explanation.</span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>We construct the <span class="in">`Predictor`</span> object using our trained learner and heldout test data:</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-Predictor}</span></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(iml)</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a><span class="co"># features in test data</span></span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>credit_x <span class="ot">=</span> tsk_german<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> split<span class="sc">$</span>test,</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>  <span class="at">cols =</span> tsk_german<span class="sc">$</span>feature_names)</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a><span class="co"># target in test data</span></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>credit_y <span class="ot">=</span> tsk_german<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> split<span class="sc">$</span>test,</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>  <span class="at">cols =</span> tsk_german<span class="sc">$</span>target_names)</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>predictor <span class="ot">=</span> Predictor<span class="sc">$</span><span class="fu">new</span>(lrn_gbm, <span class="at">data =</span> credit_x, <span class="at">y =</span> credit_y)</span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a>With our <span class="in">`Predictor`</span> setup we can now consider different model interpretation methods.</span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### `r index('Feature Importance')` {#sec-feat-importance}</span></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>When deploying a model in practice, it is often of interest to know which features contribute the most to the *predictive performance* of the model.</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>This can be useful to better understand the problem at hand and the relationship between features and target.</span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a>In model development, this can be used to filter features (@sec-fs-filter) that do not contribute a lot to the model's predictive ability.</span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a>In this book, we use the term 'feature importance' to describe global methods that calculate a single score per feature that reflect the importance regarding a given quantity of interest, e.g., model performance, thus allowing features to be ranked.</span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>One of the most popular feature importance methods is the <span class="in">`r index('permutation feature importance', aside = TRUE)`</span> (PFI), originally introduced by @breiman2001random for random forests\index{random forest} and adapted by @Fisher2019pfi as a model-agnostic feature importance measure (originally termed, 'model reliance').</span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a>Feature permutation is the process of randomly shuffling observed values for a single feature in a dataset.</span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a>This removes the original dependency structure of the feature with the target variable and with all other features while maintaining the marginal distribution of the feature.</span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a>The PFI measures the change in the model performance before (original model performance) and after (permuted model performance) permuting a feature.</span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>If a feature is not important, then there will be little change in model performance after permuting that feature.</span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a>Conversely, we would expect a clear decrease in model performance if the feature is more important.</span>
<span id="cb44-85"><a href="#cb44-85" aria-hidden="true" tabindex="-1"></a>It is generally recommended to repeat the permutation process and aggregate performance changes over multiple repetitions to decrease randomness in results.</span>
<span id="cb44-86"><a href="#cb44-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-87"><a href="#cb44-87" aria-hidden="true" tabindex="-1"></a>PFI is run in <span class="in">`iml`</span> by constructing an object of class <span class="in">`r ref("iml::FeatureImp")`</span> and specifying the performance measure, below we use classification error.</span>
<span id="cb44-88"><a href="#cb44-88" aria-hidden="true" tabindex="-1"></a>By default, the permutation is repeated five times to keep computation time low (this can be changed with <span class="in">`n.repetitions`</span> when calling the constructor <span class="in">`$new()`</span>, below we set <span class="in">`n.repetitions = 100`</span>) and in each repetition, the importance value corresponding to the change in the classification error is calculated.</span>
<span id="cb44-89"><a href="#cb44-89" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$plot()`</span> method shows the median of the five resulting importance values (as a point) and the boundaries of the error bars in the plot refer to the 5% and 95% quantiles of the importance values (@fig-iml-pfi).</span>
<span id="cb44-90"><a href="#cb44-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-91"><a href="#cb44-91" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-92"><a href="#cb44-92" aria-hidden="true" tabindex="-1"></a><span class="fu">## Increase the Number of Repetitions to Obtain Useful Error Bars</span></span>
<span id="cb44-93"><a href="#cb44-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-94"><a href="#cb44-94" aria-hidden="true" tabindex="-1"></a>The default number of repetitions when constructing a <span class="in">`FeatureImp`</span> object is <span class="in">`5`</span>.</span>
<span id="cb44-95"><a href="#cb44-95" aria-hidden="true" tabindex="-1"></a>However, the number of repetitions should be increased if you want to obtain useful error bars from the resulting plot.</span>
<span id="cb44-96"><a href="#cb44-96" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-97"><a href="#cb44-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-98"><a href="#cb44-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-007}</span></span>
<span id="cb44-99"><a href="#cb44-99" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-100"><a href="#cb44-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb44-101"><a href="#cb44-101" aria-hidden="true" tabindex="-1"></a>importance <span class="ot">=</span> FeatureImp<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">loss =</span> <span class="st">"ce"</span>, <span class="at">n.repetitions =</span> <span class="dv">100</span>)</span>
<span id="cb44-102"><a href="#cb44-102" aria-hidden="true" tabindex="-1"></a>importance<span class="sc">$</span><span class="fu">plot</span>()</span>
<span id="cb44-103"><a href="#cb44-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-104"><a href="#cb44-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-107"><a href="#cb44-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-108"><a href="#cb44-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3</span></span>
<span id="cb44-109"><a href="#cb44-109" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-iml-pfi</span></span>
<span id="cb44-110"><a href="#cb44-110" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Permutation feature importance (PFI). Points indicate the median and bars the 5% and 95% quantiles of the PFI over five repetitions of the permutation process.</span></span>
<span id="cb44-111"><a href="#cb44-111" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "x-axis says 'Feature Importance (loss: ce)' and y-axis lists the features in the data. Plot shows 10 error bars, one for each feature, with solid black circles in the middle (the median importance value across the repetitions) and horizontal black lines on each row (from the 5% to 95% quantile of the feature importance values). Top three most important features are `status`, `duration`, and `savings`."</span></span>
<span id="cb44-112"><a href="#cb44-112" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb44-113"><a href="#cb44-113" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb44-114"><a href="#cb44-114" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb44-115"><a href="#cb44-115" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb44-116"><a href="#cb44-116" aria-hidden="true" tabindex="-1"></a>plt<span class="sc">$</span>layers[[<span class="dv">1</span>]]<span class="sc">$</span>aes_params<span class="sc">$</span>colour <span class="ot">=</span> <span class="st">"grey30"</span></span>
<span id="cb44-117"><a href="#cb44-117" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(plt)</span>
<span id="cb44-118"><a href="#cb44-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-119"><a href="#cb44-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-120"><a href="#cb44-120" aria-hidden="true" tabindex="-1"></a>The plot automatically ranks features from most (largest median performance change) to least (smallest median performance change) important.</span>
<span id="cb44-121"><a href="#cb44-121" aria-hidden="true" tabindex="-1"></a>In @fig-iml-pfi, the feature ``r importance$results$feature<span class="co">[</span><span class="ot">1</span><span class="co">]</span>`` is most important, if we permute the ``r importance$results$feature<span class="co">[</span><span class="ot">1</span><span class="co">]</span>`<span class="in">` column in the data the classification error of our model increases by a factor of around `</span>r round(importance$results$importance<span class="co">[</span><span class="ot">1</span><span class="co">]</span>,2)`.</span>
<span id="cb44-122"><a href="#cb44-122" aria-hidden="true" tabindex="-1"></a>By default, <span class="in">`FeatureImp`</span> calculates the *ratio* of the model performance before and after permutation as an importance value; the *difference* of the performance measures can be returned by passing <span class="in">`compare = "difference"`</span> when calling <span class="in">`$new()`</span>.</span>
<span id="cb44-123"><a href="#cb44-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-124"><a href="#cb44-124" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Effects {#sec-feature-effects}</span></span>
<span id="cb44-125"><a href="#cb44-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-126"><a href="#cb44-126" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Feature effect")`</span> methods describe how or to what extent a feature contributes towards the *model predictions* by analyzing how the predictions change when changing a feature.</span>
<span id="cb44-127"><a href="#cb44-127" aria-hidden="true" tabindex="-1"></a>These methods can be distinguished between local and global feature effect methods.</span>
<span id="cb44-128"><a href="#cb44-128" aria-hidden="true" tabindex="-1"></a>Global feature effect methods refer to how a prediction changes *on average* when a feature is changed.</span>
<span id="cb44-129"><a href="#cb44-129" aria-hidden="true" tabindex="-1"></a>In contrast, local feature effect methods address the question of how a *single* prediction of a given observation changes when a feature value is changed.</span>
<span id="cb44-130"><a href="#cb44-130" aria-hidden="true" tabindex="-1"></a>To a certain extent, local feature effect methods can reveal interactions in the model that become visible when the local effects are heterogeneous, i.e., if changes in the local effect are different across the observations.</span>
<span id="cb44-131"><a href="#cb44-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-132"><a href="#cb44-132" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Partial dependence', aside = TRUE)`</span> (PD) plots <span class="co">[</span><span class="ot">@Friedman2001pdp</span><span class="co">]</span> can be used to visualize global feature effects by visualizing how model predictions change on average when varying the values of a given feature of interest.</span>
<span id="cb44-133"><a href="#cb44-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-134"><a href="#cb44-134" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Individual conditional expectation', aside = TRUE)`</span> (ICE) curves  <span class="co">[</span><span class="ot">@Goldstein2015ice</span><span class="co">]</span> (a.k.a. Ceteris Paribus Effects\index{ceteris paribus|see{individual conditional expectation (ICE) curves}}) are a local feature effects method that display how the prediction of a *single* observation changes when varying a feature of interest, while all other features stay constant.</span>
<span id="cb44-135"><a href="#cb44-135" aria-hidden="true" tabindex="-1"></a>@Goldstein2015ice demonstrated that the PD plot is the average of ICE curves.</span>
<span id="cb44-136"><a href="#cb44-136" aria-hidden="true" tabindex="-1"></a>ICE curves are constructed by taking a single observation and feature of interest, and then replacing the feature's value with another value and plotting the new prediction, this is then repeated for many feature values (e.g., across an equidistant grid of the feature's value range).</span>
<span id="cb44-137"><a href="#cb44-137" aria-hidden="true" tabindex="-1"></a>The x-axis of an ICE curve visualizes the set of replacement feature values and the y-axis is the model prediction.</span>
<span id="cb44-138"><a href="#cb44-138" aria-hidden="true" tabindex="-1"></a>Each ICE curve is a local explanation that assesses the feature effect of a single observation on the model prediction.</span>
<span id="cb44-139"><a href="#cb44-139" aria-hidden="true" tabindex="-1"></a>An ICE plot contains one ICE curve (line) per observation.</span>
<span id="cb44-140"><a href="#cb44-140" aria-hidden="true" tabindex="-1"></a>If the ICE curves are heterogeneous, i.e., not parallel, then the model may have estimated an interaction involving the considered feature.</span>
<span id="cb44-141"><a href="#cb44-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-142"><a href="#cb44-142" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-143"><a href="#cb44-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Effects Can Be Non-Linear</span></span>
<span id="cb44-144"><a href="#cb44-144" aria-hidden="true" tabindex="-1"></a>Feature effects are very similar to regression coefficients, $\beta$, in linear models which offer interpretations such as</span>
<span id="cb44-145"><a href="#cb44-145" aria-hidden="true" tabindex="-1"></a>"if you increase this feature by one unit, your prediction increases on average by $\beta$ if all other features stay constant".</span>
<span id="cb44-146"><a href="#cb44-146" aria-hidden="true" tabindex="-1"></a>However, feature effects are not limited to linear effects and can be applied to any type of predictive model.</span>
<span id="cb44-147"><a href="#cb44-147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-148"><a href="#cb44-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-149"><a href="#cb44-149" aria-hidden="true" tabindex="-1"></a>Let us put this into practice by considering how the feature <span class="in">`amount`</span> influences the predictions in our subsetted credit classification task.</span>
<span id="cb44-150"><a href="#cb44-150" aria-hidden="true" tabindex="-1"></a>Below we initialize an object of class <span class="in">`r ref("iml::FeatureEffect")`</span> by passing the feature name of interest and the feature effect method, we use <span class="in">`"pdp+ice"`</span> to indicate that we want to visualize ICE curves with a PD plot (average of the ICE curves).</span>
<span id="cb44-151"><a href="#cb44-151" aria-hidden="true" tabindex="-1"></a>We recommend always plotting PD and ICE curves together as PD plots on their own could mask heterogeneous effects.</span>
<span id="cb44-152"><a href="#cb44-152" aria-hidden="true" tabindex="-1"></a>We use <span class="in">`$plot()`</span> to visualize the results (@fig-iml-pdice).</span>
<span id="cb44-153"><a href="#cb44-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-154"><a href="#cb44-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-pdp}</span></span>
<span id="cb44-155"><a href="#cb44-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3</span></span>
<span id="cb44-156"><a href="#cb44-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-iml-pdice</span></span>
<span id="cb44-157"><a href="#cb44-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Partial dependence (PD) plot (yellow) and individual conditional expectation (ICE) curves (black) that show how the credit amount affects the predicted credit risk.</span></span>
<span id="cb44-158"><a href="#cb44-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Two plots are visualized side-by-side. The x-axis for both says 'amount' and ranges from 0 to around 16000. The y-axis for both says 'Predicted credit_risk' and ranges from 0 to 1. The left plot is captioned 'good' and shows many thin black curves that are roughly parallel and slowly decrease from 0-10000 and then are roughly flat until the end of the plot. The right plot is captioned 'bad' and shows many thin black curves that are roughly parallel and slowly increase from 0-10000 and then are roughly flat until the end of the plot."</span></span>
<span id="cb44-159"><a href="#cb44-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-160"><a href="#cb44-160" aria-hidden="true" tabindex="-1"></a>effect <span class="ot">=</span> FeatureEffect<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">feature =</span> <span class="st">"amount"</span>,</span>
<span id="cb44-161"><a href="#cb44-161" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"pdp+ice"</span>)</span>
<span id="cb44-162"><a href="#cb44-162" aria-hidden="true" tabindex="-1"></a>effect<span class="sc">$</span><span class="fu">plot</span>()</span>
<span id="cb44-163"><a href="#cb44-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-164"><a href="#cb44-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-165"><a href="#cb44-165" aria-hidden="true" tabindex="-1"></a>@fig-iml-pdice shows that if the <span class="in">`amount`</span> is smaller than roughly 10,000 then on average there is a high chance that the predicted creditworthiness will be <span class="in">`good`</span>. Furthermore, the ICE curves are roughly parallel, meaning that there do not seem to be strong interactions present where <span class="in">`amount`</span> is involved.</span>
<span id="cb44-166"><a href="#cb44-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-167"><a href="#cb44-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### Surrogate Models</span></span>
<span id="cb44-168"><a href="#cb44-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-169"><a href="#cb44-169" aria-hidden="true" tabindex="-1"></a>Interpretable models such as decision trees or linear models can be used as <span class="in">`r index("surrogate models", "surrogate model")`</span> to approximate or mimic an, often very complex, black box model.</span>
<span id="cb44-170"><a href="#cb44-170" aria-hidden="true" tabindex="-1"></a>Inspecting the surrogate model can provide insights into the behavior of a black box model, for example by looking at the model coefficients in a linear regression or splits in a decision tree.</span>
<span id="cb44-171"><a href="#cb44-171" aria-hidden="true" tabindex="-1"></a>We differentiate between local surrogate models, which approximate a model locally around a specific data point of interest, and global surrogate models which approximate the model across the entire input space <span class="co">[</span><span class="ot">@Ribeiro2016lime; @Molnar2022</span><span class="co">]</span>.</span>
<span id="cb44-172"><a href="#cb44-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-173"><a href="#cb44-173" aria-hidden="true" tabindex="-1"></a>The features used to train a surrogate model are usually the same features used to train the black box model or at least data with the same distribution to ensure a representative input space.</span>
<span id="cb44-174"><a href="#cb44-174" aria-hidden="true" tabindex="-1"></a>However, the target used to train the surrogate model is the predictions obtained from the black box model, not the real outcome of the underlying data.</span>
<span id="cb44-175"><a href="#cb44-175" aria-hidden="true" tabindex="-1"></a>Hence, conclusions drawn from the surrogate model are only valid if the surrogate model approximates the black box model very well (i.e., if the model fidelity is high).</span>
<span id="cb44-176"><a href="#cb44-176" aria-hidden="true" tabindex="-1"></a>It is therefore also important to measure and report the approximation error of the surrogate model.</span>
<span id="cb44-177"><a href="#cb44-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-178"><a href="#cb44-178" aria-hidden="true" tabindex="-1"></a>The data used to train the black box model may be very complex or limited, making it challenging to directly train a well-performing interpretable model on that data.</span>
<span id="cb44-179"><a href="#cb44-179" aria-hidden="true" tabindex="-1"></a>Instead, we can use the black box model to generate new labeled data in specific regions of the input space with which we can augment the original data.</span>
<span id="cb44-180"><a href="#cb44-180" aria-hidden="true" tabindex="-1"></a>The augmented data can then be used to train an interpretable model that captures and explains the relationships learned by the black box model (in specific regions) or to identify flaws or unexpected behavior.</span>
<span id="cb44-181"><a href="#cb44-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-182"><a href="#cb44-182" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Global Surrogate Model</span></span>
<span id="cb44-183"><a href="#cb44-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-184"><a href="#cb44-184" aria-hidden="true" tabindex="-1"></a>\index{surrogate model!global}Initializing the <span class="in">`r ref("iml::TreeSurrogate")`</span> class fits a conditional inference tree (<span class="in">`r ref("partykit::ctree()")`</span>) surrogate model to the predictions from our trained model.</span>
<span id="cb44-185"><a href="#cb44-185" aria-hidden="true" tabindex="-1"></a>This class extracts the decision rules created by the tree surrogate and the <span class="in">`$plot()`</span> method visualizes the distribution of the predicted outcomes from each terminal node.</span>
<span id="cb44-186"><a href="#cb44-186" aria-hidden="true" tabindex="-1"></a>Below, we pass <span class="in">`maxdepth = 2`</span> to the constructor to build a tree with two binary splits, yielding four terminal nodes.</span>
<span id="cb44-187"><a href="#cb44-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-188"><a href="#cb44-188" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-globalsurrogate,message=FALSE}</span></span>
<span id="cb44-189"><a href="#cb44-189" aria-hidden="true" tabindex="-1"></a>tree_surrogate <span class="ot">=</span> TreeSurrogate<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">maxdepth =</span> 2L)</span>
<span id="cb44-190"><a href="#cb44-190" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-191"><a href="#cb44-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-192"><a href="#cb44-192" aria-hidden="true" tabindex="-1"></a>Before inspecting this model, we need to first check if the surrogate model approximates the prediction model accurately, which we can assess by comparing the predictions of the tree surrogate and the predictions of the black box model.</span>
<span id="cb44-193"><a href="#cb44-193" aria-hidden="true" tabindex="-1"></a>For example, we could quantify the number of matching predictions and measure the accuracy of the surrogate in predicting the predictions of the black box GBM model:</span>
<span id="cb44-194"><a href="#cb44-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-195"><a href="#cb44-195" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-crosstable}</span></span>
<span id="cb44-196"><a href="#cb44-196" aria-hidden="true" tabindex="-1"></a>pred_surrogate <span class="ot">=</span> tree_surrogate<span class="sc">$</span><span class="fu">predict</span>(credit_x, <span class="at">type =</span> <span class="st">"class"</span>)<span class="sc">$</span>.class</span>
<span id="cb44-197"><a href="#cb44-197" aria-hidden="true" tabindex="-1"></a>pred_surrogate <span class="ot">=</span> <span class="fu">factor</span>(pred_surrogate, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"good"</span>, <span class="st">"bad"</span>))</span>
<span id="cb44-198"><a href="#cb44-198" aria-hidden="true" tabindex="-1"></a>pred_gbm <span class="ot">=</span> lrn_gbm<span class="sc">$</span><span class="fu">predict_newdata</span>(credit_x)<span class="sc">$</span>response</span>
<span id="cb44-199"><a href="#cb44-199" aria-hidden="true" tabindex="-1"></a>confusion <span class="ot">=</span> mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(pred_surrogate, pred_gbm,</span>
<span id="cb44-200"><a href="#cb44-200" aria-hidden="true" tabindex="-1"></a>  <span class="at">positive =</span> <span class="st">"good"</span>)</span>
<span id="cb44-201"><a href="#cb44-201" aria-hidden="true" tabindex="-1"></a>confusion</span>
<span id="cb44-202"><a href="#cb44-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-203"><a href="#cb44-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-204"><a href="#cb44-204" aria-hidden="true" tabindex="-1"></a>This shows an accuracy of around <span class="in">`r round(confusion$measures[["acc"]] * 100)`</span>% in predictions from the surrogate compared to the black box model, which is good enough for us to use our surrogate for further interpretation, for example by plotting the splits in the terminal node:</span>
<span id="cb44-205"><a href="#cb44-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-206"><a href="#cb44-206" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-globalsurrogate-plot,message=FALSE}</span></span>
<span id="cb44-207"><a href="#cb44-207" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Distribution of the predicted outcomes for each terminal node identified by the tree surrogate. The top two nodes consist of applications with a positive balance in the account (`status`is either `"0 &lt;= ... &lt; 200 DM"`, `"... &gt;= 200 DM"` or `"salary for at least 1 year"`) and either a duration of less or equal than 42 months (top left), or more than 42 months (top right). The bottom nodes contain applicants that either have no checking account or a negative balance (`status`) and either a duration of less than or equal to 36 months (bottom left) or more than 36 months (bottom right).</span></span>
<span id="cb44-208"><a href="#cb44-208" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Four barplots with 'count' on the y-axis and '.class' on the x-axis. Top left shows 150 'good' credit predictions and around 1 'bad' prediction. Top right shows around 10 'good' predictions and 1 'bad' one. Bottom left shows around 120 'good' predictions and 40 'bad' ones. Bottom right shows about 23 'bad' predictions and around 5 'good' ones.</span></span>
<span id="cb44-209"><a href="#cb44-209" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-iml-surro</span></span>
<span id="cb44-210"><a href="#cb44-210" aria-hidden="true" tabindex="-1"></a>tree_surrogate<span class="sc">$</span><span class="fu">plot</span>()</span>
<span id="cb44-211"><a href="#cb44-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-212"><a href="#cb44-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-213"><a href="#cb44-213" aria-hidden="true" tabindex="-1"></a>Or we could access the trained tree surrogate via the <span class="in">`$tree`</span> field of the <span class="in">`TreeSurrogate`</span> object and then have access to all methods in <span class="in">`r ref_pkg("partykit")`</span>:</span>
<span id="cb44-214"><a href="#cb44-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-215"><a href="#cb44-215" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-globalsurrogate-tree}</span></span>
<span id="cb44-216"><a href="#cb44-216" aria-hidden="true" tabindex="-1"></a>partykit<span class="sc">::</span><span class="fu">print.party</span>(tree_surrogate<span class="sc">$</span>tree)</span>
<span id="cb44-217"><a href="#cb44-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-218"><a href="#cb44-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-219"><a href="#cb44-219" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Since the surrogate model only uses the predictions of the black box model (here, the GBM model) and not the real outcomes of the underlying data, the conclusions drawn from the surrogate model do not apply generally, but only to the black box model (if the approximation of the surrogate model is accurate enough). --&gt;</span></span>
<span id="cb44-220"><a href="#cb44-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-221"><a href="#cb44-221" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Local Surrogate Model</span></span>
<span id="cb44-222"><a href="#cb44-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-223"><a href="#cb44-223" aria-hidden="true" tabindex="-1"></a>\index{surrogate model!local}In general, it can be very difficult to accurately approximate the black box model with an interpretable surrogate in the entire feature space.</span>
<span id="cb44-224"><a href="#cb44-224" aria-hidden="true" tabindex="-1"></a>Therefore, local surrogate models focus on a small area in the feature space surrounding a point of interest.</span>
<span id="cb44-225"><a href="#cb44-225" aria-hidden="true" tabindex="-1"></a>Local surrogate models are constructed as follows:</span>
<span id="cb44-226"><a href="#cb44-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-227"><a href="#cb44-227" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Obtain predictions from the black box model for a given dataset.</span>
<span id="cb44-228"><a href="#cb44-228" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Weight the observations in this dataset by their proximity to our point of interest.</span>
<span id="cb44-229"><a href="#cb44-229" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Fit an interpretable, surrogate model on the weighted dataset using the predictions of the black box model as the target.</span>
<span id="cb44-230"><a href="#cb44-230" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Explain the prediction of our point of interest with the surrogate model.</span>
<span id="cb44-231"><a href="#cb44-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-232"><a href="#cb44-232" aria-hidden="true" tabindex="-1"></a>To illustrate this, we will select a random data point to explain.</span>
<span id="cb44-233"><a href="#cb44-233" aria-hidden="true" tabindex="-1"></a>As we are dealing with people, we will name our observation "Charlie" and first look at the black box predictions:</span>
<span id="cb44-234"><a href="#cb44-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-235"><a href="#cb44-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{r Charlie,  asis='results'}</span></span>
<span id="cb44-236"><a href="#cb44-236" aria-hidden="true" tabindex="-1"></a>Charlie <span class="ot">=</span> credit_x[<span class="dv">35</span>, ]</span>
<span id="cb44-237"><a href="#cb44-237" aria-hidden="true" tabindex="-1"></a>gbm_predict <span class="ot">=</span> predictor<span class="sc">$</span><span class="fu">predict</span>(Charlie)</span>
<span id="cb44-238"><a href="#cb44-238" aria-hidden="true" tabindex="-1"></a>gbm_predict</span>
<span id="cb44-239"><a href="#cb44-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-240"><a href="#cb44-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-241"><a href="#cb44-241" aria-hidden="true" tabindex="-1"></a>We can see that the model predicts the class '<span class="in">`r names(which.max(predictor$predict(Charlie)))`</span>' with <span class="in">`r round(max(predictor$predict(Charlie))*100, 1)`</span>%  probability, so now we can use <span class="in">`r ref("iml::LocalModel")`</span> to find out why this prediction was made.</span>
<span id="cb44-242"><a href="#cb44-242" aria-hidden="true" tabindex="-1"></a>The underlying surrogate model is a locally weighted L1-penalized linear regression model such that only a pre-defined number of features per class, <span class="in">`k`</span> (default is <span class="in">`3`</span>), will have a non-zero coefficient and as such are the <span class="in">`k`</span> most influential features, below we set <span class="in">`k = 2`</span>.</span>
<span id="cb44-243"><a href="#cb44-243" aria-hidden="true" tabindex="-1"></a>We can also set the parameter <span class="in">`gower.power`</span> which specifies the size of the neighborhood for the local model (default is <span class="in">`gower.power = 1`</span>), the smaller the value, the more the model will focus on points closer to the point of interest, below we set <span class="in">`gower.power = 0.1`</span>.</span>
<span id="cb44-244"><a href="#cb44-244" aria-hidden="true" tabindex="-1"></a>This implementation is very closely related to Local Interpretable Model-agnostic Explanations (<span class="in">`r index('LIME', lower = FALSE)`</span>) <span class="co">[</span><span class="ot">@Ribeiro2016lime</span><span class="co">]</span>, the differences are outlined in the documentation of <span class="in">`iml::LocalModel`</span>.</span>
<span id="cb44-245"><a href="#cb44-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-246"><a href="#cb44-246" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-local_surrogate,message=FALSE,warning=FALSE}</span></span>
<span id="cb44-247"><a href="#cb44-247" aria-hidden="true" tabindex="-1"></a>predictor<span class="sc">$</span>class <span class="ot">=</span> <span class="st">"good"</span> <span class="co"># explain the 'good' class</span></span>
<span id="cb44-248"><a href="#cb44-248" aria-hidden="true" tabindex="-1"></a>local_surrogate <span class="ot">=</span> LocalModel<span class="sc">$</span><span class="fu">new</span>(predictor, Charlie, <span class="at">gower.power =</span> <span class="fl">0.1</span>,</span>
<span id="cb44-249"><a href="#cb44-249" aria-hidden="true" tabindex="-1"></a>  <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb44-250"><a href="#cb44-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-251"><a href="#cb44-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-252"><a href="#cb44-252" aria-hidden="true" tabindex="-1"></a>If the prediction of the local model and the prediction of the black box GBM model greatly differ, then you might want to experiment with changing the <span class="in">`k`</span> and <span class="in">`gower.power`</span> parameters.</span>
<span id="cb44-253"><a href="#cb44-253" aria-hidden="true" tabindex="-1"></a>These parameters can be considered as hyperparameters of the local surrogate model, which should be tuned to obtain an accurate local surrogate.</span>
<span id="cb44-254"><a href="#cb44-254" aria-hidden="true" tabindex="-1"></a>First, we check if the predictions for Charlie match:</span>
<span id="cb44-255"><a href="#cb44-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-258"><a href="#cb44-258" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-259"><a href="#cb44-259" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">gbm =</span> gbm_predict[[<span class="dv">1</span>]], <span class="at">local =</span> local_surrogate<span class="sc">$</span><span class="fu">predict</span>()[[<span class="dv">1</span>]])</span>
<span id="cb44-260"><a href="#cb44-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-261"><a href="#cb44-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-262"><a href="#cb44-262" aria-hidden="true" tabindex="-1"></a>Ideally, we should assess the fidelity of the surrogate model in the local neighborhood of Charlie, i.e., how well the local surrogate model approximates the predictions of the black box GBM model for multiple data points in the vicinity of Charlie.</span>
<span id="cb44-263"><a href="#cb44-263" aria-hidden="true" tabindex="-1"></a>A practical approach to assess this local model fidelity involves generating artificial data points within Charlie's local neighborhood (and potentially applying distance-based weighting) or selecting the $k$ nearest neighbors from the original data.</span>
<span id="cb44-264"><a href="#cb44-264" aria-hidden="true" tabindex="-1"></a>For illustration purposes, we now quantify the approximation error using the mean absolute error calculated from the 10 nearest neighbors (including Charlie) according to the Gower distance <span class="co">[</span><span class="ot">@gower1971general</span><span class="co">]</span>:</span>
<span id="cb44-265"><a href="#cb44-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-268"><a href="#cb44-268" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-269"><a href="#cb44-269" aria-hidden="true" tabindex="-1"></a>ind_10nn <span class="ot">=</span> gower<span class="sc">::</span><span class="fu">gower_topn</span>(Charlie, credit_x, <span class="at">n =</span> <span class="dv">10</span>)<span class="sc">$</span>index[, <span class="dv">1</span>]</span>
<span id="cb44-270"><a href="#cb44-270" aria-hidden="true" tabindex="-1"></a>Charlie_10nn <span class="ot">=</span> credit_x[ind_10nn, ]</span>
<span id="cb44-271"><a href="#cb44-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-272"><a href="#cb44-272" aria-hidden="true" tabindex="-1"></a>gbm_pred_10nn <span class="ot">=</span> predictor<span class="sc">$</span><span class="fu">predict</span>(Charlie_10nn)[[<span class="dv">1</span>]]</span>
<span id="cb44-273"><a href="#cb44-273" aria-hidden="true" tabindex="-1"></a>local_pred_10nn <span class="ot">=</span> local_surrogate<span class="sc">$</span><span class="fu">predict</span>(Charlie_10nn)[[<span class="dv">1</span>]]</span>
<span id="cb44-274"><a href="#cb44-274" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(gbm_pred_10nn <span class="sc">-</span> local_pred_10nn))</span>
<span id="cb44-275"><a href="#cb44-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-276"><a href="#cb44-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-277"><a href="#cb44-277" aria-hidden="true" tabindex="-1"></a>As we see good agreement between the local and black box model (on average, the predictions of both the local surrogate and the black box model for Charlie's 10 nearest neighbors differ only by <span class="in">`r round(mean(abs(gbm_pred_10nn - local_pred_10nn)), 3)`</span>), we can move on to look at the most influential features for Charlie's predictions:</span>
<span id="cb44-278"><a href="#cb44-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-279"><a href="#cb44-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval = FALSE}</span></span>
<span id="cb44-280"><a href="#cb44-280" aria-hidden="true" tabindex="-1"></a>local_surrogate<span class="sc">$</span>results[, <span class="fu">c</span>(<span class="st">"feature.value"</span>, <span class="st">"effect"</span>)]</span>
<span id="cb44-281"><a href="#cb44-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-282"><a href="#cb44-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb44-283"><a href="#cb44-283" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> local_surrogate<span class="sc">$</span>results[, <span class="fu">c</span>(<span class="st">"feature.value"</span>, <span class="st">"effect"</span>)]</span>
<span id="cb44-284"><a href="#cb44-284" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(x) <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb44-285"><a href="#cb44-285" aria-hidden="true" tabindex="-1"></a>x</span>
<span id="cb44-286"><a href="#cb44-286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-287"><a href="#cb44-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-288"><a href="#cb44-288" aria-hidden="true" tabindex="-1"></a>In this case, 'duration' and 'status' were most important and both have a negative effect on the prediction of Charlie.</span>
<span id="cb44-289"><a href="#cb44-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-290"><a href="#cb44-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Shapley Values {#sec-shapley}</span></span>
<span id="cb44-291"><a href="#cb44-291" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Shapley values", lower = FALSE)`</span> were originally developed in the context of cooperative game theory to study how the payout of a game can be fairly distributed among the players that form a team.</span>
<span id="cb44-292"><a href="#cb44-292" aria-hidden="true" tabindex="-1"></a>This concept has been adapted for use in ML as a local interpretation method to explain the contributions of each input feature to the final model prediction of a single observation <span class="co">[</span><span class="ot">@Trumbelj2013Shapley</span><span class="co">]</span>.</span>
<span id="cb44-293"><a href="#cb44-293" aria-hidden="true" tabindex="-1"></a>Hence, the 'players' are the features, and the 'payout', which should be fairly distributed among features, refers to the difference between the individual observation's prediction and the mean prediction.</span>
<span id="cb44-294"><a href="#cb44-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-295"><a href="#cb44-295" aria-hidden="true" tabindex="-1"></a>Shapley values estimate how much each input feature contributed to the final prediction for a single observation (after subtracting the mean prediction).</span>
<span id="cb44-296"><a href="#cb44-296" aria-hidden="true" tabindex="-1"></a>By assigning a value to each feature, we can gain insights into which features were the most important ones for the considered observation.</span>
<span id="cb44-297"><a href="#cb44-297" aria-hidden="true" tabindex="-1"></a>Compared to the penalized linear model as a local surrogate model, Shapley values guarantee that the prediction is fairly distributed among the features as they also inherently consider interactions between features when calculating the contribution of each feature.</span>
<span id="cb44-298"><a href="#cb44-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-299"><a href="#cb44-299" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb44-300"><a href="#cb44-300" aria-hidden="true" tabindex="-1"></a><span class="fu">## Correctly Interpreting Shapley Values</span></span>
<span id="cb44-301"><a href="#cb44-301" aria-hidden="true" tabindex="-1"></a>Shapley values are frequently **misinterpreted** as the difference between the predicted value after removing the feature from model training.</span>
<span id="cb44-302"><a href="#cb44-302" aria-hidden="true" tabindex="-1"></a>The Shapley value of a feature is calculated by considering all possible subsets of features and computing the difference in the model prediction with and without the feature of interest included.</span>
<span id="cb44-303"><a href="#cb44-303" aria-hidden="true" tabindex="-1"></a>Hence, it refers to the average marginal contribution of a feature to the difference between the actual prediction and the mean prediction, given the current set of features.</span>
<span id="cb44-304"><a href="#cb44-304" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-305"><a href="#cb44-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-306"><a href="#cb44-306" aria-hidden="true" tabindex="-1"></a>Shapley values can be calculated by passing the <span class="in">`Predictor`</span> and the observation of interest to the constructor of <span class="in">`r ref("iml::Shapley")`</span>.</span>
<span id="cb44-307"><a href="#cb44-307" aria-hidden="true" tabindex="-1"></a>The exact computation of Shapley values is time consuming, as it involves taking into account all possible combinations of features to calculate the marginal contribution of a feature.</span>
<span id="cb44-308"><a href="#cb44-308" aria-hidden="true" tabindex="-1"></a>Therefore, the estimation of Shapley values is often approximated.</span>
<span id="cb44-309"><a href="#cb44-309" aria-hidden="true" tabindex="-1"></a>The <span class="in">`sample.size`</span> argument (default is <span class="in">`sample.size = 100`</span>) can be increased to obtain a more accurate approximation of exact Shapley values.</span>
<span id="cb44-310"><a href="#cb44-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-311"><a href="#cb44-311" aria-hidden="true" tabindex="-1"></a><span class="in">```{r iml-006}</span></span>
<span id="cb44-312"><a href="#cb44-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3</span></span>
<span id="cb44-313"><a href="#cb44-313" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Shapley values for Charlie. The actual prediction (0.63) displays the prediction of the model for the observation we are interested in, the average prediction (0.71) displays the average prediction over the given test dataset. Each horizontal bar is the Shapley value (phi) for the given feature.</span></span>
<span id="cb44-314"><a href="#cb44-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: 10 bar plots of Shapley values, one for each feature. x-axis says 'phi' and ranges from -0.1 to 0.05. The strongest positive contributions are from the `duration`, `purpose` and `property` variables. The strongest negative contributions are `status`, `amount`, and `savings`.</span></span>
<span id="cb44-315"><a href="#cb44-315" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-iml-shapley</span></span>
<span id="cb44-316"><a href="#cb44-316" aria-hidden="true" tabindex="-1"></a>shapley <span class="ot">=</span> Shapley<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">x.interest =</span> Charlie,</span>
<span id="cb44-317"><a href="#cb44-317" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample.size =</span> <span class="dv">1000</span>)</span>
<span id="cb44-318"><a href="#cb44-318" aria-hidden="true" tabindex="-1"></a>shapley<span class="sc">$</span><span class="fu">plot</span>()</span>
<span id="cb44-319"><a href="#cb44-319" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-320"><a href="#cb44-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-321"><a href="#cb44-321" aria-hidden="true" tabindex="-1"></a>In @fig-iml-shapley, the Shapley values (<span class="in">`phi`</span>) of the features show us how to fairly distribute the difference of Charlie's probability of being creditworthy to the dataset's average probability among the given features.</span>
<span id="cb44-322"><a href="#cb44-322" aria-hidden="true" tabindex="-1"></a>The approximation is sufficiently good if all Shapley values (<span class="in">`phi`</span>) sum up to the difference of the actual prediction and the average prediction.</span>
<span id="cb44-323"><a href="#cb44-323" aria-hidden="true" tabindex="-1"></a>Here, we used <span class="in">`sample.size = 1000`</span> leading to sufficiently good prediction difference of <span class="in">`r round(sum(shapley$results$phi), 3)`</span> between the actual prediction of Charlie (<span class="in">`r round(gbm_predict[,1], 3)`</span>) and the average prediction (<span class="in">`r round(mean(lrn_gbm$predict_newdata(credit_x)$prob[,1]), 3)`</span>).</span>
<span id="cb44-324"><a href="#cb44-324" aria-hidden="true" tabindex="-1"></a>The 'purpose' variable has the most positive effect on the probability of being creditworthy, with an increase in the predicted probability of around 5%.</span>
<span id="cb44-325"><a href="#cb44-325" aria-hidden="true" tabindex="-1"></a>In contrast, the 'status' variable leads to a decrease in the predicted probability of over 10%.</span>
<span id="cb44-326"><a href="#cb44-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-327"><a href="#cb44-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## The counterfactuals Package {#sec-counterfactuals}</span></span>
<span id="cb44-328"><a href="#cb44-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-329"><a href="#cb44-329" aria-hidden="true" tabindex="-1"></a>\index{\texttt{counterfactuals}}<span class="in">`r index('Counterfactual')`</span> explanations try to identify the smallest possible changes to the input features of a given observation that would lead to a different prediction <span class="co">[</span><span class="ot">@Wachter2017</span><span class="co">]</span>.</span>
<span id="cb44-330"><a href="#cb44-330" aria-hidden="true" tabindex="-1"></a>In other words, a counterfactual explanation provides an answer to the question: "What changes in the current feature values are necessary to achieve a different prediction?".</span>
<span id="cb44-331"><a href="#cb44-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-332"><a href="#cb44-332" aria-hidden="true" tabindex="-1"></a>Counterfactual explanations can have many applications in different areas such as healthcare, finance, and criminal justice, where it may be important to understand how small changes in input features could affect the model's prediction.</span>
<span id="cb44-333"><a href="#cb44-333" aria-hidden="true" tabindex="-1"></a>For example, a counterfactual explanation could be used to suggest lifestyle changes to a patient to reduce their risk of developing a particular disease, or to suggest actions that would increase the chance of a credit being approved.</span>
<span id="cb44-334"><a href="#cb44-334" aria-hidden="true" tabindex="-1"></a>For our <span class="in">`tsk("german_credit")`</span> example, we might consider what changes in features would turn a 'bad' credit prediction into a 'good' one (@fig-counterfactuals-ill).</span>
<span id="cb44-335"><a href="#cb44-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-336"><a href="#cb44-336" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-counterfactuals-fig, echo=FALSE}</span></span>
<span id="cb44-337"><a href="#cb44-337" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-counterfactuals-ill</span></span>
<span id="cb44-338"><a href="#cb44-338" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 50%</span></span>
<span id="cb44-339"><a href="#cb44-339" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Illustration of a counterfactual explanation. The real observation (blue, right dot) is predicted to have 'bad' credit. The brown (left) dot is one possible counterfactual that would result in a 'good' credit prediction.</span></span>
<span id="cb44-340"><a href="#cb44-340" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Figure shows a rectangle where bottom left triangle is light blue and labeled 'good' and top right triangle is brown and labeled 'bad'. There is a dot in the 'bad' area and a dot in the 'good' area and an arrow pointing from the 'bad dot' to the 'good dot'. The x-axis is labeled 'duration' and the y-axis is labeled 'amount'.</span></span>
<span id="cb44-341"><a href="#cb44-341" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/counterfactuals.png"</span>, <span class="at">dpi =</span> <span class="dv">600</span>)</span>
<span id="cb44-342"><a href="#cb44-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-343"><a href="#cb44-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-344"><a href="#cb44-344" aria-hidden="true" tabindex="-1"></a>A simple counterfactual method is the <span class="in">`r index('What-If', aside = TRUE, lower = FALSE)`</span> approach <span class="co">[</span><span class="ot">@Wexler2019</span><span class="co">]</span> where, for a given prediction to explain, the counterfactual is the closest data point in the dataset with the desired prediction.</span>
<span id="cb44-345"><a href="#cb44-345" aria-hidden="true" tabindex="-1"></a>Usually, many possible counterfactual data points can exist.</span>
<span id="cb44-346"><a href="#cb44-346" aria-hidden="true" tabindex="-1"></a>However, the approach by @Wexler2019, and several other early counterfactual methods (see @guidotti2022counterfactual for a comprehensive overview), only produce a single, somewhat arbitrary counterfactual explanation, which can be regarded as problematic when counterfactuals are used for insights or actions against the model.</span>
<span id="cb44-347"><a href="#cb44-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-348"><a href="#cb44-348" aria-hidden="true" tabindex="-1"></a>In contrast, the <span class="in">`r index('multi-objective counterfactuals', aside = TRUE)`</span> method (MOC) <span class="co">[</span><span class="ot">@Dandl2020</span><span class="co">]</span> generates multiple artificially-generated counterfactuals that may not be equal to observations in a given dataset.</span>
<span id="cb44-349"><a href="#cb44-349" aria-hidden="true" tabindex="-1"></a>The generation of counterfactuals is based on an optimization problem that aims for counterfactuals that:</span>
<span id="cb44-350"><a href="#cb44-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-351"><a href="#cb44-351" aria-hidden="true" tabindex="-1"></a>1) Have the desired prediction;</span>
<span id="cb44-352"><a href="#cb44-352" aria-hidden="true" tabindex="-1"></a>2) Are close to the observation of interest;</span>
<span id="cb44-353"><a href="#cb44-353" aria-hidden="true" tabindex="-1"></a>3) Only require changes in a few features; and</span>
<span id="cb44-354"><a href="#cb44-354" aria-hidden="true" tabindex="-1"></a>4) Originate from the same distribution as the observations in the given dataset.</span>
<span id="cb44-355"><a href="#cb44-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-356"><a href="#cb44-356" aria-hidden="true" tabindex="-1"></a>In MOC, all four objectives are optimized simultaneously via a multi-objective optimization method.</span>
<span id="cb44-357"><a href="#cb44-357" aria-hidden="true" tabindex="-1"></a>Several other counterfactual methods rely on single-objective optimization methods, where multiple objectives are combined into a single objective, e.g., using a weighted sum.</span>
<span id="cb44-358"><a href="#cb44-358" aria-hidden="true" tabindex="-1"></a>However, a single-objective approach raises concerns about the appropriate weighting of objectives and is unable to account for inherent trade-offs among individual objectives.</span>
<span id="cb44-359"><a href="#cb44-359" aria-hidden="true" tabindex="-1"></a>Moreover, it may restrict the solution set of the counterfactural search to a single candidate.</span>
<span id="cb44-360"><a href="#cb44-360" aria-hidden="true" tabindex="-1"></a>MOC returns a set of non-dominated and, therefore equally good, counterfactuals with respect to the four objectives (similarly to the <span class="in">`r index('Pareto front', lower = FALSE)`</span> we saw in @sec-multi-metrics-tuning).</span>
<span id="cb44-361"><a href="#cb44-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-362"><a href="#cb44-362" aria-hidden="true" tabindex="-1"></a>Counterfactual explanations are available in the <span class="in">`counterfactuals`</span> package, which depends on <span class="in">`r ref("iml::Predictor")`</span> objects as inputs.</span>
<span id="cb44-363"><a href="#cb44-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-364"><a href="#cb44-364" aria-hidden="true" tabindex="-1"></a><span class="fu">### What-If Method</span></span>
<span id="cb44-365"><a href="#cb44-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-366"><a href="#cb44-366" aria-hidden="true" tabindex="-1"></a>Continuing our previous example, we saw that the GBM model classifies Charlie as having <span class="in">`r predictor$class`</span> credit with a predicted probability of <span class="in">`r round(max(predictor$predict(Charlie))*100, 1)`</span>%.</span>
<span id="cb44-367"><a href="#cb44-367" aria-hidden="true" tabindex="-1"></a>We can use the What-If method to understand how the features need to change for this predicted probability to increase to 75%.</span>
<span id="cb44-368"><a href="#cb44-368" aria-hidden="true" tabindex="-1"></a>We initialize a <span class="in">`r ref("counterfactuals::WhatIfClassif")`</span> object with our <span class="in">`Predictor`</span> and state that we only want to find one counterfactual (<span class="in">`n_counterfactuals = 1L`</span>), increasing <span class="in">`n_counterfactuals`</span> would return the specified number of counterfactuals closest to the point of interest.</span>
<span id="cb44-369"><a href="#cb44-369" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$find_counterfactuals()`</span> method generates a counterfactual of class  <span class="in">`r ref("counterfactuals::Counterfactuals")`</span>, below we set our desired predicted probability to be between <span class="in">`0.75`</span> and <span class="in">`1`</span> (<span class="in">`desired_prob = c(0.75, 1)`</span>).</span>
<span id="cb44-370"><a href="#cb44-370" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$evaluate(show_diff = TRUE)`</span> method tells us how features need to be changed to generate our desired class.</span>
<span id="cb44-371"><a href="#cb44-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-372"><a href="#cb44-372" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-whatif}</span></span>
<span id="cb44-373"><a href="#cb44-373" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(counterfactuals)</span>
<span id="cb44-374"><a href="#cb44-374" aria-hidden="true" tabindex="-1"></a>whatif <span class="ot">=</span> WhatIfClassif<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">n_counterfactuals =</span> 1L)</span>
<span id="cb44-375"><a href="#cb44-375" aria-hidden="true" tabindex="-1"></a>cfe <span class="ot">=</span> whatif<span class="sc">$</span><span class="fu">find_counterfactuals</span>(Charlie,</span>
<span id="cb44-376"><a href="#cb44-376" aria-hidden="true" tabindex="-1"></a>  <span class="at">desired_class =</span> <span class="st">"good"</span>, <span class="at">desired_prob =</span> <span class="fu">c</span>(<span class="fl">0.75</span>, <span class="dv">1</span>))</span>
<span id="cb44-377"><a href="#cb44-377" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(cfe<span class="sc">$</span><span class="fu">evaluate</span>(<span class="at">show_diff =</span> <span class="cn">TRUE</span>))</span>
<span id="cb44-378"><a href="#cb44-378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-379"><a href="#cb44-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-380"><a href="#cb44-380" aria-hidden="true" tabindex="-1"></a>Here we can see that, to achieve a predicted probability of at least 75% for good credit, Charlie would have to be three years younger, the duration of credit would have to be reduced by three months, the amount would have to be increased by 1417 DM and the status would have to be '... &lt; 0 DM' (instead of 'no checking account') .</span>
<span id="cb44-381"><a href="#cb44-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-382"><a href="#cb44-382" aria-hidden="true" tabindex="-1"></a><span class="fu">### MOC Method</span></span>
<span id="cb44-383"><a href="#cb44-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-384"><a href="#cb44-384" aria-hidden="true" tabindex="-1"></a>Calling the MOC method is similar to the What-If method but with a <span class="in">`r ref("counterfactuals::MOCClassif()")`</span> object.</span>
<span id="cb44-385"><a href="#cb44-385" aria-hidden="true" tabindex="-1"></a>We set the <span class="in">`epsilon`</span> parameter to 0 to penalize counterfactuals in the optimization process with predictions outside the desired range.</span>
<span id="cb44-386"><a href="#cb44-386" aria-hidden="true" tabindex="-1"></a>With MOC, we can also prohibit changes in specific features via the <span class="in">`fixed_features`</span> argument, below we restrict changes in the 'age' variable.</span>
<span id="cb44-387"><a href="#cb44-387" aria-hidden="true" tabindex="-1"></a>For illustrative purposes, we only run the multi-objective optimizer for 30 generations.</span>
<span id="cb44-388"><a href="#cb44-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-389"><a href="#cb44-389" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-mocmulti,message=FALSE}</span></span>
<span id="cb44-390"><a href="#cb44-390" aria-hidden="true" tabindex="-1"></a>moc <span class="ot">=</span> MOCClassif<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">epsilon =</span> <span class="dv">0</span>, <span class="at">n_generations =</span> 30L,</span>
<span id="cb44-391"><a href="#cb44-391" aria-hidden="true" tabindex="-1"></a>  <span class="at">fixed_features =</span> <span class="st">"age"</span>)</span>
<span id="cb44-392"><a href="#cb44-392" aria-hidden="true" tabindex="-1"></a>cfe_multi <span class="ot">=</span> moc<span class="sc">$</span><span class="fu">find_counterfactuals</span>(Charlie,</span>
<span id="cb44-393"><a href="#cb44-393" aria-hidden="true" tabindex="-1"></a>  <span class="at">desired_class =</span> <span class="st">"good"</span>, <span class="at">desired_prob =</span> <span class="fu">c</span>(<span class="fl">0.75</span>, <span class="dv">1</span>))</span>
<span id="cb44-394"><a href="#cb44-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-395"><a href="#cb44-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-396"><a href="#cb44-396" aria-hidden="true" tabindex="-1"></a>The multi-objective approach does not guarantee that all counterfactuals have the desired prediction so we use <span class="in">`$subset_to_valid()`</span> to restrict counterfactuals to those we are interested in:</span>
<span id="cb44-397"><a href="#cb44-397" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-mocmulti-subset}</span></span>
<span id="cb44-398"><a href="#cb44-398" aria-hidden="true" tabindex="-1"></a>cfe_multi<span class="sc">$</span><span class="fu">subset_to_valid</span>()</span>
<span id="cb44-399"><a href="#cb44-399" aria-hidden="true" tabindex="-1"></a>cfe_multi</span>
<span id="cb44-400"><a href="#cb44-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-401"><a href="#cb44-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-402"><a href="#cb44-402" aria-hidden="true" tabindex="-1"></a>This method generated <span class="in">`r nrow(cfe_multi$data)`</span> counterfactuals but as these are artificially generated they are not necessarily equal to actual observations in the underlying dataset.</span>
<span id="cb44-403"><a href="#cb44-403" aria-hidden="true" tabindex="-1"></a>For a concise overview of the required feature changes, we can use the <span class="in">`plot_freq_of_feature_changes()`</span> method, which visualizes the frequency of feature changes across all returned counterfactuals.</span>
<span id="cb44-404"><a href="#cb44-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-405"><a href="#cb44-405" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-mocfreq}</span></span>
<span id="cb44-406"><a href="#cb44-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3.5</span></span>
<span id="cb44-407"><a href="#cb44-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Barplots of the relative frequency of feature changes of the counterfactuals found by MOC.</span></span>
<span id="cb44-408"><a href="#cb44-408" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: x-axis says 'relative frequency' and ranges from 0 to just over 0.3. Changed features were 'status' (in 35% of the counterfactuals), 'savings' (35%), 'purpose' (10%), 'employment_duration' (10%), 'duration' (10%), and 'amount' (10%).</span></span>
<span id="cb44-409"><a href="#cb44-409" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cf-mocfreq</span></span>
<span id="cb44-410"><a href="#cb44-410" aria-hidden="true" tabindex="-1"></a>cfe_multi<span class="sc">$</span><span class="fu">plot_freq_of_feature_changes</span>()</span>
<span id="cb44-411"><a href="#cb44-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-412"><a href="#cb44-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-413"><a href="#cb44-413" aria-hidden="true" tabindex="-1"></a>We can see that 'status' and 'savings' were changed most frequently in the counterfactuals.</span>
<span id="cb44-414"><a href="#cb44-414" aria-hidden="true" tabindex="-1"></a>To see *how* the features were changed, we can visualize the counterfactuals for two features on a two-dimensional ICE plot.</span>
<span id="cb44-415"><a href="#cb44-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-416"><a href="#cb44-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-mocsurface}</span></span>
<span id="cb44-417"><a href="#cb44-417" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3.5</span></span>
<span id="cb44-418"><a href="#cb44-418" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Two-dimensional surface plot for the 'status' and 'savings' variables, higher predictions are lighter. The colors and contour lines indicate the predicted value of the model when 'status' and 'savings' differ while all other features are set to the true (Charlie's) values. The white point displays the true prediction (Charlie), and the black points are the counterfactuals that only propose changes in the two features.</span></span>
<span id="cb44-419"><a href="#cb44-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Surface plot that is primarily light blue when status is positive and dark blue when status is negative. y-axis is the 'savings' variable and x-axis is the 'status' variable. There is a white dot in the bottom left corner at (status = 'no checking account', savings = unknown/no savings account'). Two black dots are in a straight line above the white dot and two black dots are in a roughly straight line to the right of the white dot.</span></span>
<span id="cb44-420"><a href="#cb44-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cf-mocsurface</span></span>
<span id="cb44-421"><a href="#cb44-421" aria-hidden="true" tabindex="-1"></a>cfe_multi<span class="sc">$</span><span class="fu">plot_surface</span>(<span class="at">feature_names =</span> <span class="fu">c</span>(<span class="st">"status"</span>, <span class="st">"savings"</span>)) <span class="sc">+</span></span>
<span id="cb44-422"><a href="#cb44-422" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">15</span>, <span class="at">hjust =</span> .<span class="dv">7</span>))</span>
<span id="cb44-423"><a href="#cb44-423" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-424"><a href="#cb44-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-425"><a href="#cb44-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## The `DALEX` Package {#sec-dalex}</span></span>
<span id="cb44-426"><a href="#cb44-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-427"><a href="#cb44-427" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref_pkg("DALEX")`</span>\index{\texttt{DALEX}} <span class="co">[</span><span class="ot">@Biecek2018</span><span class="co">]</span> implements a similar set of methods as <span class="in">`iml`</span>, but the architecture of <span class="in">`DALEX`</span> is oriented towards model comparison.</span>
<span id="cb44-428"><a href="#cb44-428" aria-hidden="true" tabindex="-1"></a>The logic behind working with this package assumes that the process of exploring models is iterative, and in successive iterations, we want to compare different perspectives, including perspectives presented/learned by different models.</span>
<span id="cb44-429"><a href="#cb44-429" aria-hidden="true" tabindex="-1"></a>This logic is commonly referred to as the <span class="in">`r index('Rashomon', lower = FALSE)`</span> perspective, first described in @Breiman2001 and more extensively developed and formalized as interactive explanatory model analysis <span class="co">[</span><span class="ot">@Baniecki2023</span><span class="co">]</span>.</span>
<span id="cb44-430"><a href="#cb44-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-431"><a href="#cb44-431" aria-hidden="true" tabindex="-1"></a>You can use the <span class="in">`DALEX`</span> package with any classification and regression model built with <span class="in">`mlr3`</span> as well as with other frameworks in R.</span>
<span id="cb44-432"><a href="#cb44-432" aria-hidden="true" tabindex="-1"></a>As we have already explored the methodology behind most of the methods discussed in this section, we will just focus on the implementations of these methods in <span class="in">`DALEX`</span> using the <span class="in">`tsk("german_credit")`</span> running example.</span>
<span id="cb44-433"><a href="#cb44-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-434"><a href="#cb44-434" aria-hidden="true" tabindex="-1"></a>Once you become familiar with the philosophy of working with the <span class="in">`DALEX`</span> package, you can use other packages from this family such as <span class="in">`r ref_pkg("fairmodels")`</span> <span class="co">[</span><span class="ot">@Wisniewski2022</span><span class="co">]</span> for detection and mitigation of biases, <span class="in">`r ref_pkg("modelStudio")`</span> <span class="co">[</span><span class="ot">@Baniecki2019</span><span class="co">]</span> for interactive model exploration, <span class="in">`r ref_pkg("modelDown")`</span> <span class="co">[</span><span class="ot">@Romaszko2019</span><span class="co">]</span> for the automatic generation of IML model documentation, <span class="in">`r ref_pkg("survex")`</span> <span class="co">[</span><span class="ot">@Krzyzinski2023</span><span class="co">]</span> for the explanation of survival models, or <span class="in">`r ref_pkg("treeshap")`</span> for the analysis of tree-based models.</span>
<span id="cb44-435"><a href="#cb44-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-436"><a href="#cb44-436" aria-hidden="true" tabindex="-1"></a>The analysis of a model is usually an interactive process starting with evaluating a model based on one or more performance metrics, known as a 'shallow analysis'.</span>
<span id="cb44-437"><a href="#cb44-437" aria-hidden="true" tabindex="-1"></a>In a series of subsequent steps, one can systematically deepen understanding of the model by exploring the importance of single variables or pairs of variables to an in-depth analysis of the relationship between selected variables to the model outcome.</span>
<span id="cb44-438"><a href="#cb44-438" aria-hidden="true" tabindex="-1"></a>See @Bucker2022 for a broader discussion of what the model exploration process looks like.</span>
<span id="cb44-439"><a href="#cb44-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-440"><a href="#cb44-440" aria-hidden="true" tabindex="-1"></a>This <span class="in">`r index('explanatory model analysis', aside = TRUE)`</span> (EMA) process can focus on a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we refer to global model analysis.</span>
<span id="cb44-441"><a href="#cb44-441" aria-hidden="true" tabindex="-1"></a>@fig-dalex-fig-plot-01 visualizes an overview of the key functions in these two scenarios that we will discuss in this section.</span>
<span id="cb44-442"><a href="#cb44-442" aria-hidden="true" tabindex="-1"></a>An in-depth description of this methodology can be found in @biecek_burzykowski_2021.</span>
<span id="cb44-443"><a href="#cb44-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-444"><a href="#cb44-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-012, echo=FALSE}</span></span>
<span id="cb44-445"><a href="#cb44-445" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-fig-plot-01</span></span>
<span id="cb44-446"><a href="#cb44-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 92%</span></span>
<span id="cb44-447"><a href="#cb44-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Taxonomy of methods for model exploration presented in this section. The left side shows global analysis methods and the right shows local analysis methods. Methods increase in analysis complexity from top to bottom.</span></span>
<span id="cb44-448"><a href="#cb44-448" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Title says 'Explanatory Model Analysis', just below that in code font says 'DALEX::explain()'. Far left side is an arrow pointing upwards labeled 'Shallow' and one pointing down labeled 'Deep'. To the right of these arrows is the text 'Global Analysis' with an arrow pointing down to 'Model Performance, AUC, RMSE; DALEX::model_performance()', which has an arrow pointing down to 'Feature Importance, VIP; DALEX::model_parts()', which has an arrow pointing down to 'Feature Profiles, PD, ALE; DALEX::model_profile()'. To the right of 'Global Analysis' is the text 'Local Analysis', which has an arrow pointing to 'Model Predict; DALEX::predict()', which has an arrow pointing down to 'Feature Attributions, SHAP, BD; DALEX::predict_parts()', which has an arrow pointing down to 'Feature Profiles, Ceteris Paribus; DALEX::predict_profile()'."</span></span>
<span id="cb44-449"><a href="#cb44-449" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/DALEX_ema_process.png"</span>)</span>
<span id="cb44-450"><a href="#cb44-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-451"><a href="#cb44-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-452"><a href="#cb44-452" aria-hidden="true" tabindex="-1"></a>As with <span class="in">`iml`</span>, <span class="in">`DALEX`</span> also implements a wrapper that enables a unified interface to its functionality.</span>
<span id="cb44-453"><a href="#cb44-453" aria-hidden="true" tabindex="-1"></a>For models created with the <span class="in">`mlr3`</span> package, we would use <span class="in">`r ref("DALEXtra::explain_mlr3()")`</span>, which creates an S3 <span class="in">`explainer`</span> object, which is a list containing at least: the model object, the dataset that will be used for calculation of explanations, the predict function, the function that calculates residuals, name/label of the model name and other additional information about the model.</span>
<span id="cb44-454"><a href="#cb44-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-455"><a href="#cb44-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-019, eval=FALSE}</span></span>
<span id="cb44-456"><a href="#cb44-456" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DALEX)</span>
<span id="cb44-457"><a href="#cb44-457" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DALEXtra)</span>
<span id="cb44-458"><a href="#cb44-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-459"><a href="#cb44-459" aria-hidden="true" tabindex="-1"></a>gbm_exp <span class="ot">=</span> DALEXtra<span class="sc">::</span><span class="fu">explain_mlr3</span>(lrn_gbm,</span>
<span id="cb44-460"><a href="#cb44-460" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> credit_x,</span>
<span id="cb44-461"><a href="#cb44-461" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">as.numeric</span>(credit_y<span class="sc">$</span>credit_risk <span class="sc">==</span> <span class="st">"bad"</span>),</span>
<span id="cb44-462"><a href="#cb44-462" aria-hidden="true" tabindex="-1"></a>  <span class="at">label =</span> <span class="st">"GBM Credit"</span>,</span>
<span id="cb44-463"><a href="#cb44-463" aria-hidden="true" tabindex="-1"></a>  <span class="at">colorize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-464"><a href="#cb44-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-465"><a href="#cb44-465" aria-hidden="true" tabindex="-1"></a>gbm_exp</span>
<span id="cb44-466"><a href="#cb44-466" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-467"><a href="#cb44-467" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, results='hide', echo=FALSE, include=FALSE}</span></span>
<span id="cb44-468"><a href="#cb44-468" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DALEX)</span>
<span id="cb44-469"><a href="#cb44-469" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DALEXtra)</span>
<span id="cb44-470"><a href="#cb44-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-471"><a href="#cb44-471" aria-hidden="true" tabindex="-1"></a>gbm_exp <span class="ot">=</span> DALEXtra<span class="sc">::</span><span class="fu">explain_mlr3</span>(lrn_gbm,</span>
<span id="cb44-472"><a href="#cb44-472" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> credit_x,</span>
<span id="cb44-473"><a href="#cb44-473" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">as.numeric</span>(credit_y<span class="sc">$</span>credit_risk <span class="sc">==</span> <span class="st">"bad"</span>),</span>
<span id="cb44-474"><a href="#cb44-474" aria-hidden="true" tabindex="-1"></a>  <span class="at">label =</span> <span class="st">"GBM Credit"</span>,</span>
<span id="cb44-475"><a href="#cb44-475" aria-hidden="true" tabindex="-1"></a>  <span class="at">colorize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-476"><a href="#cb44-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-477"><a href="#cb44-477" aria-hidden="true" tabindex="-1"></a>gbm_exp</span>
<span id="cb44-478"><a href="#cb44-478" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-479"><a href="#cb44-479" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE}</span></span>
<span id="cb44-480"><a href="#cb44-480" aria-hidden="true" tabindex="-1"></a>gbm_exp</span>
<span id="cb44-481"><a href="#cb44-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-482"><a href="#cb44-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-483"><a href="#cb44-483" aria-hidden="true" tabindex="-1"></a><span class="fu">### Global EMA {#sec-interpretability-dataset-level}</span></span>
<span id="cb44-484"><a href="#cb44-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-485"><a href="#cb44-485" aria-hidden="true" tabindex="-1"></a>Global EMA aims to understand how a model behaves on average for a set of observations.</span>
<span id="cb44-486"><a href="#cb44-486" aria-hidden="true" tabindex="-1"></a>In <span class="in">`DALEX`</span>, functions for global level analysis are prefixed with <span class="in">`model_`</span>.</span>
<span id="cb44-487"><a href="#cb44-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-488"><a href="#cb44-488" aria-hidden="true" tabindex="-1"></a>The model exploration process starts (@fig-dalex-fig-plot-01) by evaluating the performance of a model.</span>
<span id="cb44-489"><a href="#cb44-489" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("DALEX::model_performance()")`</span> detects the task type and selects the most appropriate measure, as we are using binary classification the function automatically suggests recall, precision, F1-score, accuracy, and AUC; similarly the default plotting method is selected based on the task type, below ROC is selected.</span>
<span id="cb44-490"><a href="#cb44-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-491"><a href="#cb44-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-020a}</span></span>
<span id="cb44-492"><a href="#cb44-492" aria-hidden="true" tabindex="-1"></a>perf_credit <span class="ot">=</span> <span class="fu">model_performance</span>(gbm_exp)</span>
<span id="cb44-493"><a href="#cb44-493" aria-hidden="true" tabindex="-1"></a>perf_credit</span>
<span id="cb44-494"><a href="#cb44-494" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-495"><a href="#cb44-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-498"><a href="#cb44-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-499"><a href="#cb44-499" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-500"><a href="#cb44-500" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb44-501"><a href="#cb44-501" aria-hidden="true" tabindex="-1"></a>old_theme <span class="ot">=</span> <span class="fu">set_theme_dalex</span>(<span class="st">"ema"</span>)</span>
<span id="cb44-502"><a href="#cb44-502" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(perf_credit, <span class="at">geom =</span> <span class="st">"roc"</span>)</span>
<span id="cb44-503"><a href="#cb44-503" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-504"><a href="#cb44-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-507"><a href="#cb44-507" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-508"><a href="#cb44-508" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb44-509"><a href="#cb44-509" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 5</span></span>
<span id="cb44-510"><a href="#cb44-510" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-roc</span></span>
<span id="cb44-511"><a href="#cb44-511" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 60%</span></span>
<span id="cb44-512"><a href="#cb44-512" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Graphical summary of model performance using the Receiver Operator Curve (@sec-roc).</span></span>
<span id="cb44-513"><a href="#cb44-513" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: ROC curve with 'True positive rate' on the y-axis and 'False positive rate' on the x-axis, curve shows reasonably good model fit as it sits comfortably in the upper left diagonal.</span></span>
<span id="cb44-514"><a href="#cb44-514" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb44-515"><a href="#cb44-515" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb44-516"><a href="#cb44-516" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb44-517"><a href="#cb44-517" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb44-518"><a href="#cb44-518" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> plt <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_color_grey</span>()</span>
<span id="cb44-519"><a href="#cb44-519" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(plt)</span>
<span id="cb44-520"><a href="#cb44-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-521"><a href="#cb44-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-522"><a href="#cb44-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-523"><a href="#cb44-523" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-524"><a href="#cb44-524" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visual Summaries</span></span>
<span id="cb44-525"><a href="#cb44-525" aria-hidden="true" tabindex="-1"></a>Various visual summaries may be selected with the <span class="in">`geom`</span> parameter.</span>
<span id="cb44-526"><a href="#cb44-526" aria-hidden="true" tabindex="-1"></a>For the credit risk task, the LIFT curve is a popular graphical summary.</span>
<span id="cb44-527"><a href="#cb44-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-528"><a href="#cb44-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-529"><a href="#cb44-529" aria-hidden="true" tabindex="-1"></a>Feature importance methods can be calculated with <span class="in">`r ref("DALEX::model_parts()")`</span> and then plotted.</span>
<span id="cb44-530"><a href="#cb44-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-531"><a href="#cb44-531" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-021}</span></span>
<span id="cb44-532"><a href="#cb44-532" aria-hidden="true" tabindex="-1"></a>gbm_effect <span class="ot">=</span> <span class="fu">model_parts</span>(gbm_exp)</span>
<span id="cb44-533"><a href="#cb44-533" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(gbm_effect)</span>
<span id="cb44-534"><a href="#cb44-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-535"><a href="#cb44-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-538"><a href="#cb44-538" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-539"><a href="#cb44-539" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-540"><a href="#cb44-540" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb44-541"><a href="#cb44-541" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gbm_effect, <span class="at">show_boxplots =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-542"><a href="#cb44-542" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-543"><a href="#cb44-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-546"><a href="#cb44-546" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-547"><a href="#cb44-547" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb44-548"><a href="#cb44-548" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb44-549"><a href="#cb44-549" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 90%</span></span>
<span id="cb44-550"><a href="#cb44-550" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Graphical summary of permutation importance of features. The longer the bar, the larger the change in the loss function after permutation of the particular feature and therefore the more important the feature. This plot shows that 'status' is the most important feature and 'other_debtors' is the least important.</span></span>
<span id="cb44-551"><a href="#cb44-551" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Feature importance plot. x-axis label is 'One minus AUC loss after permutations', y-axis labels are features. Horizontal bars range from 0.24 to 0.35.</span></span>
<span id="cb44-552"><a href="#cb44-552" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-featimp</span></span>
<span id="cb44-553"><a href="#cb44-553" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb44-554"><a href="#cb44-554" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb44-555"><a href="#cb44-555" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb44-556"><a href="#cb44-556" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb44-557"><a href="#cb44-557" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> plt <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_color_grey</span>()</span>
<span id="cb44-558"><a href="#cb44-558" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(plt)</span>
<span id="cb44-559"><a href="#cb44-559" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-560"><a href="#cb44-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-561"><a href="#cb44-561" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-562"><a href="#cb44-562" aria-hidden="true" tabindex="-1"></a><span class="fu">## Calculating Importance</span></span>
<span id="cb44-563"><a href="#cb44-563" aria-hidden="true" tabindex="-1"></a>The <span class="in">`type`</span> argument in the <span class="in">`model_parts`</span> function allows you to specify how the importance of the features is to be calculated, by the difference of the loss functions (<span class="in">`type = "difference"`</span>), by the quotient (<span class="in">`type = "ratio"`</span>), or without any transformation (<span class="in">`type = "raw"`</span>).</span>
<span id="cb44-564"><a href="#cb44-564" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-565"><a href="#cb44-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-566"><a href="#cb44-566" aria-hidden="true" tabindex="-1"></a>Feature effects can be calculated with <span class="in">`r ref("DALEX::model_profile()")`</span> and by default are plotted as PD plots.</span>
<span id="cb44-567"><a href="#cb44-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-568"><a href="#cb44-568" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-024, warning=FALSE}</span></span>
<span id="cb44-569"><a href="#cb44-569" aria-hidden="true" tabindex="-1"></a>gbm_profiles <span class="ot">=</span> <span class="fu">model_profile</span>(gbm_exp)</span>
<span id="cb44-570"><a href="#cb44-570" aria-hidden="true" tabindex="-1"></a>gbm_profiles</span>
<span id="cb44-571"><a href="#cb44-571" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-572"><a href="#cb44-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-575"><a href="#cb44-575" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-576"><a href="#cb44-576" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb44-577"><a href="#cb44-577" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: false</span></span>
<span id="cb44-578"><a href="#cb44-578" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gbm_profiles) <span class="sc">+</span></span>
<span id="cb44-579"><a href="#cb44-579" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>) <span class="sc">+</span></span>
<span id="cb44-580"><a href="#cb44-580" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Partial Dependence for GBM Credit model"</span>,<span class="st">""</span>)</span>
<span id="cb44-581"><a href="#cb44-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-582"><a href="#cb44-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-585"><a href="#cb44-585" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-586"><a href="#cb44-586" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb44-587"><a href="#cb44-587" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb44-588"><a href="#cb44-588" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 90%</span></span>
<span id="cb44-589"><a href="#cb44-589" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-pdp</span></span>
<span id="cb44-590"><a href="#cb44-590" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Graphical summary of the model's partial dependence profile for three selected variables (age, amount, duration).</span></span>
<span id="cb44-591"><a href="#cb44-591" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Left plot is PD plot of 'age' against 'average prediction', between ages 20-40 the prediction dips from 0.35 to 0.3 then is flat. Middle plot is PD plot of 'amount', between amounts 0-5000 the prediction starts at 0.3 then spikes briefly then returns to 0.3, then between 5000-15000 the plot slowly increases to 0.5. Right plot is PD plot of 'duration', between duration 0-40 the prediction linearly increases from 0.2 to 0.45 then stays flat.</span></span>
<span id="cb44-592"><a href="#cb44-592" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb44-593"><a href="#cb44-593" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb44-594"><a href="#cb44-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb44-595"><a href="#cb44-595" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb44-596"><a href="#cb44-596" aria-hidden="true" tabindex="-1"></a>plt<span class="sc">$</span>layers[[<span class="dv">1</span>]]<span class="sc">$</span>aes_params<span class="sc">$</span>colour <span class="ot">=</span> <span class="st">"grey30"</span></span>
<span id="cb44-597"><a href="#cb44-597" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(plt)</span>
<span id="cb44-598"><a href="#cb44-598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-599"><a href="#cb44-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-600"><a href="#cb44-600" aria-hidden="true" tabindex="-1"></a>From @fig-dalex-pdp, we can see that the GBM model has learned a non-monotonic relationship for the feature <span class="in">`amount`</span>.</span>
<span id="cb44-601"><a href="#cb44-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-602"><a href="#cb44-602" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-603"><a href="#cb44-603" aria-hidden="true" tabindex="-1"></a><span class="fu">## Marginal and Accumulated Local Profiles</span></span>
<span id="cb44-604"><a href="#cb44-604" aria-hidden="true" tabindex="-1"></a>The <span class="in">`type`</span> argument of the <span class="in">`r ref("DALEX::model_profile()")`</span> function also allows *marginal profiles* (with `type = "conditional"`) and *accumulated local profiles* (with <span class="in">`type = "accumulated"`</span>) to be calculated.</span>
<span id="cb44-605"><a href="#cb44-605" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-606"><a href="#cb44-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-607"><a href="#cb44-607" aria-hidden="true" tabindex="-1"></a><span class="fu">### Local EMA {#sec-interpretability-instance-level}</span></span>
<span id="cb44-608"><a href="#cb44-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-609"><a href="#cb44-609" aria-hidden="true" tabindex="-1"></a>Local EMA aims to understand how a model behaves for a single observation.</span>
<span id="cb44-610"><a href="#cb44-610" aria-hidden="true" tabindex="-1"></a>In <span class="in">`DALEX`</span>, functions for local analysis are prefixed with <span class="in">`predict_`</span>.</span>
<span id="cb44-611"><a href="#cb44-611" aria-hidden="true" tabindex="-1"></a>We will carry out the following examples using Charlie again.</span>
<span id="cb44-612"><a href="#cb44-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-613"><a href="#cb44-613" aria-hidden="true" tabindex="-1"></a>Local analysis starts with the calculation of a model prediction (@fig-dalex-fig-plot-01).</span>
<span id="cb44-614"><a href="#cb44-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-615"><a href="#cb44-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-025}</span></span>
<span id="cb44-616"><a href="#cb44-616" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(gbm_exp, Charlie)</span>
<span id="cb44-617"><a href="#cb44-617" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-618"><a href="#cb44-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-619"><a href="#cb44-619" aria-hidden="true" tabindex="-1"></a>As a next step, we might consider break-down plots, which decompose the model's prediction into contributions that can be attributed to different explanatory variables (see the *Break-down Plots for Additive Attributions* chapter in @biecek_burzykowski_2021 for more on this method).</span>
<span id="cb44-620"><a href="#cb44-620" aria-hidden="true" tabindex="-1"></a>These are calculated with <span class="in">`r ref("DALEX::predict_parts()")`</span>:</span>
<span id="cb44-621"><a href="#cb44-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-622"><a href="#cb44-622" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-027}</span></span>
<span id="cb44-623"><a href="#cb44-623" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4.5</span></span>
<span id="cb44-624"><a href="#cb44-624" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb44-625"><a href="#cb44-625" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 90%</span></span>
<span id="cb44-626"><a href="#cb44-626" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Graphical summary of local attributions of features calculated by the break-down method. Positive attributions are shown in green and negative attributions in red. The violet bar corresponds to the model prediction for the explained observation and the dashed line corresponds to the average model prediction.</span></span>
<span id="cb44-627"><a href="#cb44-627" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: On the x-axis are numbers from 0.2 to 0.5, and y-axis is variables from the dataset. There are four bars in red with negative number labels and five bars in green with positive number labels. A dashed vertical lines runs through x=0.3 and there is a violet bar with text '0.365'.</span></span>
<span id="cb44-628"><a href="#cb44-628" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-breakdown</span></span>
<span id="cb44-629"><a href="#cb44-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-630"><a href="#cb44-630" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict_parts</span>(gbm_exp, <span class="at">new_observation =</span> Charlie))</span>
<span id="cb44-631"><a href="#cb44-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-632"><a href="#cb44-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-633"><a href="#cb44-633" aria-hidden="true" tabindex="-1"></a>Looking at @fig-dalex-breakdown, we can read that the biggest contributors to the final prediction for Charlie were the features <span class="in">`status`</span> and <span class="in">`savings`</span>.</span>
<span id="cb44-634"><a href="#cb44-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-635"><a href="#cb44-635" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-636"><a href="#cb44-636" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selected Order of Features</span></span>
<span id="cb44-637"><a href="#cb44-637" aria-hidden="true" tabindex="-1"></a>The <span class="in">`order`</span> argument allows you to indicate the selected order of the features.</span>
<span id="cb44-638"><a href="#cb44-638" aria-hidden="true" tabindex="-1"></a>This is a useful option when the features have some relative conditional importance (e.g. pregnancy and sex).</span>
<span id="cb44-639"><a href="#cb44-639" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-640"><a href="#cb44-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-641"><a href="#cb44-641" aria-hidden="true" tabindex="-1"></a>The <span class="in">`predict_parts()`</span> function can also be used to plot Shapley values with the SHAP algorithm <span class="co">[</span><span class="ot">@Lundberg2019</span><span class="co">]</span> by setting <span class="in">`type = "shap"`</span>:</span>
<span id="cb44-642"><a href="#cb44-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-643"><a href="#cb44-643" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-028}</span></span>
<span id="cb44-644"><a href="#cb44-644" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4.5</span></span>
<span id="cb44-645"><a href="#cb44-645" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb44-646"><a href="#cb44-646" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 90%</span></span>
<span id="cb44-647"><a href="#cb44-647" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Graphical summary of local attributions of features calculated by the Shap method. Positive attributions are shown in green and negative attributions in red. The most important feature here is the 'status' variable and least is 'other_debtors'.</span></span>
<span id="cb44-648"><a href="#cb44-648" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: x-axis says 'contribution' and ranges from -0.05 to 0.1, y-axis is feature names. Plots show four red bars with negative contributions and five green bars making positive contributions. Longest bar is for 'status' and shortest for 'other_debtors'.</span></span>
<span id="cb44-649"><a href="#cb44-649" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-shaps</span></span>
<span id="cb44-650"><a href="#cb44-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-651"><a href="#cb44-651" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict_parts</span>(gbm_exp, <span class="at">new_observation =</span> Charlie, <span class="at">type =</span> <span class="st">"shap"</span>),</span>
<span id="cb44-652"><a href="#cb44-652" aria-hidden="true" tabindex="-1"></a>  <span class="at">show_boxplots =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-653"><a href="#cb44-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-654"><a href="#cb44-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-655"><a href="#cb44-655" aria-hidden="true" tabindex="-1"></a>The results for Break Down and SHAP methods are generally similar. Differences will emerge if there are many complex interactions in the model.</span>
<span id="cb44-656"><a href="#cb44-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-657"><a href="#cb44-657" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb44-658"><a href="#cb44-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## Speeding Up Shapley Computation</span></span>
<span id="cb44-659"><a href="#cb44-659" aria-hidden="true" tabindex="-1"></a>Shapley values can take a long time to compute.</span>
<span id="cb44-660"><a href="#cb44-660" aria-hidden="true" tabindex="-1"></a>This process can be sped up at the expense of accuracy.</span>
<span id="cb44-661"><a href="#cb44-661" aria-hidden="true" tabindex="-1"></a>The parameters <span class="in">`B`</span> and <span class="in">`N`</span> can be used to tune this trade-off, where <span class="in">`N`</span> is the number of observations on which conditional expectation values are estimated (500 by default) and <span class="in">`B`</span> is the number of random paths used to calculate Shapley values (25 by default).</span>
<span id="cb44-662"><a href="#cb44-662" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb44-663"><a href="#cb44-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-664"><a href="#cb44-664" aria-hidden="true" tabindex="-1"></a>Finally, we can plot ICE curves using <span class="in">`r ref("DALEX::predict_profile()")`</span>:</span>
<span id="cb44-665"><a href="#cb44-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-666"><a href="#cb44-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{r interpretation-029, warning=FALSE}</span></span>
<span id="cb44-667"><a href="#cb44-667" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb44-668"><a href="#cb44-668" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb44-669"><a href="#cb44-669" aria-hidden="true" tabindex="-1"></a><span class="co">#| out-width: 90%</span></span>
<span id="cb44-670"><a href="#cb44-670" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-dalex-ice</span></span>
<span id="cb44-671"><a href="#cb44-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Individual conditional explanations (aka Ceteris Paribus) plots for 10 rows in the credit data (including Charlie) for three selected variables (age, amount, duration).</span></span>
<span id="cb44-672"><a href="#cb44-672" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Plots have the same pattern as the previous PD plots but with 10 lines plotted in parallel.</span></span>
<span id="cb44-673"><a href="#cb44-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-674"><a href="#cb44-674" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict_profile</span>(gbm_exp,  credit_x[<span class="dv">30</span><span class="sc">:</span><span class="dv">40</span>, ]))</span>
<span id="cb44-675"><a href="#cb44-675" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-676"><a href="#cb44-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-677"><a href="#cb44-677" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusions</span></span>
<span id="cb44-678"><a href="#cb44-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-679"><a href="#cb44-679" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to gain post hoc insights into a model trained with <span class="in">`mlr3`</span> by using the most popular approaches from the field of interpretable machine learning.</span>
<span id="cb44-680"><a href="#cb44-680" aria-hidden="true" tabindex="-1"></a>The methods are all model-agnostic and so do not depend on specific model classes.</span>
<span id="cb44-681"><a href="#cb44-681" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref_pkg("iml")`</span> and <span class="in">`r ref_pkg('DALEX')`</span> offer a wide range of (partly) overlapping methods, while <span class="in">`r ref_pkg('counterfactuals')`</span> focuses solely on counterfactual methods.</span>
<span id="cb44-682"><a href="#cb44-682" aria-hidden="true" tabindex="-1"></a>We demonstrated on <span class="in">`tsk("german_credit")`</span> how these packages offer an in-depth analysis of a GBM model fitted with <span class="in">`mlr3`</span>.</span>
<span id="cb44-683"><a href="#cb44-683" aria-hidden="true" tabindex="-1"></a>As we conclude the chapter we will highlight some limitations in the methods discussed above to help guide your own post hoc analyses.</span>
<span id="cb44-684"><a href="#cb44-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-685"><a href="#cb44-685" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Correlated Features {.unnumbered .unlisted}</span></span>
<span id="cb44-686"><a href="#cb44-686" aria-hidden="true" tabindex="-1"></a>If features are correlated, the insights from the interpretation methods should be treated with caution.</span>
<span id="cb44-687"><a href="#cb44-687" aria-hidden="true" tabindex="-1"></a>Changing the feature values of an observation without taking the correlation with other features into account leads to unrealistic combinations of the feature values.</span>
<span id="cb44-688"><a href="#cb44-688" aria-hidden="true" tabindex="-1"></a>Since such feature combinations are also unlikely to be part of the training data, the model will likely extrapolate in these areas <span class="co">[</span><span class="ot">@Molnar2022pitfalls; @Hooker2019PleaseSP</span><span class="co">]</span>.</span>
<span id="cb44-689"><a href="#cb44-689" aria-hidden="true" tabindex="-1"></a>This distorts the interpretation of methods that are based on changing single feature values such as PFI, PD plots, and Shapley values.</span>
<span id="cb44-690"><a href="#cb44-690" aria-hidden="true" tabindex="-1"></a>Alternative methods can help in these cases: conditional feature importance instead of PFI <span class="co">[</span><span class="ot">@Strobl2008; @Watson2021</span><span class="co">]</span>, accumulated local effect plots instead of PD plots <span class="co">[</span><span class="ot">@Apley2020</span><span class="co">]</span>, and the KernelSHAP method instead of Shapley values <span class="co">[</span><span class="ot">@Lundberg2019</span><span class="co">]</span>.</span>
<span id="cb44-691"><a href="#cb44-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-692"><a href="#cb44-692" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Rashomon Effect {.unnumbered .unlisted}</span></span>
<span id="cb44-693"><a href="#cb44-693" aria-hidden="true" tabindex="-1"></a>Explanations derived from an interpretation method can be ambiguous.</span>
<span id="cb44-694"><a href="#cb44-694" aria-hidden="true" tabindex="-1"></a>A method can deliver multiple equally plausible but potentially contradicting explanations.</span>
<span id="cb44-695"><a href="#cb44-695" aria-hidden="true" tabindex="-1"></a>This phenomenon is also called the <span class="in">`r index('Rashomon', lower = FALSE)`</span> effect <span class="co">[</span><span class="ot">@Breiman2001</span><span class="co">]</span>.</span>
<span id="cb44-696"><a href="#cb44-696" aria-hidden="true" tabindex="-1"></a>This effect can be due to changes in hyperparameters, the underlying dataset, or even the initial seed <span class="co">[</span><span class="ot">@Molnar2022pitfalls</span><span class="co">]</span>.</span>
<span id="cb44-697"><a href="#cb44-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-698"><a href="#cb44-698" aria-hidden="true" tabindex="-1"></a><span class="fu">#### High-Dimensional Data {.unnumbered .unlisted}</span></span>
<span id="cb44-699"><a href="#cb44-699" aria-hidden="true" tabindex="-1"></a><span class="in">`tsk("german_credit")`</span> is low-dimensional with a limited number of observations.</span>
<span id="cb44-700"><a href="#cb44-700" aria-hidden="true" tabindex="-1"></a>Applying interpretation methods off-the-shelf to higher dimensional datasets is often not feasible due to the enormous computational costs and so recent methods, such as Shapley values that use kernel-based estimators, have been developed to help over come this.</span>
<span id="cb44-701"><a href="#cb44-701" aria-hidden="true" tabindex="-1"></a>Another challenge is that the high-dimensional IML output generated for high-dimensional datasets can overwhelm users.</span>
<span id="cb44-702"><a href="#cb44-702" aria-hidden="true" tabindex="-1"></a>If the features can be meaningfully grouped, grouped versions of methods, e.g. the grouped feature importance proposed by @Au2022, can be applied.</span>
<span id="cb44-703"><a href="#cb44-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-704"><a href="#cb44-704" aria-hidden="true" tabindex="-1"></a>| Class | Constructor/Function | Fields/Methods |</span>
<span id="cb44-705"><a href="#cb44-705" aria-hidden="true" tabindex="-1"></a>| --- | -- | -- |</span>
<span id="cb44-706"><a href="#cb44-706" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("iml::Predictor")`</span> | <span class="in">`$new()`</span> | - |</span>
<span id="cb44-707"><a href="#cb44-707" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("iml::FeatureImp")`</span> | <span class="in">`$new(some_predictor)`</span> | <span class="in">`$plot()`</span> |</span>
<span id="cb44-708"><a href="#cb44-708" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("iml::FeatureEffect")`</span> | <span class="in">`$new(some_predictor)`</span> | <span class="in">`$plot()`</span> |</span>
<span id="cb44-709"><a href="#cb44-709" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("iml::LocalModel")`</span> | <span class="in">`$new(some_predictor, some_x)`</span> | <span class="in">`$results()`</span> |</span>
<span id="cb44-710"><a href="#cb44-710" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("iml::Shapley")`</span> | <span class="in">`$new(some_predictor, x.interest)`</span> | <span class="in">`$plot()`</span> |</span>
<span id="cb44-711"><a href="#cb44-711" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("counterfactuals::WhatIfClassif")`</span> | <span class="in">`$new(some_predictor)`</span> | <span class="in">`$find_counterfactuals()`</span> |</span>
<span id="cb44-712"><a href="#cb44-712" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("counterfactuals::MOCClassif")`</span> | <span class="in">`$new(some_predictor)`</span> | <span class="in">`$find_counterfactuals()`</span> |</span>
<span id="cb44-713"><a href="#cb44-713" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("DALEX::explainer")`</span> | <span class="in">`r ref("DALEXtra::explain_mlr3()")`</span> | <span class="dv">&amp;nbsp;&amp;nbsp;&amp;nbsp;</span> <span class="in">`model_parts()`</span>; <span class="in">`model_performance()`</span>; <span class="in">`predict_parts()`</span> |</span>
<span id="cb44-714"><a href="#cb44-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-715"><a href="#cb44-715" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable). {#tbl-interpretation-api}</span>
<span id="cb44-716"><a href="#cb44-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-717"><a href="#cb44-717" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb44-718"><a href="#cb44-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-719"><a href="#cb44-719" aria-hidden="true" tabindex="-1"></a>The following exercises are based on predictions of the value of soccer players based on their characteristics in the FIFA video game series.  They use the 2020 <span class="in">`fifa`</span> data available in DALEX. Solve them with either <span class="in">`iml`</span> or <span class="in">`DALEX`</span>.</span>
<span id="cb44-720"><a href="#cb44-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-721"><a href="#cb44-721" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Prepare an <span class="in">`mlr3`</span> regression task for the <span class="in">`fifa`</span> data. Select only features describing the age and skills of soccer players. Train a predictive model of your own choice on this task, to predict the value of a soccer player.</span>
<span id="cb44-722"><a href="#cb44-722" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the permutation importance method to calculate feature importance ranking. Which feature is the most important? Do you find the results surprising?</span>
<span id="cb44-723"><a href="#cb44-723" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Use the partial dependence plot/profile to draw the global behavior of the model for this feature. Is it aligned with your expectations?</span>
<span id="cb44-724"><a href="#cb44-724" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Choose Manuel Neuer as a specific example and calculate and plot the Shapley values. Which feature is locally the most important and has the strongest influence on his valuation as a soccer player? Calculate the ceteris paribus profiles / individual conditional expectation curves to visualize the local behavior of the model for this feature. Is it different from the global behavior?</span>
<span id="cb44-725"><a href="#cb44-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-726"><a href="#cb44-726" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb44-727"><a href="#cb44-727" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb44-728"><a href="#cb44-728" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</div>   
    <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>